[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "Python", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to make a spider with Scrapy which follows all links on a webpage, but I need to login before it does that, how would I go about that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "I want to make a spider with Scrapy which follows all links on a webpage, but I need to login before it does that, how would I go about that?", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": null, "id": "64xks9", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 4, "report_reasons": null, "author": "MaxwellTheWalrus", "saved": false, "mod_reports": [], "name": "t3_64xks9", "subreddit_name_prefixed": "r/Python", "approved_by": null, "over_18": false, "domain": "self.Python", "hidden": false, "thumbnail": "", "subreddit_id": "t5_2qh0y", "edited": false, "link_flair_css_class": null, "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/Python/comments/64xks9/scrapy_recursive_spider_with_login/", "num_reports": null, "locked": false, "stickied": false, "created": 1492025659.0, "url": "https://www.reddit.com/r/Python/comments/64xks9/scrapy_recursive_spider_with_login/", "author_flair_text": null, "quarantine": false, "title": "Scrapy Recursive Spider with login", "created_utc": 1491996859.0, "distinguished": null, "media": null, "upvote_ratio": 0.67, "num_comments": 1, "visited": false, "subreddit_type": "public", "ups": 4}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qh0y", "removal_reason": null, "link_id": "t3_64xks9", "likes": null, "replies": "", "user_reports": [], "id": "dg5ribu", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "granitosaurus", "parent_id": "t3_64xks9", "subreddit_name_prefixed": "r/Python", "controversiality": 0, "body": "Log in then enter recurssive loop that crawls the link.\n    \n    from scrapy import Spider, Request\n    from scrapy.linkextractor import LinkExtractor\n\n    class MySpider(Spider):\n        name = 'myspider'\n        start_urls = ['http://example.com/login']\n        le = LinkExtractor()\n        \n        def parse(self, response):\n            # generate login request\n            req = Request(..., callback=self.all_pages)\n            yield req\n        \n        def all_pages(self, response):\n            # save page\n            yield {'url': response.url}\n            # continue crawl other pages on this page\n            links = self.le.extract_links(response)\n            for link in links:\n                yield Request(link.url, self.all_pages)\n \nNote: in the example I used LinkExtractor because by default it ignores links that point to static non-web files (like pdfs etc.) which in usual broad-crawl you want to ignore.\n  \nBut you should really ask this in /r/learnpython or stackoverflow(scrapy tag is very active on it) :)\n", "edited": 1491998505.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Log in then enter recurssive loop that crawls the link.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from scrapy import Spider, Request\nfrom scrapy.linkextractor import LinkExtractor\n\nclass MySpider(Spider):\n    name = &amp;#39;myspider&amp;#39;\n    start_urls = [&amp;#39;http://example.com/login&amp;#39;]\n    le = LinkExtractor()\n\n    def parse(self, response):\n        # generate login request\n        req = Request(..., callback=self.all_pages)\n        yield req\n\n    def all_pages(self, response):\n        # save page\n        yield {&amp;#39;url&amp;#39;: response.url}\n        # continue crawl other pages on this page\n        links = self.le.extract_links(response)\n        for link in links:\n            yield Request(link.url, self.all_pages)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Note: in the example I used LinkExtractor because by default it ignores links that point to static non-web files (like pdfs etc.) which in usual broad-crawl you want to ignore.&lt;/p&gt;\n\n&lt;p&gt;But you should really ask this in &lt;a href=\"/r/learnpython\"&gt;/r/learnpython&lt;/a&gt; or stackoverflow(scrapy tag is very active on it) :)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Python", "name": "t1_dg5ribu", "score_hidden": false, "stickied": false, "created": 1492027061.0, "created_utc": 1491998261.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}]