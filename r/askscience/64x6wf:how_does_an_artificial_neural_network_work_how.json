[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "askscience", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently started getting interested in &lt;a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\"&gt;Artificial Neural Networks&lt;/a&gt; and, even though I&amp;#39;m a programmer, I can&amp;#39;t grasp how the whole process works in terms of searching, scanning and evaluating, let alone storing and using new data as new criteria.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have experience on the matter? I&amp;#39;ve checked wikipedia and similar results for a brief introduction on ANNs, but the concept is still far for me. &lt;/p&gt;\n\n&lt;p&gt;Thanks, have a great day :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "Hello, \n\nI've recently started getting interested in [Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network) and, even though I'm a programmer, I can't grasp how the whole process works in terms of searching, scanning and evaluating, let alone storing and using new data as new criteria.\n\nDoes anybody have experience on the matter? I've checked wikipedia and similar results for a brief introduction on ANNs, but the concept is still far for me. \n\nThanks, have a great day :)", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Computing", "id": "64x6wf", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 9, "report_reasons": null, "author": "IAMZizzi05", "saved": false, "mod_reports": [], "name": "t3_64x6wf", "subreddit_name_prefixed": "r/askscience", "approved_by": null, "over_18": false, "domain": "self.askscience", "hidden": false, "thumbnail": "", "subreddit_id": "t5_2qm4e", "edited": false, "link_flair_css_class": "computing", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/askscience/comments/64x6wf/how_does_an_artificial_neural_network_work_how/", "num_reports": null, "locked": false, "stickied": false, "created": 1492019606.0, "url": "https://www.reddit.com/r/askscience/comments/64x6wf/how_does_an_artificial_neural_network_work_how/", "author_flair_text": null, "quarantine": false, "title": "How does an artificial neural network work? How does it \"learn\", \"evaluate\" and \"use\" data accordingly?", "created_utc": 1491990806.0, "distinguished": null, "media": null, "upvote_ratio": 0.75, "num_comments": 15, "visited": false, "subreddit_type": "public", "ups": 9}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": "", "user_reports": [], "id": "dgaju45", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "IAMZizzi05", "parent_id": "t1_dg9wbag", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Damn, that's extremely interesting! Thanks for sharing your knowledge, I had no idea that the context recognization was a work like that. \n\nAllow me a question, though. So, we have the \"prediction\" - in this *kind* of sentece there are many options, but not EVERY single word in english - like your \"animal\" example; does this kind of prediction, knowing the words with sentiment and the overall meaning of the sentence (see below*), help with giving the sentences the kind of rating other users were talking about? The one with the range of approval-disapproval, expressed in numbers.\n\n*Since it knows what to put, he kind of understands what's it about, even though it doesn't actually understand the actual, english, meaning. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Damn, that&amp;#39;s extremely interesting! Thanks for sharing your knowledge, I had no idea that the context recognization was a work like that. &lt;/p&gt;\n\n&lt;p&gt;Allow me a question, though. So, we have the &amp;quot;prediction&amp;quot; - in this &lt;em&gt;kind&lt;/em&gt; of sentece there are many options, but not EVERY single word in english - like your &amp;quot;animal&amp;quot; example; does this kind of prediction, knowing the words with sentiment and the overall meaning of the sentence (see below*), help with giving the sentences the kind of rating other users were talking about? The one with the range of approval-disapproval, expressed in numbers.&lt;/p&gt;\n\n&lt;p&gt;*Since it knows what to put, he kind of understands what&amp;#39;s it about, even though it doesn&amp;#39;t actually understand the actual, english, meaning. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dgaju45", "score_hidden": false, "stickied": false, "created": 1492284270.0, "created_utc": 1492255470.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9wbag", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "zardeh", "parent_id": "t1_dg5xtj0", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt;How do you take a Tweet, extract meaningful quantitative information from it, and then pass those data through the neural network?\n\nI can elaborate here. /u/IAMZizzi05, tagging you too, because this is cool stuff!\n\nMost linguistic models work in one of a few ways, either they are [recurrent](https://blog.openai.com/unsupervised-sentiment-neuron/), or they are based on [word2vec](https://www.tensorflow.org/tutorials/word2vec). Most sentiment based models, at least until recently (2017) were the second, so I'll focus on it.\n\nEssentially, they use context clues. If you have a lot of text (and I mean *lots* of text, gigabytes of text, \"all of wikipedia\" or \"every amazon review ever\" scale text corpora), you can see what words start to appear next to each other. \n\nImagine I give you the sentence \"My favorite animal is a ______\". We'd expect the blank would be \"dog\" or \"giraffe\", but not \"Spokane, Washington\". If you have a lot of text, you end up with a lot of sentences that look like \"My favorite animal is an X\", with lots of options for X. You can start to figure out that all of the Xs are similar. You don't necessarily know why, but statistically speaking, they have something in common.\n\nIn this case the cool part is that word2vec manages to do this kind of thing for every word in parallel. And it doesn't just do it for \"how animal is this thing\" but it also does it for a bunch of other things. The whole reason its called word2vec is because you end up with a vector in 50-250 dimensions where each dimension represents something. We generally don't know or care what, but similar words cluster together, kind of like [this](https://www.tensorflow.org/images/tsne.png). \n\nSo with that, and by labeling a small number of words (\"good\", \"bad\", \"happy\", \"unhappy\") with sentiment, you can compare where any given word is in relation to those labelled words and get a good estimate of how good or bad it is. Do that for all words in a phrase and you have a basic sentiment analyzer. And neural networks are very good at training word2vec models, and can be trained to do the analysis of weight and word relationships, which means the entire process can be done end to end with a neural network (or more often, 2 networks, 1 to train a word2vec model, and 1 to apply it).\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;How do you take a Tweet, extract meaningful quantitative information from it, and then pass those data through the neural network?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I can elaborate here. &lt;a href=\"/u/IAMZizzi05\"&gt;/u/IAMZizzi05&lt;/a&gt;, tagging you too, because this is cool stuff!&lt;/p&gt;\n\n&lt;p&gt;Most linguistic models work in one of a few ways, either they are &lt;a href=\"https://blog.openai.com/unsupervised-sentiment-neuron/\"&gt;recurrent&lt;/a&gt;, or they are based on &lt;a href=\"https://www.tensorflow.org/tutorials/word2vec\"&gt;word2vec&lt;/a&gt;. Most sentiment based models, at least until recently (2017) were the second, so I&amp;#39;ll focus on it.&lt;/p&gt;\n\n&lt;p&gt;Essentially, they use context clues. If you have a lot of text (and I mean &lt;em&gt;lots&lt;/em&gt; of text, gigabytes of text, &amp;quot;all of wikipedia&amp;quot; or &amp;quot;every amazon review ever&amp;quot; scale text corpora), you can see what words start to appear next to each other. &lt;/p&gt;\n\n&lt;p&gt;Imagine I give you the sentence &amp;quot;My favorite animal is a ______&amp;quot;. We&amp;#39;d expect the blank would be &amp;quot;dog&amp;quot; or &amp;quot;giraffe&amp;quot;, but not &amp;quot;Spokane, Washington&amp;quot;. If you have a lot of text, you end up with a lot of sentences that look like &amp;quot;My favorite animal is an X&amp;quot;, with lots of options for X. You can start to figure out that all of the Xs are similar. You don&amp;#39;t necessarily know why, but statistically speaking, they have something in common.&lt;/p&gt;\n\n&lt;p&gt;In this case the cool part is that word2vec manages to do this kind of thing for every word in parallel. And it doesn&amp;#39;t just do it for &amp;quot;how animal is this thing&amp;quot; but it also does it for a bunch of other things. The whole reason its called word2vec is because you end up with a vector in 50-250 dimensions where each dimension represents something. We generally don&amp;#39;t know or care what, but similar words cluster together, kind of like &lt;a href=\"https://www.tensorflow.org/images/tsne.png\"&gt;this&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;So with that, and by labeling a small number of words (&amp;quot;good&amp;quot;, &amp;quot;bad&amp;quot;, &amp;quot;happy&amp;quot;, &amp;quot;unhappy&amp;quot;) with sentiment, you can compare where any given word is in relation to those labelled words and get a good estimate of how good or bad it is. Do that for all words in a phrase and you have a basic sentiment analyzer. And neural networks are very good at training word2vec models, and can be trained to do the analysis of weight and word relationships, which means the entire process can be done end to end with a neural network (or more often, 2 networks, 1 to train a word2vec model, and 1 to apply it).&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg9wbag", "score_hidden": false, "stickied": false, "created": 1492239555.0, "created_utc": 1492210755.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5xtj0", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "andybmcc", "parent_id": "t1_dg5su03", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt;It can be used for further training the network. Remember the network in general has many parameters. Each variable gets a weight factor at each layer of the network. So if you've got n variables and m layers, you've got at least n*m adjustable parameters which need to be given values. These values can be tuned to optimize the performance of the network.\n\nAnd there are a lot of ways to tune these parameters.  One of the simplest is called \"backwards propagation of error\".  You essentially have a training set that is already classified that you mash through your network, and compare the desired results to that of the network.  You can then work backwards from the network output to tweak the weights on your links to move along a gradient towards a lower error at a given step size.  Often, you'll see the node transfer functions are saturating (like a sigmoid).  Small tweaks to the weights will have little effect on a saturated node, so outlying inputs  in a training set won't throw you too far off.\n\nhttps://en.wikipedia.org/wiki/Backpropagation\n\n&gt;How do you take a Tweet, extract meaningful quantitative information from it, and then pass those data through the neural network?\nThis is sort of a linguistics question as much as a math or computer science one. How does one \"quantify\" positivity or negativity of words in the Tweet? \n\nThis is probably the most difficult aspect of the tweet example.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;It can be used for further training the network. Remember the network in general has many parameters. Each variable gets a weight factor at each layer of the network. So if you&amp;#39;ve got n variables and m layers, you&amp;#39;ve got at least n*m adjustable parameters which need to be given values. These values can be tuned to optimize the performance of the network.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;And there are a lot of ways to tune these parameters.  One of the simplest is called &amp;quot;backwards propagation of error&amp;quot;.  You essentially have a training set that is already classified that you mash through your network, and compare the desired results to that of the network.  You can then work backwards from the network output to tweak the weights on your links to move along a gradient towards a lower error at a given step size.  Often, you&amp;#39;ll see the node transfer functions are saturating (like a sigmoid).  Small tweaks to the weights will have little effect on a saturated node, so outlying inputs  in a training set won&amp;#39;t throw you too far off.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://en.wikipedia.org/wiki/Backpropagation\"&gt;https://en.wikipedia.org/wiki/Backpropagation&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;How do you take a Tweet, extract meaningful quantitative information from it, and then pass those data through the neural network?\nThis is sort of a linguistics question as much as a math or computer science one. How does one &amp;quot;quantify&amp;quot; positivity or negativity of words in the Tweet? &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is probably the most difficult aspect of the tweet example.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5xtj0", "score_hidden": false, "stickied": false, "created": 1492036417.0, "created_utc": 1492007617.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": "", "user_reports": [], "id": "dg644ri", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "IAMZizzi05", "parent_id": "t1_dg5su03", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Hey, thanks for replying again! \n\nYeah, I guessed that the tweet stuff specifically was kind of a tricky thing to break down, that's pretty much 90% of why I decided to post here and discuss about it. While it's fascinating, it's kind of hard to guess how a machine could bypass and filter low quality samples and content (ironic, false or neutral) and get results out of it anyway. \n\nI guess that the range theory is quite strong when it comes to rating, as it will give the ANN a base to start working with. \n\nEnlightening and stimulating! Thanks a whole lot for your time and help, I will dig deeper into this and try to make the best use of the knowledge you provided me with.\n\nHave a nice day!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey, thanks for replying again! &lt;/p&gt;\n\n&lt;p&gt;Yeah, I guessed that the tweet stuff specifically was kind of a tricky thing to break down, that&amp;#39;s pretty much 90% of why I decided to post here and discuss about it. While it&amp;#39;s fascinating, it&amp;#39;s kind of hard to guess how a machine could bypass and filter low quality samples and content (ironic, false or neutral) and get results out of it anyway. &lt;/p&gt;\n\n&lt;p&gt;I guess that the range theory is quite strong when it comes to rating, as it will give the ANN a base to start working with. &lt;/p&gt;\n\n&lt;p&gt;Enlightening and stimulating! Thanks a whole lot for your time and help, I will dig deeper into this and try to make the best use of the knowledge you provided me with.&lt;/p&gt;\n\n&lt;p&gt;Have a nice day!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg644ri", "score_hidden": false, "stickied": false, "created": 1492043365.0, "created_utc": 1492014565.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5su03", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "RobusEtCeleritas", "parent_id": "t1_dg5seww", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt;With that said, let me put it in an example related to the presentation: let's say we have a tweet, and the ANN is already \"trained\" to tell apart \"A\" (approval) from \"B\" (disapproval). How does this roll out? How can string comparing determine what the tweet precisely mean?\n\nSo there's a lot to think about with this question. I'll talk about it in *reverse* order of what the computer will actually do. Let's say you have your neural network set up and trained so that the distribution of \"disapproval\" is more or less Gaussian with mean 0, and \"approval\" is Gaussian with mean 1, and widths small enough to resolve the two.\n\nYou network is going to go through potentially many layers and transformations (some linear and some nonlinear), and at the end of it all a number pops out. The computer is now doing a statistical hypothesis test. Is this number more consistent with 0 (disapproval) or 1 (approval)? The logical thing to do would veto set a cutoff at 0.5; anything less is considered disapproval and anything greater is considered approval.\n\nBut the hard part of the question now becomes, *how* did you get that number in the first place?\n\nHow do you take a Tweet, extract meaningful **quantitative** information from it, and then pass those data through the neural network?\n\nThis is sort of a linguistics question as much as a math or computer science one. How does one \"quantify\" positivity or negativity of words in the Tweet? I have no idea how one would do this, as the only neural networks I've worked with have taken things which are *already* numbers as inputs. Simply put, I don't personally know how to analyze the language in a Tweet and turn it into a set of numbers containing information about the intentions of the author.\n\n&gt;And most importantly: if the tweet turns out to belong to either one of the groups, how can it improve future sorting if it is the product of the same sort?\n\nIt can be used for further training the network. Remember the network in general has many parameters. Each variable gets a weight factor at each layer of the network. So if you've got n variables and m layers, you've got at least n\\*m adjustable parameters which need to be given values. These values can be tuned to optimize the performance of the network.\n\nSo if you have a bunch of Tweets which you know fall under \"approval\", you can vary some of the weight coefficients such that the largest number of these Tweets are correctly identified as \"approval\".\n\n&gt;Sorry about this follow-up, your reply was already huge and informative, so I wouldn't really want to push more, but the topic baffles me to no end.\n\nSure, ask as many questions as you want. I know we have some people here who are **real** experts in machine learning, and I'm sure they'd be glad to help out as well.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;With that said, let me put it in an example related to the presentation: let&amp;#39;s say we have a tweet, and the ANN is already &amp;quot;trained&amp;quot; to tell apart &amp;quot;A&amp;quot; (approval) from &amp;quot;B&amp;quot; (disapproval). How does this roll out? How can string comparing determine what the tweet precisely mean?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;So there&amp;#39;s a lot to think about with this question. I&amp;#39;ll talk about it in &lt;em&gt;reverse&lt;/em&gt; order of what the computer will actually do. Let&amp;#39;s say you have your neural network set up and trained so that the distribution of &amp;quot;disapproval&amp;quot; is more or less Gaussian with mean 0, and &amp;quot;approval&amp;quot; is Gaussian with mean 1, and widths small enough to resolve the two.&lt;/p&gt;\n\n&lt;p&gt;You network is going to go through potentially many layers and transformations (some linear and some nonlinear), and at the end of it all a number pops out. The computer is now doing a statistical hypothesis test. Is this number more consistent with 0 (disapproval) or 1 (approval)? The logical thing to do would veto set a cutoff at 0.5; anything less is considered disapproval and anything greater is considered approval.&lt;/p&gt;\n\n&lt;p&gt;But the hard part of the question now becomes, &lt;em&gt;how&lt;/em&gt; did you get that number in the first place?&lt;/p&gt;\n\n&lt;p&gt;How do you take a Tweet, extract meaningful &lt;strong&gt;quantitative&lt;/strong&gt; information from it, and then pass those data through the neural network?&lt;/p&gt;\n\n&lt;p&gt;This is sort of a linguistics question as much as a math or computer science one. How does one &amp;quot;quantify&amp;quot; positivity or negativity of words in the Tweet? I have no idea how one would do this, as the only neural networks I&amp;#39;ve worked with have taken things which are &lt;em&gt;already&lt;/em&gt; numbers as inputs. Simply put, I don&amp;#39;t personally know how to analyze the language in a Tweet and turn it into a set of numbers containing information about the intentions of the author.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;And most importantly: if the tweet turns out to belong to either one of the groups, how can it improve future sorting if it is the product of the same sort?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It can be used for further training the network. Remember the network in general has many parameters. Each variable gets a weight factor at each layer of the network. So if you&amp;#39;ve got n variables and m layers, you&amp;#39;ve got at least n*m adjustable parameters which need to be given values. These values can be tuned to optimize the performance of the network.&lt;/p&gt;\n\n&lt;p&gt;So if you have a bunch of Tweets which you know fall under &amp;quot;approval&amp;quot;, you can vary some of the weight coefficients such that the largest number of these Tweets are correctly identified as &amp;quot;approval&amp;quot;.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Sorry about this follow-up, your reply was already huge and informative, so I wouldn&amp;#39;t really want to push more, but the topic baffles me to no end.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Sure, ask as many questions as you want. I know we have some people here who are &lt;strong&gt;real&lt;/strong&gt; experts in machine learning, and I&amp;#39;m sure they&amp;#39;d be glad to help out as well.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5su03", "score_hidden": false, "stickied": false, "created": 1492029531.0, "created_utc": 1492000731.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": "", "user_reports": [], "id": "dg71tm1", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "xea123123", "parent_id": "t1_dg63rpf", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Well thank you, glad to hear I helped!\n\nI stored all the sample data as follows for each video frame (so, one image every 5 minutes or something to that effect): \n\n* A high resolution .PNG,\n\n* A low resolution PNG for running image-resolution independent test cases on\n\n* A CSV file containing\u200b these columns: metric ID, metric version, and values\n\n* A text file listing the pixel coordinates of regions of interest. I don't recall which algorithm I used for ROI extraction, but the general idea is image goes in, list of xy locations comes out, then that list of locations is one of the required metrics for a bunch of other computer vision algorithms. Saving that in a file meant I didn't need to repeat ROI calculations every time I calculate new metrics or fix a bug and recalculate my old metrics.\n\n\nSo that CSV file per image is how I store the data in files, and I'd read them all into memory somehow when using them. I think I was just storing them all in a big matrix, when working in Matlab, and a NumPy matrix of some kind when working in Python.\n\nI used this general pattern for storing data like this across many projects, and it was a massive sanity reducer. Anything can read CSV files, writing and testing is easy as heck, visually inspecting your data is easy, and I can't overemphasize the importance of having some way of cataloging and versioning your metrics. \n\nEven if your metrics are all just things you write from your own imagination, being able to run your latest version of a particular metric along side the previous couple versions and comparing the results is something you're going to want to do occasionally and it'll be a lot of work if you don't plan ahead for it. \n\nI still don't feel like I have good intuition or logic to decide what's a minor bug fix and what's a significant change that deserves a new version number, so I mostly used dates as my versions, so for instance I'd have a metric named DebaucheyAutoHighpass and a version number of 20140401_2 if I happened to make 2 versions that day, and the implementation would be in a file called DebaucheyAutoHighpass_20140401_2.py (or .m).\n\nAnother bonus of organizing my dev work this way is that walking someone else through previous versions or sharing/reporting my techniques at the end becomes really easy.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well thank you, glad to hear I helped!&lt;/p&gt;\n\n&lt;p&gt;I stored all the sample data as follows for each video frame (so, one image every 5 minutes or something to that effect): &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;A high resolution .PNG,&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A low resolution PNG for running image-resolution independent test cases on&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A CSV file containing\u200b these columns: metric ID, metric version, and values&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A text file listing the pixel coordinates of regions of interest. I don&amp;#39;t recall which algorithm I used for ROI extraction, but the general idea is image goes in, list of xy locations comes out, then that list of locations is one of the required metrics for a bunch of other computer vision algorithms. Saving that in a file meant I didn&amp;#39;t need to repeat ROI calculations every time I calculate new metrics or fix a bug and recalculate my old metrics.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So that CSV file per image is how I store the data in files, and I&amp;#39;d read them all into memory somehow when using them. I think I was just storing them all in a big matrix, when working in Matlab, and a NumPy matrix of some kind when working in Python.&lt;/p&gt;\n\n&lt;p&gt;I used this general pattern for storing data like this across many projects, and it was a massive sanity reducer. Anything can read CSV files, writing and testing is easy as heck, visually inspecting your data is easy, and I can&amp;#39;t overemphasize the importance of having some way of cataloging and versioning your metrics. &lt;/p&gt;\n\n&lt;p&gt;Even if your metrics are all just things you write from your own imagination, being able to run your latest version of a particular metric along side the previous couple versions and comparing the results is something you&amp;#39;re going to want to do occasionally and it&amp;#39;ll be a lot of work if you don&amp;#39;t plan ahead for it. &lt;/p&gt;\n\n&lt;p&gt;I still don&amp;#39;t feel like I have good intuition or logic to decide what&amp;#39;s a minor bug fix and what&amp;#39;s a significant change that deserves a new version number, so I mostly used dates as my versions, so for instance I&amp;#39;d have a metric named DebaucheyAutoHighpass and a version number of 20140401_2 if I happened to make 2 versions that day, and the implementation would be in a file called DebaucheyAutoHighpass_20140401_2.py (or .m).&lt;/p&gt;\n\n&lt;p&gt;Another bonus of organizing my dev work this way is that walking someone else through previous versions or sharing/reporting my techniques at the end becomes really easy.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg71tm1", "score_hidden": false, "stickied": false, "created": 1492081612.0, "created_utc": 1492052812.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg63rpf", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "IAMZizzi05", "parent_id": "t1_dg606l5", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "First of all, thanks a lot for your reply and thanks for sharing your own project, it was a pleasure to read and to hear about it! \n\nI guess that the rating system is the most logical one, and the \"compare to samples\" part when it comes down to text is made by marking words of approval/disapproval, keeping count of negations and such. The way you explained it by mentioning the picture analysis cleared that up a whole lot.\n\nDid you happen to store your sample data (the one to compare new inputs with) in some specific way? Such as a database, or simple files? \n\nThanks again, have a lovely evening \u2665", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;First of all, thanks a lot for your reply and thanks for sharing your own project, it was a pleasure to read and to hear about it! &lt;/p&gt;\n\n&lt;p&gt;I guess that the rating system is the most logical one, and the &amp;quot;compare to samples&amp;quot; part when it comes down to text is made by marking words of approval/disapproval, keeping count of negations and such. The way you explained it by mentioning the picture analysis cleared that up a whole lot.&lt;/p&gt;\n\n&lt;p&gt;Did you happen to store your sample data (the one to compare new inputs with) in some specific way? Such as a database, or simple files? &lt;/p&gt;\n\n&lt;p&gt;Thanks again, have a lovely evening \u2665&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg63rpf", "score_hidden": false, "stickied": false, "created": 1492042987.0, "created_utc": 1492014187.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg606l5", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "xea123123", "parent_id": "t1_dg5seww", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt; If so, do you know how the actual \"comparing\" and \"storing new data to minimize future misidentifications\" works?\n\nI've done a few school projects with machine learning components on radically different types of problems, and might be able to share some insight  on those that will kind of relate to the kind of projects you saw.\n\nOne project that worked pretty well took  pictures from a camera pointed at a garden and tried to guess from weather reports and those pictures how dry different parts of the garden were, ostensibly to decide which water sprinkler heads to turn on and for how long.\n\nIn my case the comparison between predictions and truth was easy. Tedious, but easy. I dug up dirt samples around a garden many times a day, weighed them very precisely, baked them for a day so they'd dry out thoroughly, and compare their weight to ultimately determine how much water was lost. Incidentally, i compared soil composition at each test site in the yard too but it was all pretty much the same. You're probably left wondering how something so wishy washy as the political intent of those tweets could be measured numerically, and my complete guess based on what I did for some projects was sit there reading tweets and writing down a number between zero and 10 based on how much I thought the tweet agreed with what I want, then get more people to do the same thing and hope the people doing the manual data annotation have roughly the same biases and opinions as the people who would theoretically be using the tweet analyzer program out in the real world.\n\nSo, I spent a week periodically measuring soil moisture and recorded a time lapse video of the yard for a week, and then I was ready to build a model to predict soil moisture. I took the first half of the data set, called it my training data, and wrote a program to record all the easily measurable metrics I could think of like \"ratio of average brightness of the pixels in one image to average brightness of all images recorded so far\", \"what % of the intensity of the image is red versus blue in the brightest 200 pixel wide rectangle of the screen\", \"what temperature does the weather forecasting website think it is\", and \"what time of day it is\".\n\nIn your tweet analyzer example they probably used whatever computational linguistics metrics they recently learned about, or were recommended by their prof, I imagine. They'll be substantially weirder and less intuitive than mine, for sure. \n\nAfter my first pass at getting a model working I ran it on the second half of my data set, and unsurprisingly it didn't work especially well. At that point I added more metrics taken from research publications, but if I kept going on the project (because, let's say, it was my job instead of a school project) I would probably have also collected more data to feed in, that I would manually measure soil moisture for. Since that's really tedious sounding, and since a self improving algorithm sounded novel, I tried a classic not-especially-statistically-valid approach that I learned about by the name of bias feedback, and this is likely what your peers used. When your model scores a tweet from 0 to 100 on a scale of strongly disagreeing with the hashtag to strongly supporting the hashtag, take any tweets which score below 10 or above 90 as perceived truth, add them to your training data set, and reselect your parameters. This is obviously not a good way to get a more accurate model, but it does produce a model which gives more decisive answers which means that it's easier for casual observers to tells if they agree with the algorithm or not, which we tend to want. \n\nMaking a self refining model that actually gets more accurate over time requires that you successfully pick a rule for finding accurate predictions. Encouraging people to rate the accuracy of your predictions by building a rating system into some app or fun looking website is commonly used, but that falls down when the people doing the rating aren't a good representation of the people using the prediction which is also a common problem.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;If so, do you know how the actual &amp;quot;comparing&amp;quot; and &amp;quot;storing new data to minimize future misidentifications&amp;quot; works?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I&amp;#39;ve done a few school projects with machine learning components on radically different types of problems, and might be able to share some insight  on those that will kind of relate to the kind of projects you saw.&lt;/p&gt;\n\n&lt;p&gt;One project that worked pretty well took  pictures from a camera pointed at a garden and tried to guess from weather reports and those pictures how dry different parts of the garden were, ostensibly to decide which water sprinkler heads to turn on and for how long.&lt;/p&gt;\n\n&lt;p&gt;In my case the comparison between predictions and truth was easy. Tedious, but easy. I dug up dirt samples around a garden many times a day, weighed them very precisely, baked them for a day so they&amp;#39;d dry out thoroughly, and compare their weight to ultimately determine how much water was lost. Incidentally, i compared soil composition at each test site in the yard too but it was all pretty much the same. You&amp;#39;re probably left wondering how something so wishy washy as the political intent of those tweets could be measured numerically, and my complete guess based on what I did for some projects was sit there reading tweets and writing down a number between zero and 10 based on how much I thought the tweet agreed with what I want, then get more people to do the same thing and hope the people doing the manual data annotation have roughly the same biases and opinions as the people who would theoretically be using the tweet analyzer program out in the real world.&lt;/p&gt;\n\n&lt;p&gt;So, I spent a week periodically measuring soil moisture and recorded a time lapse video of the yard for a week, and then I was ready to build a model to predict soil moisture. I took the first half of the data set, called it my training data, and wrote a program to record all the easily measurable metrics I could think of like &amp;quot;ratio of average brightness of the pixels in one image to average brightness of all images recorded so far&amp;quot;, &amp;quot;what % of the intensity of the image is red versus blue in the brightest 200 pixel wide rectangle of the screen&amp;quot;, &amp;quot;what temperature does the weather forecasting website think it is&amp;quot;, and &amp;quot;what time of day it is&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;In your tweet analyzer example they probably used whatever computational linguistics metrics they recently learned about, or were recommended by their prof, I imagine. They&amp;#39;ll be substantially weirder and less intuitive than mine, for sure. &lt;/p&gt;\n\n&lt;p&gt;After my first pass at getting a model working I ran it on the second half of my data set, and unsurprisingly it didn&amp;#39;t work especially well. At that point I added more metrics taken from research publications, but if I kept going on the project (because, let&amp;#39;s say, it was my job instead of a school project) I would probably have also collected more data to feed in, that I would manually measure soil moisture for. Since that&amp;#39;s really tedious sounding, and since a self improving algorithm sounded novel, I tried a classic not-especially-statistically-valid approach that I learned about by the name of bias feedback, and this is likely what your peers used. When your model scores a tweet from 0 to 100 on a scale of strongly disagreeing with the hashtag to strongly supporting the hashtag, take any tweets which score below 10 or above 90 as perceived truth, add them to your training data set, and reselect your parameters. This is obviously not a good way to get a more accurate model, but it does produce a model which gives more decisive answers which means that it&amp;#39;s easier for casual observers to tells if they agree with the algorithm or not, which we tend to want. &lt;/p&gt;\n\n&lt;p&gt;Making a self refining model that actually gets more accurate over time requires that you successfully pick a rule for finding accurate predictions. Encouraging people to rate the accuracy of your predictions by building a rating system into some app or fun looking website is commonly used, but that falls down when the people doing the rating aren&amp;#39;t a good representation of the people using the prediction which is also a common problem.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg606l5", "score_hidden": false, "stickied": false, "created": 1492039094.0, "created_utc": 1492010294.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5seww", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "IAMZizzi05", "parent_id": "t1_dg5rmg5", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Actually, your reply was enlightening, to be honest. Thanks for your input.\n\nSo, in case you had the chance to test an ANN in the practical way, may I ask another follow-up question? \n\nIf so, do you know how the actual \"comparing\" and \"storing new data to minimize future misidentifications\" works? \n\n------\n\nLet me explain myself a little better. I've recently attended a lesson at my local university in which a group of students made a presentation on their ANN on Twitter. Basically, it stores trends (sorted by hashtag), and then proceeds to analyze all the relative tweets. By using a previously trained ANN, their software can successfully decide wether or not a single tweet supports or not the object of the hashtag; this way, the program can finally make statistics about general approval of a subject.\n\n-----\n\nWith that said, let me put it in an example related to the presentation: let's say we have a tweet, and the ANN is already \"trained\" to tell apart \"A\" (approval) from \"B\" (disapproval). How does this roll out? How can string comparing determine what the tweet precisely means? \n\nAnd most importantly: if the tweet turns out to belong to either one of the groups, how can it improve future sorting if it *is* the product of the same sort? \n\nSorry about this follow-up, your reply was already huge and informative, so I wouldn't really want to push more, but the topic baffles me to no end.\n\nCheers, and thanks again!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Actually, your reply was enlightening, to be honest. Thanks for your input.&lt;/p&gt;\n\n&lt;p&gt;So, in case you had the chance to test an ANN in the practical way, may I ask another follow-up question? &lt;/p&gt;\n\n&lt;p&gt;If so, do you know how the actual &amp;quot;comparing&amp;quot; and &amp;quot;storing new data to minimize future misidentifications&amp;quot; works? &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Let me explain myself a little better. I&amp;#39;ve recently attended a lesson at my local university in which a group of students made a presentation on their ANN on Twitter. Basically, it stores trends (sorted by hashtag), and then proceeds to analyze all the relative tweets. By using a previously trained ANN, their software can successfully decide wether or not a single tweet supports or not the object of the hashtag; this way, the program can finally make statistics about general approval of a subject.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;With that said, let me put it in an example related to the presentation: let&amp;#39;s say we have a tweet, and the ANN is already &amp;quot;trained&amp;quot; to tell apart &amp;quot;A&amp;quot; (approval) from &amp;quot;B&amp;quot; (disapproval). How does this roll out? How can string comparing determine what the tweet precisely means? &lt;/p&gt;\n\n&lt;p&gt;And most importantly: if the tweet turns out to belong to either one of the groups, how can it improve future sorting if it &lt;em&gt;is&lt;/em&gt; the product of the same sort? &lt;/p&gt;\n\n&lt;p&gt;Sorry about this follow-up, your reply was already huge and informative, so I wouldn&amp;#39;t really want to push more, but the topic baffles me to no end.&lt;/p&gt;\n\n&lt;p&gt;Cheers, and thanks again!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5seww", "score_hidden": false, "stickied": false, "created": 1492028801.0, "created_utc": 1492000001.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5rmg5", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "RobusEtCeleritas", "parent_id": "t3_64x6wf", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "An artificial neural network is just a way for a machine to make a decision. What you're basically doing is having a computer perform a statistical hypothesis test, and make a decision based on a *quantitative* test statistic.\n\nSo you have to come up with an appropriate test statistic, optimized to make the neural network work as well as it can.\n\nHere's a very simple example of a neural network. You have some data x*_i_*, where i runs from 1 to n. These are just a bunch of different quantities which potentially contain information that your network can use to make its decision. You can define a simple linear test statistic for this data as\n\nt = \u03a3*_i_* a*_i_* x*_i_*, where a*_i_* are just weight coefficients. And there are ways to optimize these weight coefficients.\n\nSo to make that equation completely transparent, say you've got two quantities x*_1_* and x*_2_*, the equation above would become t = a*_1_* x*_1_* + a*_2_* x*_2_*, just a weighted linear sum of the two variables.\n\nAnyway now you have this simple linear test statistic, which also has the advantage of being a \"scalar\" (meaning a single number instead of n individual numbers).\n\nNow you can send this test statistic through an *activator function*. There are many possible activator functions for neural networks, some common ones being the unit step function, the logistic sigmoid, etc.\n\nFor this example, let's take the logistic sigmoid:\n\ns(t) = 1/(1 + e^(-t)).\n\nThis is now a nonlinear transformation which you can send your data through, which will hopefully make it easier to make a decision about it (or have a computer do so).\n\nIn a more complicated neural network, you can add additional layers, and \"train\" the network to optimize all of the coefficients in each of your layers. But in principle this is already a functioning neural network.\n\nNow say you have some data for all of the x*_i_* and you want to know whether it came from distribution X or distribution Y. Maybe X and Y are pretty similar and you can't tell just by looking at the data. You can feed the data through that nonlinear activation function above, and maybe it will be easier to tell X and Y apart.\n\nSo that's a very basic explanation of what exactly the network is and how it works.\n\nAs for your specific questions, \"learning\" in this case would be using data that you *know* came from X and Y to optimize the parameters (a*_i_*) of the neural network. In other words it could be easier or harder to tell X data and Y data apart based on the values of the weight coefficients (a*_i_*). You are \"training\" the network by choosing the best values of a*_i_* such that X and Y are easiest to tell apart.\n\nOnce you're confident that your neural network is functioning, you can have a computer make these decisions for you. For each data point it will calculate the test statistic, run it through the activator function, and then based on the value which pops out, it should either be closer to what you'd expect from distribution X or to what you'd expect from distribution Y. Then comparing that number to a pre-defined cutoff point, the computer will decide whether the data point you've input has come from distribution X or Y. Of course there is some error percentage, where data points will inevitably be misidentified. But part of training the network is minimizing those misidentifications.\n\nAnyway, I hope this helped a little. I'm not a computer scientist or an expert in machine learning. But neural networks are used sometimes in data analysis in experimental physics.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;An artificial neural network is just a way for a machine to make a decision. What you&amp;#39;re basically doing is having a computer perform a statistical hypothesis test, and make a decision based on a &lt;em&gt;quantitative&lt;/em&gt; test statistic.&lt;/p&gt;\n\n&lt;p&gt;So you have to come up with an appropriate test statistic, optimized to make the neural network work as well as it can.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a very simple example of a neural network. You have some data x&lt;em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/em&gt;, where i runs from 1 to n. These are just a bunch of different quantities which potentially contain information that your network can use to make its decision. You can define a simple linear test statistic for this data as&lt;/p&gt;\n\n&lt;p&gt;t = \u03a3&lt;em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/em&gt; a&lt;em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/em&gt; x&lt;em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/em&gt;, where a&lt;em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/em&gt; are just weight coefficients. And there are ways to optimize these weight coefficients.&lt;/p&gt;\n\n&lt;p&gt;So to make that equation completely transparent, say you&amp;#39;ve got two quantities x&lt;em&gt;&lt;em&gt;1&lt;/em&gt;&lt;/em&gt; and x&lt;em&gt;&lt;em&gt;2&lt;/em&gt;&lt;/em&gt;, the equation above would become t = a&lt;em&gt;&lt;em&gt;1&lt;/em&gt;&lt;/em&gt; x&lt;em&gt;&lt;em&gt;1&lt;/em&gt;&lt;/em&gt; + a&lt;em&gt;&lt;em&gt;2&lt;/em&gt;&lt;/em&gt; x&lt;em&gt;&lt;em&gt;2&lt;/em&gt;&lt;/em&gt;, just a weighted linear sum of the two variables.&lt;/p&gt;\n\n&lt;p&gt;Anyway now you have this simple linear test statistic, which also has the advantage of being a &amp;quot;scalar&amp;quot; (meaning a single number instead of n individual numbers).&lt;/p&gt;\n\n&lt;p&gt;Now you can send this test statistic through an &lt;em&gt;activator function&lt;/em&gt;. There are many possible activator functions for neural networks, some common ones being the unit step function, the logistic sigmoid, etc.&lt;/p&gt;\n\n&lt;p&gt;For this example, let&amp;#39;s take the logistic sigmoid:&lt;/p&gt;\n\n&lt;p&gt;s(t) = 1/(1 + e&lt;sup&gt;-t&lt;/sup&gt;).&lt;/p&gt;\n\n&lt;p&gt;This is now a nonlinear transformation which you can send your data through, which will hopefully make it easier to make a decision about it (or have a computer do so).&lt;/p&gt;\n\n&lt;p&gt;In a more complicated neural network, you can add additional layers, and &amp;quot;train&amp;quot; the network to optimize all of the coefficients in each of your layers. But in principle this is already a functioning neural network.&lt;/p&gt;\n\n&lt;p&gt;Now say you have some data for all of the x&lt;em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/em&gt; and you want to know whether it came from distribution X or distribution Y. Maybe X and Y are pretty similar and you can&amp;#39;t tell just by looking at the data. You can feed the data through that nonlinear activation function above, and maybe it will be easier to tell X and Y apart.&lt;/p&gt;\n\n&lt;p&gt;So that&amp;#39;s a very basic explanation of what exactly the network is and how it works.&lt;/p&gt;\n\n&lt;p&gt;As for your specific questions, &amp;quot;learning&amp;quot; in this case would be using data that you &lt;em&gt;know&lt;/em&gt; came from X and Y to optimize the parameters (a&lt;em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/em&gt;) of the neural network. In other words it could be easier or harder to tell X data and Y data apart based on the values of the weight coefficients (a&lt;em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/em&gt;). You are &amp;quot;training&amp;quot; the network by choosing the best values of a&lt;em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/em&gt; such that X and Y are easiest to tell apart.&lt;/p&gt;\n\n&lt;p&gt;Once you&amp;#39;re confident that your neural network is functioning, you can have a computer make these decisions for you. For each data point it will calculate the test statistic, run it through the activator function, and then based on the value which pops out, it should either be closer to what you&amp;#39;d expect from distribution X or to what you&amp;#39;d expect from distribution Y. Then comparing that number to a pre-defined cutoff point, the computer will decide whether the data point you&amp;#39;ve input has come from distribution X or Y. Of course there is some error percentage, where data points will inevitably be misidentified. But part of training the network is minimizing those misidentifications.&lt;/p&gt;\n\n&lt;p&gt;Anyway, I hope this helped a little. I&amp;#39;m not a computer scientist or an expert in machine learning. But neural networks are used sometimes in data analysis in experimental physics.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5rmg5", "score_hidden": false, "stickied": false, "created": 1492027289.0, "created_utc": 1491998489.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64x6wf", "likes": null, "replies": "", "user_reports": [], "id": "dg7ax2c", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "IAMZizzi05", "parent_id": "t1_dg7a67x", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Thanks, I've got it now :) \n\nYeah, I actually came to learn about this in this very thread, so I was one of the ones who overlooked this method, but gladly learned about it here.\n\nThanks again for your follow-up! Cheers :)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, I&amp;#39;ve got it now :) &lt;/p&gt;\n\n&lt;p&gt;Yeah, I actually came to learn about this in this very thread, so I was one of the ones who overlooked this method, but gladly learned about it here.&lt;/p&gt;\n\n&lt;p&gt;Thanks again for your follow-up! Cheers :)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg7ax2c", "score_hidden": false, "stickied": false, "created": 1492099439.0, "created_utc": 1492070639.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg7a67x", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Puteh", "parent_id": "t1_dg795ye", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "For example. Suppose you want your network to model how y (output value) depends on x[1], x[2], .. x[N] (input values).\n\nTo train the network, you need to show it a reasonable range of each of the x[i] inputs, which typically are scaled to fall in the range 0 to 1. Suppose you choose/have training data in which each input variable = 0.1, 0.3, 0.5, 0.7 and 0.9 (5 values in all). There are N such variables, all independent, so you need 5x5x5... up to N training samples in total in order to ensure that you have sampled all possible combinations of the variables (to within +/-0.1).\n\nIt's a simple idea, but often overlooked by people who use NNs as black boxes.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For example. Suppose you want your network to model how y (output value) depends on x[1], x[2], .. x[N] (input values).&lt;/p&gt;\n\n&lt;p&gt;To train the network, you need to show it a reasonable range of each of the x[i] inputs, which typically are scaled to fall in the range 0 to 1. Suppose you choose/have training data in which each input variable = 0.1, 0.3, 0.5, 0.7 and 0.9 (5 values in all). There are N such variables, all independent, so you need 5x5x5... up to N training samples in total in order to ensure that you have sampled all possible combinations of the variables (to within +/-0.1).&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a simple idea, but often overlooked by people who use NNs as black boxes.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg7a67x", "score_hidden": false, "stickied": false, "created": 1492097316.0, "created_utc": 1492068516.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg795ye", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "IAMZizzi05", "parent_id": "t1_dg6jx6i", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Basically, would that mean that for each sample there's multiple samples within, each one based on each layer? I'm not sure I'm getting this right, but if that's so, it's more complex than I thought.\n\nThanks for your reply!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Basically, would that mean that for each sample there&amp;#39;s multiple samples within, each one based on each layer? I&amp;#39;m not sure I&amp;#39;m getting this right, but if that&amp;#39;s so, it&amp;#39;s more complex than I thought.&lt;/p&gt;\n\n&lt;p&gt;Thanks for your reply!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg795ye", "score_hidden": false, "stickied": false, "created": 1492094613.0, "created_utc": 1492065813.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6jx6i", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Puteh", "parent_id": "t3_64x6wf", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "The most tricky part of coding a multilayer perceptron (MLP) is updating the coefficients during training. It is quite a time-demanding optimization process, and should be approached with the Marquardt algorithm or similar (see Numerical recipes in Physics). Many books discuss the MLP in terms of the easily programmed backpropagation algorithm, but this not efficient for large networks.\n\nTo my mind the MLP is a glorified procedure for multi-variable non-linear regression, somewhat akin to fitting (say) a Fourier series or wavelet series. MLPs are usually best suited for large data sets, since they often span a large function space that must be properly explored during the training process. For example, if you have 5 inputs mapped to one ouput, you are going to need N^5 sets of training data, where N = 5 to 10 reflects the coarseness with which you wish to sample in each variable dimension.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The most tricky part of coding a multilayer perceptron (MLP) is updating the coefficients during training. It is quite a time-demanding optimization process, and should be approached with the Marquardt algorithm or similar (see Numerical recipes in Physics). Many books discuss the MLP in terms of the easily programmed backpropagation algorithm, but this not efficient for large networks.&lt;/p&gt;\n\n&lt;p&gt;To my mind the MLP is a glorified procedure for multi-variable non-linear regression, somewhat akin to fitting (say) a Fourier series or wavelet series. MLPs are usually best suited for large data sets, since they often span a large function space that must be properly explored during the training process. For example, if you have 5 inputs mapped to one ouput, you are going to need N&lt;sup&gt;5&lt;/sup&gt; sets of training data, where N = 5 to 10 reflects the coarseness with which you wish to sample in each variable dimension.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6jx6i", "score_hidden": false, "stickied": false, "created": 1492059597.0, "created_utc": 1492030797.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]