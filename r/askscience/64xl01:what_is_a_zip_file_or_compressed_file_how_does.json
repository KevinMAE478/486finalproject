[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "askscience", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand the basic concept. It compresses the data to use less drive space. But how does it do that? How does my folder&amp;#39;s data become smaller? Where does the &amp;quot;extra&amp;quot; or non-compressed data go? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "I understand the basic concept. It compresses the data to use less drive space. But how does it do that? How does my folder's data become smaller? Where does the \"extra\" or non-compressed data go? ", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Computing", "id": "64xl01", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 8958, "report_reasons": null, "author": "TheRaven1", "saved": false, "mod_reports": [], "name": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "approved_by": null, "over_18": false, "domain": "self.askscience", "hidden": false, "thumbnail": "", "subreddit_id": "t5_2qm4e", "edited": false, "link_flair_css_class": "computing", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/askscience/comments/64xl01/what_is_a_zip_file_or_compressed_file_how_does/", "num_reports": null, "locked": false, "stickied": false, "created": 1492025739.0, "url": "https://www.reddit.com/r/askscience/comments/64xl01/what_is_a_zip_file_or_compressed_file_how_does/", "author_flair_text": null, "quarantine": false, "title": "What is a \"zip file\" or \"compressed file?\" How does formatting it that way compress it and what is compressing?", "created_utc": 1491996939.0, "distinguished": null, "media": null, "upvote_ratio": 0.86, "num_comments": 556, "visited": false, "subreddit_type": "public", "ups": 8958}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg726up", "gilded": 0, "archived": false, "score": 10, "report_reasons": null, "author": "PropgandaNZ", "parent_id": "t1_dg6rfdu", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Because a change in result code equals a switch in value (from 1 to 2) only works in binary format ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because a change in result code equals a switch in value (from 1 to 2) only works in binary format &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg726up", "score_hidden": false, "stickied": false, "created": 1492082113.0, "created_utc": 1492053313.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 10}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6rfdu", "gilded": 0, "archived": false, "score": 13, "report_reasons": null, "author": "Cyber_Cheese", "parent_id": "t1_dg696zm", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Something i didn't pick up immediately - this works because it only alternates between 2s and 1s. You're throwing out the individual data and purely recording how long each group of numbers is.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Something i didn&amp;#39;t pick up immediately - this works because it only alternates between 2s and 1s. You&amp;#39;re throwing out the individual data and purely recording how long each group of numbers is.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6rfdu", "score_hidden": false, "stickied": false, "created": 1492068709.0, "created_utc": 1492039909.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 13}}], "after": null, "before": null}}, "user_reports": [], "id": "dg696zm", "gilded": 0, "archived": false, "score": 101, "report_reasons": null, "author": "Kebble", "parent_id": "t1_dg66vnu", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Shamelessly copying [this comment](https://www.reddit.com/r/math/comments/3hgxp6/a000002_kolakoski_sequence_discussion/cu7e057/) before it's the one that made it click for me:\n\n&gt;I've grouped the \"runs\" for you:\n\n&gt;      1   2 2   1 1   2   1   2 2   1   2 2   1 1   2 ...\n    \\-/ \\---/ \\---/ \\-/ \\-/ \\---/ \\-/ \\---/ \\---/ \\-/\n     1    2     2    1   1    2    1    2     2    1 ...\n\n&gt;The top sequence equals the bottom sequence; this is the Kolakoski sequence.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Shamelessly copying &lt;a href=\"https://www.reddit.com/r/math/comments/3hgxp6/a000002_kolakoski_sequence_discussion/cu7e057/\"&gt;this comment&lt;/a&gt; before it&amp;#39;s the one that made it click for me:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I&amp;#39;ve grouped the &amp;quot;runs&amp;quot; for you:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt; 1   2 2   1 1   2   1   2 2   1   2 2   1 1   2 ...\n\\-/ \\---/ \\---/ \\-/ \\-/ \\---/ \\-/ \\---/ \\---/ \\-/\n 1    2     2    1   1    2    1    2     2    1 ...\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The top sequence equals the bottom sequence; this is the Kolakoski sequence.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg696zm", "score_hidden": false, "stickied": false, "created": 1492048516.0, "created_utc": 1492019716.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 101}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg68alz", "gilded": 0, "archived": false, "score": 24, "report_reasons": null, "author": "ThatDeadDude", "parent_id": "t1_dg66vnu", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Because the sequence only has two symbols it is not necessary to include the symbol in the RLE. Instead, the numbers only give the length of each run before a change in symbol.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because the sequence only has two symbols it is not necessary to include the symbol in the RLE. Instead, the numbers only give the length of each run before a change in symbol.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg68alz", "score_hidden": false, "stickied": false, "created": 1492047613.0, "created_utc": 1492018813.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 24}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg66vnu", "gilded": 0, "archived": false, "score": 49, "report_reasons": null, "author": "okraOkra", "parent_id": "t1_dg655w3", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "can you elaborate on this? do you mean the sequence is a fixed point of a RLE compression algorithm? this isn't obvious to me; how can I see this?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;can you elaborate on this? do you mean the sequence is a fixed point of a RLE compression algorithm? this isn&amp;#39;t obvious to me; how can I see this?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg66vnu", "score_hidden": false, "stickied": false, "created": 1492046198.0, "created_utc": 1492017398.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 49}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6tv1j", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "voltagex", "parent_id": "t1_dg6qwwc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Isn't that more due to there being problems with the way the original Zip specification was written?\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Isn&amp;#39;t that more due to there being problems with the way the original Zip specification was written?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6tv1j", "score_hidden": false, "stickied": false, "created": 1492071728.0, "created_utc": 1492042928.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6qwwc", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "mandragara", "parent_id": "t1_dg655w3", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "There's also a zip file out there that decompresses to a copy of itself", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s also a zip file out there that decompresses to a copy of itself&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6qwwc", "score_hidden": false, "stickied": false, "created": 1492068087.0, "created_utc": 1492039287.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg655w3", "gilded": 0, "archived": false, "score": 212, "report_reasons": null, "author": "Kebble", "parent_id": "t1_dg5ykog", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "I'm also fond of the [Kolakoski sequence](https://en.wikipedia.org/wiki/Kolakoski_sequence) that is its own run-length encoding", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m also fond of the &lt;a href=\"https://en.wikipedia.org/wiki/Kolakoski_sequence\"&gt;Kolakoski sequence&lt;/a&gt; that is its own run-length encoding&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg655w3", "score_hidden": false, "stickied": false, "created": 1492044437.0, "created_utc": 1492015637.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 212}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg7cu14", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "coolkid1717", "parent_id": "t1_dg6ri2z", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "No, they're professional terms for expediating hadndjobs. Good luck getting a full length stroke with tips that are unmatched in girth or height.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No, they&amp;#39;re professional terms for expediating hadndjobs. Good luck getting a full length stroke with tips that are unmatched in girth or height.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg7cu14", "score_hidden": false, "stickied": false, "created": 1492105270.0, "created_utc": 1492076470.0, "depth": 9, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6ri2z", "gilded": 0, "archived": false, "score": 25, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dg6l80b", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "[removed]", "edited": 1492224474.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6ri2z", "score_hidden": false, "stickied": false, "created": 1492068800.0, "created_utc": 1492040000.0, "depth": 8, "mod_reports": [], "subreddit_type": "public", "ups": 25}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6l80b", "gilded": 0, "archived": false, "score": 26, "report_reasons": null, "author": "toastofferson", "parent_id": "t1_dg6k6y3", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Some constriction algorithms allow for a change in girth however these algorithms move slower on the compression stroke to prevent tip decoupling.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Some constriction algorithms allow for a change in girth however these algorithms move slower on the compression stroke to prevent tip decoupling.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6l80b", "score_hidden": false, "stickied": false, "created": 1492061093.0, "created_utc": 1492032293.0, "depth": 7, "mod_reports": [], "subreddit_type": "public", "ups": 26}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6k6y3", "gilded": 0, "archived": false, "score": 34, "report_reasons": null, "author": "ImRodILikeToParty", "parent_id": "t1_dg6dk27", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Would girth affect the compatibility?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Would girth affect the compatibility?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6k6y3", "score_hidden": false, "stickied": false, "created": 1492059906.0, "created_utc": 1492031106.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 34}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6dk27", "gilded": 0, "archived": false, "score": 82, "report_reasons": null, "author": "toastofferson", "parent_id": "t1_dg6avm6", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "These can be compressed further by putting two tips together and working from the middle out. However, one must consider the floor to tip ratio when finding compatibility.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;These can be compressed further by putting two tips together and working from the middle out. However, one must consider the floor to tip ratio when finding compatibility.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6dk27", "score_hidden": false, "stickied": false, "created": 1492053010.0, "created_utc": 1492024210.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 82}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6avm6", "gilded": 0, "archived": false, "score": 54, "report_reasons": null, "author": "eugesd", "parent_id": "t1_dg68xth", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Pied piper?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pied piper?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6avm6", "score_hidden": false, "stickied": false, "created": 1492050248.0, "created_utc": 1492021448.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 54}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg68xth", "gilded": 0, "archived": false, "score": 74, "report_reasons": null, "author": "Weirfish", "parent_id": "t1_dg68bvk", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "For the sake of clarity, I'll delimit it a bit more. A pipe `|` separates the number of values and the value, and a semicolon `;` separates number-value pairs. So the examples given would be \n\n* `1` -&gt; \"There is one 1\" -&gt; `1|1;`\n* `11` -&gt; \"There are two 1's\" -&gt; `2|1;`\n* `21` -&gt; \"There is one 2 and one 1\" -&gt; `1|2;1|1;`\n* `1211` -&gt; \"There is one 1, one 2, and two 1's\" -&gt; `1|1;1|2;2|1;`\n\nConsider the example `1111111111111111112222222222`. This would compress to `18|1;10|2;` which is a lot shorter.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For the sake of clarity, I&amp;#39;ll delimit it a bit more. A pipe &lt;code&gt;|&lt;/code&gt; separates the number of values and the value, and a semicolon &lt;code&gt;;&lt;/code&gt; separates number-value pairs. So the examples given would be &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;1&lt;/code&gt; -&amp;gt; &amp;quot;There is one 1&amp;quot; -&amp;gt; &lt;code&gt;1|1;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;11&lt;/code&gt; -&amp;gt; &amp;quot;There are two 1&amp;#39;s&amp;quot; -&amp;gt; &lt;code&gt;2|1;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;21&lt;/code&gt; -&amp;gt; &amp;quot;There is one 2 and one 1&amp;quot; -&amp;gt; &lt;code&gt;1|2;1|1;&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;1211&lt;/code&gt; -&amp;gt; &amp;quot;There is one 1, one 2, and two 1&amp;#39;s&amp;quot; -&amp;gt; &lt;code&gt;1|1;1|2;2|1;&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Consider the example &lt;code&gt;1111111111111111112222222222&lt;/code&gt;. This would compress to &lt;code&gt;18|1;10|2;&lt;/code&gt; which is a lot shorter.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg68xth", "score_hidden": false, "stickied": false, "created": 1492048255.0, "created_utc": 1492019455.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 74}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6blpm", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "Ardub23", "parent_id": "t1_dg68bvk", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Nope, [it keeps growing longer forever](https://en.wikipedia.org/wiki/Look-and-say_sequence#Growth) unless the starting value is 22.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nope, &lt;a href=\"https://en.wikipedia.org/wiki/Look-and-say_sequence#Growth\"&gt;it keeps growing longer forever&lt;/a&gt; unless the starting value is 22.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6blpm", "score_hidden": false, "stickied": false, "created": 1492050982.0, "created_utc": 1492022182.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg68bvk", "gilded": 0, "archived": false, "score": 19, "report_reasons": null, "author": "davidgro", "parent_id": "t1_dg5ykog", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Does it ever successfully 'compress'?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does it ever successfully &amp;#39;compress&amp;#39;?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg68bvk", "score_hidden": false, "stickied": false, "created": 1492047647.0, "created_utc": 1492018847.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 19}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg5ykog", "gilded": 0, "archived": false, "score": 808, "report_reasons": null, "author": "giltwist", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt; In some special cases, the filesize actually increases due to the rules I introduced to properly represent numbers and slashes.\n\nA great example of this is the Conway or \"See-it-say-it\" sequence.\n\n* 1 -&gt; \"There is one 1\" -&gt; 11\n* 11 -&gt; \"There are two 1's\" -&gt; 21\n* 21 -&gt; \"There is one 2 and one 1\" -&gt; 1211\n* 1211 -&gt; \"There is one 1, one 2, and two 1's\" -&gt; 111221", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;In some special cases, the filesize actually increases due to the rules I introduced to properly represent numbers and slashes.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;A great example of this is the Conway or &amp;quot;See-it-say-it&amp;quot; sequence.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;1 -&amp;gt; &amp;quot;There is one 1&amp;quot; -&amp;gt; 11&lt;/li&gt;\n&lt;li&gt;11 -&amp;gt; &amp;quot;There are two 1&amp;#39;s&amp;quot; -&amp;gt; 21&lt;/li&gt;\n&lt;li&gt;21 -&amp;gt; &amp;quot;There is one 2 and one 1&amp;quot; -&amp;gt; 1211&lt;/li&gt;\n&lt;li&gt;1211 -&amp;gt; &amp;quot;There is one 1, one 2, and two 1&amp;#39;s&amp;quot; -&amp;gt; 111221&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5ykog", "score_hidden": false, "stickied": false, "created": 1492037285.0, "created_utc": 1492008485.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 808}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg79uxg", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "Roach-less", "parent_id": "t1_dg6l3kv", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "N mst css th abv wld b a cls engh apprxmtn t rtrn a sn rnfltd mssg, bt sm infrmtn wld b lst.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;N mst css th abv wld b a cls engh apprxmtn t rtrn a sn rnfltd mssg, bt sm infrmtn wld b lst.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg79uxg", "score_hidden": false, "stickied": false, "created": 1492096447.0, "created_utc": 1492067647.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6l3kv", "gilded": 0, "archived": false, "score": 29, "report_reasons": null, "author": "Avenage", "parent_id": "t1_dg6imkr", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "A simple form of lossy compression would be to convert any uppercase character to lowercase to reduce the alphabet size and therefore the number of bits required to store a character.\n\nOr another example could be to change words for numbers into their number form i.e. three becomes 3. \n\nIn most cases the above would be a close enough approximation to return a sane reinflated message, but some information would be lost.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A simple form of lossy compression would be to convert any uppercase character to lowercase to reduce the alphabet size and therefore the number of bits required to store a character.&lt;/p&gt;\n\n&lt;p&gt;Or another example could be to change words for numbers into their number form i.e. three becomes 3. &lt;/p&gt;\n\n&lt;p&gt;In most cases the above would be a close enough approximation to return a sane reinflated message, but some information would be lost.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6l3kv", "score_hidden": false, "stickied": false, "created": 1492060946.0, "created_utc": 1492032146.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 29}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg7ac8b", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "realfuzzhead", "parent_id": "t1_dg6xnqu", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "\"Information Coefficient\" is another way of thinking of the information-theoretic concept on [entropy](https://en.wikipedia.org/wiki/Entropy_\\(information_theory\\)). Surprisingly, human language is not too dense with information, a result that Claude Shannon (father of the field) showed is some of his [early work](http://people.seas.harvard.edu/~jones/cscie129/papers/stanford_info_paper/entropy_of_english_9.htm) (English is around 50% redundant). ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Information Coefficient&amp;quot; is another way of thinking of the information-theoretic concept on &lt;a href=\"https://en.wikipedia.org/wiki/Entropy_(information_theory)\"&gt;entropy&lt;/a&gt;. Surprisingly, human language is not too dense with information, a result that Claude Shannon (father of the field) showed is some of his &lt;a href=\"http://people.seas.harvard.edu/%7Ejones/cscie129/papers/stanford_info_paper/entropy_of_english_9.htm\"&gt;early work&lt;/a&gt; (English is around 50% redundant). &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg7ac8b", "score_hidden": false, "stickied": false, "created": 1492097786.0, "created_utc": 1492068986.0, "depth": 8, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6xnqu", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "EyeBreakThings", "parent_id": "t1_dg6x0k9", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Ahh, I get what you are getting at now.  I'd go so far as to say text has a fairly high \"information coefficient\".  A lot of info / small size. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ahh, I get what you are getting at now.  I&amp;#39;d go so far as to say text has a fairly high &amp;quot;information coefficient&amp;quot;.  A lot of info / small size. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6xnqu", "score_hidden": false, "stickied": false, "created": 1492076331.0, "created_utc": 1492047531.0, "depth": 7, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6x0k9", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "HoopyHobo", "parent_id": "t1_dg6vfc8", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Oh sure, text files can grow to be quite large, but they have to contain a lot of actual text to get large. That's all I mean when I say text doesn't take up much data.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh sure, text files can grow to be quite large, but they have to contain a lot of actual text to get large. That&amp;#39;s all I mean when I say text doesn&amp;#39;t take up much data.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6x0k9", "score_hidden": false, "stickied": false, "created": 1492075541.0, "created_utc": 1492046741.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6x377", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "georgeeking", "parent_id": "t1_dg6vfc8", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "You're talking about computer generated logs? In which case wouldn't it be easier to encode the original log message more efficiently rather than lossily  compress the message itself. You could replace all the messages used, with numbers (like error codes) and then have any variables in the message follow the code. The code would refer to a dictionary of descriptive messages. Actually, typing that out I realised that the same would be accomplished automatically by some the lossless compression algorithms mentioned above. Point being surely the vast majority of the gains on something as predictably repetitive as a log would be lossless anyway. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;re talking about computer generated logs? In which case wouldn&amp;#39;t it be easier to encode the original log message more efficiently rather than lossily  compress the message itself. You could replace all the messages used, with numbers (like error codes) and then have any variables in the message follow the code. The code would refer to a dictionary of descriptive messages. Actually, typing that out I realised that the same would be accomplished automatically by some the lossless compression algorithms mentioned above. Point being surely the vast majority of the gains on something as predictably repetitive as a log would be lossless anyway. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6x377", "score_hidden": false, "stickied": false, "created": 1492075630.0, "created_utc": 1492046830.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6vfc8", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "EyeBreakThings", "parent_id": "t1_dg6tuwg", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "I'm going to argue about text (not your actual point, just that text isn't big enough).  Logs.  Logs get huge.  They get especially huge when set to verbose, which usually means they are important.  A PBX log for a call center can get to multiple GBs pretty damn quick.  ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m going to argue about text (not your actual point, just that text isn&amp;#39;t big enough).  Logs.  Logs get huge.  They get especially huge when set to verbose, which usually means they are important.  A PBX log for a call center can get to multiple GBs pretty damn quick.  &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6vfc8", "score_hidden": false, "stickied": false, "created": 1492073572.0, "created_utc": 1492044772.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6tuwg", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "HoopyHobo", "parent_id": "t1_dg6imkr", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Yes, you could come up with lossy compression schemes for lots of things besides multimedia files, it's just that in practice you run into questions like how difficult is the compression algorithm to build and run, how do you measure what is an acceptable amount of data loss, and is the amount of data saved even worth it. Text takes up so little data to begin with that there's pretty much no pressure on anyone to develop lossy compression techniques for it, so I believe it's still mostly just a topic of academic interest. Wikipedia's article on lossy compression does include a link to[ this paper from 1994](http://compression.ru/download/articles/text/witten_1994cj_lossy_text_compression.pdf) about lossy English text compression.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, you could come up with lossy compression schemes for lots of things besides multimedia files, it&amp;#39;s just that in practice you run into questions like how difficult is the compression algorithm to build and run, how do you measure what is an acceptable amount of data loss, and is the amount of data saved even worth it. Text takes up so little data to begin with that there&amp;#39;s pretty much no pressure on anyone to develop lossy compression techniques for it, so I believe it&amp;#39;s still mostly just a topic of academic interest. Wikipedia&amp;#39;s article on lossy compression does include a link to&lt;a href=\"http://compression.ru/download/articles/text/witten_1994cj_lossy_text_compression.pdf\"&gt; this paper from 1994&lt;/a&gt; about lossy English text compression.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6tuwg", "score_hidden": false, "stickied": false, "created": 1492071723.0, "created_utc": 1492042923.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6imkr", "gilded": 0, "archived": false, "score": 20, "report_reasons": null, "author": "TheoryOfSomething", "parent_id": "t1_dg6f85b", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "I bet you could lossily compress English text. That's essentially what text message shorthand is. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I bet you could lossily compress English text. That&amp;#39;s essentially what text message shorthand is. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6imkr", "score_hidden": false, "stickied": false, "created": 1492058214.0, "created_utc": 1492029414.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 20}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6uly3", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "Rafv", "parent_id": "t1_dg6f85b", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Time-series databases are also big on lossy compression. Time-series databases (OSIsoft PI, AspenTech IP21, etc) let you define flat bounds as well as general linear bounds where deviation from a flatline or linear trend is not significant. So if you have 100 data points but they're all in a line, you only need to store 2 points: the first and the last.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Time-series databases are also big on lossy compression. Time-series databases (OSIsoft PI, AspenTech IP21, etc) let you define flat bounds as well as general linear bounds where deviation from a flatline or linear trend is not significant. So if you have 100 data points but they&amp;#39;re all in a line, you only need to store 2 points: the first and the last.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6uly3", "score_hidden": false, "stickied": false, "created": 1492072615.0, "created_utc": 1492043815.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6f85b", "gilded": 0, "archived": false, "score": 36, "report_reasons": null, "author": "HoopyHobo", "parent_id": "t1_dg60pjr", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Any method of compression such as ZIP where you don't know what kind of data you're going to be compressing has to be lossless. Lossy compression requires knowing what the data represents so that you can ensure the end result actually resembles the uncompressed version. Images, audio, and video are really the only kinds of data than can be lossily compressed.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Any method of compression such as ZIP where you don&amp;#39;t know what kind of data you&amp;#39;re going to be compressing has to be lossless. Lossy compression requires knowing what the data represents so that you can ensure the end result actually resembles the uncompressed version. Images, audio, and video are really the only kinds of data than can be lossily compressed.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6f85b", "score_hidden": false, "stickied": false, "created": 1492054722.0, "created_utc": 1492025922.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 36}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6i0bh", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "ScrewAttackThis", "parent_id": "t1_dg60pjr", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "DEFLATE is LZ77 and Huffman Coding.  It's two forms of compression that can reduce repeating data in different ways.  Since you already went over LZ77, it's probably important to talk about Huffman Coding.  This isn't exactly ELI5, but here goes:\n\n&gt; AAAAABBBBCCCDDE\n\nThe above string can be compressed by identifying symbols that are being used repeatedly and creating a \"code\" for each based on their frequency.  So if we count each symbol and how many times it appears, we end up with a frequency table that looks like:\n\n| Symbol | Frequency |\n|--------|-----------|\n| A      | 5         |\n| B      | 4         |\n| C      | 3         |\n| D      | 2         |\n| E      | 1         |\n\nThe next step is to use this information to create a data structure known as a binary tree.  A binary tree is called that because 1) it looks like a tree and 2) each \"node\" has no more than 2 children nodes.  That means starting at the top node (root node), you can only ever go left or right.  The tree is constructed as such that the most frequently used nodes are closest to the root, while the least frequently used are farther.  Each symbol is considered a \"leaf\" node, meaning there are no children.\n\nBuilding this tree is half the fun/work of the algorithm.  To do this, we use another data structure known as a priority queue.  Simply explained, this is like having a group of people stand in line (queue) ordered by how young they are (priority).  It doesn't matter when they show up, if they're the youngest person then they go to the front of the line while the oldest is in the back.\n\nSo to build the tree:  You initialize each symbol with its frequency as a node.  The node has no children, yet.  This makes our queue look like:\n\n    (E, 1), (D, 2), (C, 3), (B, 4), (A, 5)\n\nWe grab the first two and create a new node that only holds the sum of their frequencies.  Now we have a tree with 3 nodes total.  We add this back to the queue.  Remember, adding an item to this queue will place it in the correct order.  Now we just repeat this process like so:\n\n    1. (E, 1), (D, 2), (C, 3), (B, 4), (A, 5)\n    2. (3, (E, 1), (D, 2)), (C, 3), (B, 4), (A, 5)\n    3. (B, 4), (A, 5), (6, (3, (E, 1), (D, 2)), (C, 3))\n    4. (6, (3, (E, 1), (D, 2)), (C, 3)), (9, (B, 4), (A, 5))\n    5. (15, (6, (3, (E, 1), (D, 2)), (C, 3)), (9, (B, 4), (A, 5)))\n\nSorry that the textual representation isn't exactly clear.   But the final result looks like this: http://imgur.com/a/PA1td.  Side note: If the image doesn't exactly line up with my textual representation, that's fine.  Trees can generate differently when you end up with two symbols sharing the same frequency, but it doesn't really matter.\n\nNow here's where the beauty of the algorithm comes in.  This tree efficiently lets us creating codings for each symbol so that they're the smallest possible **without** being difficult to read (as in, there isn't ambiguity).  This is mathematically proven.  If you want to create a coding like this, Huffman's algorithm is it.\n\nTo do this, we simply start at the root node and move to each leaf node, counting our moves along the way.  Going \"left\" is a 0, going \"right\" is a 1.  This produces an encoding table like so (using the image above):\n\n| Symbol | Code |\n|--------|-----------|\n| A      | 11         |\n| B      | 10         |\n| C      | 01         |\n| D      | 001         |\n| E      | 000         |\n\nUsing this, we encode our original string `AAAAABBBBCCCDDE` to `1111111111110101010010101001001000`.  Hey, wait, why is that longer?  Well, good question.  The trick is that it's actually shorter as far as a computer is concerned.  To represent `A` in ASCII actually requires ~1 byte (8 1s and 0s).  We reduced each occurence of `A` to only two bits `11`.  So overall, we just compressed `AAAAABBBBCCCDDE` from 15 * 8 = 120 bits to only 33 bits.  That's a pretty massive difference.\n\nThe drawback is that in order to decode `1111111111110101010010101001001000`, we need the tree we generated.  So unfortunately for small strings, you could potentially increase the size once you factor this in.  But with the tree, we simply read the encoded string from left to right in order to decode the bits back to their symbols.  So using the same image as before (http://imgur.com/a/PA1td) we can use it as a map.  For every bit, we go left or right.  When we get to a \"leaf\" node, we write that down and start back at the root.  Before we know it, we have our original string again!\n\nSo how does LZ77 and Huffman fit together to form DEFLATE?  Well, I'm not 100% certain on those details.  I understand Huffman coding many times more than LZ77.  I believe the general process is that LZ77 reduces repeating strings as demonstrated above, and then those individual symbols are compressed with Huffman.  There's a little more to the algorithm that's necessary like how all of the information needed to properly decompress is stored and passed along.  But this is more structure related and kinda uninteresting.\n\nFinal note: I want to emphasize something I said above.  \"This tree efficiently lets us creating codings for each symbol so that they're the smallest possible **without** being difficult to read (as in, there isn't ambiguity).\"  This is the key to huffman coding and why I find it such a cool algorithm.  If you look at the codes for each symbol, there's a special property that they have.  No whole symbol is a prefix of another.  What this means is that the code `A:11` does not exist in the start of any other code.  This means that reading the bits left to right, there's never any confusion while following the tree.  A code that would violate this property would be something like `Z:1` or `Y:00` since we couldn't tell if the stream of bits `001` is supposed to be `YZ` or `D`.\n\ne: Oh, this is /r/askscience and not /r/eli5", "edited": 1492031818.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;DEFLATE is LZ77 and Huffman Coding.  It&amp;#39;s two forms of compression that can reduce repeating data in different ways.  Since you already went over LZ77, it&amp;#39;s probably important to talk about Huffman Coding.  This isn&amp;#39;t exactly ELI5, but here goes:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;AAAAABBBBCCCDDE&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The above string can be compressed by identifying symbols that are being used repeatedly and creating a &amp;quot;code&amp;quot; for each based on their frequency.  So if we count each symbol and how many times it appears, we end up with a frequency table that looks like:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Symbol&lt;/th&gt;\n&lt;th&gt;Frequency&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;B&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;C&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;D&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;E&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;The next step is to use this information to create a data structure known as a binary tree.  A binary tree is called that because 1) it looks like a tree and 2) each &amp;quot;node&amp;quot; has no more than 2 children nodes.  That means starting at the top node (root node), you can only ever go left or right.  The tree is constructed as such that the most frequently used nodes are closest to the root, while the least frequently used are farther.  Each symbol is considered a &amp;quot;leaf&amp;quot; node, meaning there are no children.&lt;/p&gt;\n\n&lt;p&gt;Building this tree is half the fun/work of the algorithm.  To do this, we use another data structure known as a priority queue.  Simply explained, this is like having a group of people stand in line (queue) ordered by how young they are (priority).  It doesn&amp;#39;t matter when they show up, if they&amp;#39;re the youngest person then they go to the front of the line while the oldest is in the back.&lt;/p&gt;\n\n&lt;p&gt;So to build the tree:  You initialize each symbol with its frequency as a node.  The node has no children, yet.  This makes our queue look like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;(E, 1), (D, 2), (C, 3), (B, 4), (A, 5)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;We grab the first two and create a new node that only holds the sum of their frequencies.  Now we have a tree with 3 nodes total.  We add this back to the queue.  Remember, adding an item to this queue will place it in the correct order.  Now we just repeat this process like so:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;1. (E, 1), (D, 2), (C, 3), (B, 4), (A, 5)\n2. (3, (E, 1), (D, 2)), (C, 3), (B, 4), (A, 5)\n3. (B, 4), (A, 5), (6, (3, (E, 1), (D, 2)), (C, 3))\n4. (6, (3, (E, 1), (D, 2)), (C, 3)), (9, (B, 4), (A, 5))\n5. (15, (6, (3, (E, 1), (D, 2)), (C, 3)), (9, (B, 4), (A, 5)))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Sorry that the textual representation isn&amp;#39;t exactly clear.   But the final result looks like this: &lt;a href=\"http://imgur.com/a/PA1td\"&gt;http://imgur.com/a/PA1td&lt;/a&gt;.  Side note: If the image doesn&amp;#39;t exactly line up with my textual representation, that&amp;#39;s fine.  Trees can generate differently when you end up with two symbols sharing the same frequency, but it doesn&amp;#39;t really matter.&lt;/p&gt;\n\n&lt;p&gt;Now here&amp;#39;s where the beauty of the algorithm comes in.  This tree efficiently lets us creating codings for each symbol so that they&amp;#39;re the smallest possible &lt;strong&gt;without&lt;/strong&gt; being difficult to read (as in, there isn&amp;#39;t ambiguity).  This is mathematically proven.  If you want to create a coding like this, Huffman&amp;#39;s algorithm is it.&lt;/p&gt;\n\n&lt;p&gt;To do this, we simply start at the root node and move to each leaf node, counting our moves along the way.  Going &amp;quot;left&amp;quot; is a 0, going &amp;quot;right&amp;quot; is a 1.  This produces an encoding table like so (using the image above):&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Symbol&lt;/th&gt;\n&lt;th&gt;Code&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;11&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;B&lt;/td&gt;\n&lt;td&gt;10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;C&lt;/td&gt;\n&lt;td&gt;01&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;D&lt;/td&gt;\n&lt;td&gt;001&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;E&lt;/td&gt;\n&lt;td&gt;000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Using this, we encode our original string &lt;code&gt;AAAAABBBBCCCDDE&lt;/code&gt; to &lt;code&gt;1111111111110101010010101001001000&lt;/code&gt;.  Hey, wait, why is that longer?  Well, good question.  The trick is that it&amp;#39;s actually shorter as far as a computer is concerned.  To represent &lt;code&gt;A&lt;/code&gt; in ASCII actually requires ~1 byte (8 1s and 0s).  We reduced each occurence of &lt;code&gt;A&lt;/code&gt; to only two bits &lt;code&gt;11&lt;/code&gt;.  So overall, we just compressed &lt;code&gt;AAAAABBBBCCCDDE&lt;/code&gt; from 15 * 8 = 120 bits to only 33 bits.  That&amp;#39;s a pretty massive difference.&lt;/p&gt;\n\n&lt;p&gt;The drawback is that in order to decode &lt;code&gt;1111111111110101010010101001001000&lt;/code&gt;, we need the tree we generated.  So unfortunately for small strings, you could potentially increase the size once you factor this in.  But with the tree, we simply read the encoded string from left to right in order to decode the bits back to their symbols.  So using the same image as before (&lt;a href=\"http://imgur.com/a/PA1td\"&gt;http://imgur.com/a/PA1td&lt;/a&gt;) we can use it as a map.  For every bit, we go left or right.  When we get to a &amp;quot;leaf&amp;quot; node, we write that down and start back at the root.  Before we know it, we have our original string again!&lt;/p&gt;\n\n&lt;p&gt;So how does LZ77 and Huffman fit together to form DEFLATE?  Well, I&amp;#39;m not 100% certain on those details.  I understand Huffman coding many times more than LZ77.  I believe the general process is that LZ77 reduces repeating strings as demonstrated above, and then those individual symbols are compressed with Huffman.  There&amp;#39;s a little more to the algorithm that&amp;#39;s necessary like how all of the information needed to properly decompress is stored and passed along.  But this is more structure related and kinda uninteresting.&lt;/p&gt;\n\n&lt;p&gt;Final note: I want to emphasize something I said above.  &amp;quot;This tree efficiently lets us creating codings for each symbol so that they&amp;#39;re the smallest possible &lt;strong&gt;without&lt;/strong&gt; being difficult to read (as in, there isn&amp;#39;t ambiguity).&amp;quot;  This is the key to huffman coding and why I find it such a cool algorithm.  If you look at the codes for each symbol, there&amp;#39;s a special property that they have.  No whole symbol is a prefix of another.  What this means is that the code &lt;code&gt;A:11&lt;/code&gt; does not exist in the start of any other code.  This means that reading the bits left to right, there&amp;#39;s never any confusion while following the tree.  A code that would violate this property would be something like &lt;code&gt;Z:1&lt;/code&gt; or &lt;code&gt;Y:00&lt;/code&gt; since we couldn&amp;#39;t tell if the stream of bits &lt;code&gt;001&lt;/code&gt; is supposed to be &lt;code&gt;YZ&lt;/code&gt; or &lt;code&gt;D&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;e: Oh, this is &lt;a href=\"/r/askscience\"&gt;/r/askscience&lt;/a&gt; and not &lt;a href=\"/r/eli5\"&gt;/r/eli5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6i0bh", "score_hidden": false, "stickied": false, "created": 1492057565.0, "created_utc": 1492028765.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 9}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6g2t3", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "wildjokers", "parent_id": "t1_dg60pjr", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It is \"LZ\" not \"LV\".", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is &amp;quot;LZ&amp;quot; not &amp;quot;LV&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6g2t3", "score_hidden": false, "stickied": false, "created": 1492055589.0, "created_utc": 1492026789.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6ob86", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "dziban303", "parent_id": "t1_dg60pjr", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Fixing the Wikipedia links:\n\nhttps://en.wikipedia.org/wiki/Zip_(file_format)#Compression_methods\n\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fixing the Wikipedia links:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://en.wikipedia.org/wiki/Zip_(file_format)#Compression_methods\"&gt;https://en.wikipedia.org/wiki/Zip_(file_format)#Compression_methods&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://en.wikipedia.org/wiki/Entropy_(information_theory)\"&gt;https://en.wikipedia.org/wiki/Entropy_(information_theory)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6ob86", "score_hidden": false, "stickied": false, "created": 1492064860.0, "created_utc": 1492036060.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg60pjr", "gilded": 0, "archived": false, "score": 140, "report_reasons": null, "author": "ericGraves", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Zip has many different forms of lossless compression [(at least according to wikipedia)](https://en.wikipedia.org/wiki/Zip_(file_format)#Compression_methods). But the most common, deflate is based on a paper by Lempel and Ziv entitled [a universal algorithm for sequential data compression.](http://ieeexplore.ieee.org/document/1055714/) The subsequent year, they released a similar paper for [variable rate codes.](http://ieeexplore.ieee.org/document/1055934/) These algorithms are known as LV77 and LV78 respectively, and form the major basis for zip files. [Since those papers are pay-walled, here are some class notes describing the schemes pretty well.](http://math.mit.edu/~goemans/18310S15/lempel-ziv-notes.pdf)\n\nUniversal source coding can be understood as an extension of the information theoretic standpoint on compression, in specific that *there is an underlying generating distribution*. If that assumption is indeed valid, then on average nH(P) bits are required to represent n symbols in the sequence, where H is the [shannon entropy function](https://en.wikipedia.org/wiki/Entropy_(information_theory)) and P is the generating distribution. As an aside, particular LV77 really shines when the distribution can be represented by an ergodic process. Back to compression, there are many types of codes which achieve optimal compression, such as the [huffman code](https://en.wikipedia.org/wiki/Huffman_coding) or the [Shannon-Fano code](https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding). But all of these codes **require you to know the distribution**.\n\nObviously knowing the distribution is not always practically valid. Enter universal source coding which formulates the problem as \"I don't know what the underlying distribution is, but can I make a code which converges to it regardless without using too many extra symbols?\" And from the information theorists point of view, the answer is trivially yes. Take for instance a source where each symbol is generated IID according to some distribution which is initially unknown. If I were to look an the entire sequence in one go, I could simply estimate the distribution of the sequence and then use an optimal encoder that achieves the lower bound. I would still be required to also state which encoder I used. But for n symbols from a q-ary alphabet, the number of possible empirical distributions is at most n^(q), which takes at most q log(n) bits to store. Thus the overhead required to store n bits of our unknown sequence is n( H(P') + n^(-1) q log (n) ), symbols where P' converges to P asymptotically with n. And as a result we end up approaching the best possible compression rate, without knowing the source distribution to begin with.\n\n\nOf course, LV77 is a little more complicated, but really no different. LV77, more or less, uses [Zipf laws](https://en.wikipedia.org/wiki/Zipf%27s_law). More specifically, in an ergodic process the expected recurrence time of a symbol should be equal to the inverse of the probability of that symbol. And as such, the depth we need to observe to find the next symbol in the sequence should have a probability distribution that on average converges to the underlying process distribution. Lets look at an example,\n\n    AAABABBABBAABABABBBAAAB\n\nwhich LV77 parses as \n\n    A, AA, B, AB, BABBAB, BA, ABAB, ABB, BAA, AB \n\nand encodes as \n\n    (0,A),(1,1,2),(0,B),(1,2,2),(1,3,6),(1,3,2),(1,4,4),(1,8,3), (1,9,3),(1,6,2) .\n\nIn more detail, the encoder has not seen A before, and states so leaving A uncompressed. The next two entries are A, so the encoder notes first the **number of symbols since the last occurrence of the next sequence** and **how many times the sequence occurs**. It then sees B and notes it is a new symbol, and from there-on everything can be represented as conditional.\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Zip has many different forms of lossless compression &lt;a href=\"https://en.wikipedia.org/wiki/Zip_(file_format\"&gt;(at least according to wikipedia)&lt;/a&gt;#Compression_methods). But the most common, deflate is based on a paper by Lempel and Ziv entitled &lt;a href=\"http://ieeexplore.ieee.org/document/1055714/\"&gt;a universal algorithm for sequential data compression.&lt;/a&gt; The subsequent year, they released a similar paper for &lt;a href=\"http://ieeexplore.ieee.org/document/1055934/\"&gt;variable rate codes.&lt;/a&gt; These algorithms are known as LV77 and LV78 respectively, and form the major basis for zip files. &lt;a href=\"http://math.mit.edu/%7Egoemans/18310S15/lempel-ziv-notes.pdf\"&gt;Since those papers are pay-walled, here are some class notes describing the schemes pretty well.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Universal source coding can be understood as an extension of the information theoretic standpoint on compression, in specific that &lt;em&gt;there is an underlying generating distribution&lt;/em&gt;. If that assumption is indeed valid, then on average nH(P) bits are required to represent n symbols in the sequence, where H is the &lt;a href=\"https://en.wikipedia.org/wiki/Entropy_(information_theory\"&gt;shannon entropy function&lt;/a&gt;) and P is the generating distribution. As an aside, particular LV77 really shines when the distribution can be represented by an ergodic process. Back to compression, there are many types of codes which achieve optimal compression, such as the &lt;a href=\"https://en.wikipedia.org/wiki/Huffman_coding\"&gt;huffman code&lt;/a&gt; or the &lt;a href=\"https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding\"&gt;Shannon-Fano code&lt;/a&gt;. But all of these codes &lt;strong&gt;require you to know the distribution&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Obviously knowing the distribution is not always practically valid. Enter universal source coding which formulates the problem as &amp;quot;I don&amp;#39;t know what the underlying distribution is, but can I make a code which converges to it regardless without using too many extra symbols?&amp;quot; And from the information theorists point of view, the answer is trivially yes. Take for instance a source where each symbol is generated IID according to some distribution which is initially unknown. If I were to look an the entire sequence in one go, I could simply estimate the distribution of the sequence and then use an optimal encoder that achieves the lower bound. I would still be required to also state which encoder I used. But for n symbols from a q-ary alphabet, the number of possible empirical distributions is at most n&lt;sup&gt;q&lt;/sup&gt;, which takes at most q log(n) bits to store. Thus the overhead required to store n bits of our unknown sequence is n( H(P&amp;#39;) + n&lt;sup&gt;-1&lt;/sup&gt; q log (n) ), symbols where P&amp;#39; converges to P asymptotically with n. And as a result we end up approaching the best possible compression rate, without knowing the source distribution to begin with.&lt;/p&gt;\n\n&lt;p&gt;Of course, LV77 is a little more complicated, but really no different. LV77, more or less, uses &lt;a href=\"https://en.wikipedia.org/wiki/Zipf%27s_law\"&gt;Zipf laws&lt;/a&gt;. More specifically, in an ergodic process the expected recurrence time of a symbol should be equal to the inverse of the probability of that symbol. And as such, the depth we need to observe to find the next symbol in the sequence should have a probability distribution that on average converges to the underlying process distribution. Lets look at an example,&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;AAABABBABBAABABABBBAAAB\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;which LV77 parses as &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;A, AA, B, AB, BABBAB, BA, ABAB, ABB, BAA, AB \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and encodes as &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;(0,A),(1,1,2),(0,B),(1,2,2),(1,3,6),(1,3,2),(1,4,4),(1,8,3), (1,9,3),(1,6,2) .\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;In more detail, the encoder has not seen A before, and states so leaving A uncompressed. The next two entries are A, so the encoder notes first the &lt;strong&gt;number of symbols since the last occurrence of the next sequence&lt;/strong&gt; and &lt;strong&gt;how many times the sequence occurs&lt;/strong&gt;. It then sees B and notes it is a new symbol, and from there-on everything can be represented as conditional.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg60pjr", "score_hidden": false, "stickied": false, "created": 1492039672.0, "created_utc": 1492010872.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 140}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6jymf", "gilded": 0, "archived": false, "score": 12, "report_reasons": null, "author": "cuulcars", "parent_id": "t1_dg5y6t4", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Heh, you escaped characters for a guy talking about escaped characters", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Heh, you escaped characters for a guy talking about escaped characters&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6jymf", "score_hidden": false, "stickied": false, "created": 1492059642.0, "created_utc": 1492030842.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 12}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg5y6t4", "gilded": 0, "archived": false, "score": 199, "report_reasons": null, "author": "halborn", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt;(w/o quotes)  \n\nJust so you know, if you want to make text appear differently but without the use of quotation marks, you can use the **\\`** symbol on either side instead to invoke the 'code' format `like this`.  ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;(w/o quotes)  &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Just so you know, if you want to make text appear differently but without the use of quotation marks, you can use the &lt;strong&gt;`&lt;/strong&gt; symbol on either side instead to invoke the &amp;#39;code&amp;#39; format &lt;code&gt;like this&lt;/code&gt;.  &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5y6t4", "score_hidden": false, "stickied": false, "created": 1492036848.0, "created_utc": 1492008048.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 199}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg61pyg", "gilded": 0, "archived": false, "score": 284, "report_reasons": null, "author": "monsieurpoirot", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Compressing is all about finding redundancies in the data. If you look harder, you might compress better. However, it may take longer to find the redundancy and the extra time might not be worth it.\n\nSo you have a compromise between time and space, and the settings allow you to tweak that.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Compressing is all about finding redundancies in the data. If you look harder, you might compress better. However, it may take longer to find the redundancy and the extra time might not be worth it.&lt;/p&gt;\n\n&lt;p&gt;So you have a compromise between time and space, and the settings allow you to tweak that.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61pyg", "score_hidden": false, "stickied": false, "created": 1492040784.0, "created_utc": 1492011984.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 284}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg62ax0", "gilded": 0, "archived": false, "score": 18, "report_reasons": null, "author": "masklinn", "parent_id": "t1_dg61q29", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt; The result for a higher compression setting is a tradeoff with computational power needed (and time needed to complete the compression).\n\nAnd the memory as well. When they have non-consecutive repetitions, compression algorithms can create \"back references\", encoding \"repeat 20 bytes you find 60 bytes back\" in very little compared to putting those 20 bytes there. How far back it'll look for repetitions is the \"window\", which you can configure, and the memory it takes is a factor of the window size.\n\nSome algorithms also have a tradeoff between speed and memory, in that you can specify a \"cache size\" of sort, reducing it will lower memory consumption but require the system to perform more work as it keeps recomputing the same data.", "edited": 1492023516.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The result for a higher compression setting is a tradeoff with computational power needed (and time needed to complete the compression).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;And the memory as well. When they have non-consecutive repetitions, compression algorithms can create &amp;quot;back references&amp;quot;, encoding &amp;quot;repeat 20 bytes you find 60 bytes back&amp;quot; in very little compared to putting those 20 bytes there. How far back it&amp;#39;ll look for repetitions is the &amp;quot;window&amp;quot;, which you can configure, and the memory it takes is a factor of the window size.&lt;/p&gt;\n\n&lt;p&gt;Some algorithms also have a tradeoff between speed and memory, in that you can specify a &amp;quot;cache size&amp;quot; of sort, reducing it will lower memory consumption but require the system to perform more work as it keeps recomputing the same data.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg62ax0", "score_hidden": false, "stickied": false, "created": 1492041418.0, "created_utc": 1492012618.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 18}}], "after": null, "before": null}}, "user_reports": [], "id": "dg61q29", "gilded": 0, "archived": false, "score": 44, "report_reasons": null, "author": "blerg34", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "The result for a higher compression setting is a tradeoff with computational power needed (and time needed to complete the compression). More resources are being used to try to find 'matches' to compress. \n\nIf you choose the lowest setting (it might be called something like 'store' in a rar compression program), your file might not be much smaller than the source material, even if compression was possible. You might still notice major size reductions for simple reasons such as a file with a lot of \"empty\" space buffering its size, or identical files in different directories.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The result for a higher compression setting is a tradeoff with computational power needed (and time needed to complete the compression). More resources are being used to try to find &amp;#39;matches&amp;#39; to compress. &lt;/p&gt;\n\n&lt;p&gt;If you choose the lowest setting (it might be called something like &amp;#39;store&amp;#39; in a rar compression program), your file might not be much smaller than the source material, even if compression was possible. You might still notice major size reductions for simple reasons such as a file with a lot of &amp;quot;empty&amp;quot; space buffering its size, or identical files in different directories.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61q29", "score_hidden": false, "stickied": false, "created": 1492040787.0, "created_utc": 1492011987.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 44}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg639lc", "gilded": 0, "archived": false, "score": 34, "report_reasons": null, "author": "xiaodown", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "That (and gzip and bzip2 etc) are basically giving the user a choice of a tradeoff between \"how hard do you want the zip program to look for complex and complicated efficiencies and take advantage of them in order to make the file smaller\" vs. \"how long do you want it to do this, and how much memory and CPU are you willing to let it use\".\n\nFor small files like your average 300kb text doc, this isn't really a concern; even an older, weaker computer can do a great job compressing it.  But what if you want to compress a 2GB file?\n\nFor example, what if you dump a SQL database that's, say, 5 GB (which is not uncommon).  Your sql file is going to have lots of repeating phrases like the sql syntax (INSERT INTO foo VALUES bar, TRUE/FALSE/NULL, etc).  And, it's not going to be random binary data (*usually*); it's going to be nearly entirely ascii upper and lower characters and numbers.  \n\nBut just the fact that the file is so huge means that it's going to require a lot of memory to zip it, as it holds larger and larger chunks of the file in memory in order to scan through them and look for similarities that it can obfuscate to save space.\n\nIn this case, it's entirely likely you can get your 5GB database file zipped down to ... I dunno, 500 megs.  But if your computer only has 2GB of ram, the zip program is going to CHUG-A-LUG your computer while it's zipping. \n\nSo applications like 7zip or gzip offer the choice of resource allocation to the user.  From the [gzip man page](https://linux.die.net/man/1/gzip):\n\n&gt; -# --fast --best    \n&gt; Regulate the speed of compression using the specified digit #, where -1 or --fast indicates the fastest compression method (less compression) and -9 or --best indicates the slowest compression method (best compression). The default compression level is -6 (that is, biased towards high compression at expense of speed).\n\n**edit** changed word doc -&gt; text doc, fixed spelling", "edited": 1492020021.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That (and gzip and bzip2 etc) are basically giving the user a choice of a tradeoff between &amp;quot;how hard do you want the zip program to look for complex and complicated efficiencies and take advantage of them in order to make the file smaller&amp;quot; vs. &amp;quot;how long do you want it to do this, and how much memory and CPU are you willing to let it use&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;For small files like your average 300kb text doc, this isn&amp;#39;t really a concern; even an older, weaker computer can do a great job compressing it.  But what if you want to compress a 2GB file?&lt;/p&gt;\n\n&lt;p&gt;For example, what if you dump a SQL database that&amp;#39;s, say, 5 GB (which is not uncommon).  Your sql file is going to have lots of repeating phrases like the sql syntax (INSERT INTO foo VALUES bar, TRUE/FALSE/NULL, etc).  And, it&amp;#39;s not going to be random binary data (&lt;em&gt;usually&lt;/em&gt;); it&amp;#39;s going to be nearly entirely ascii upper and lower characters and numbers.  &lt;/p&gt;\n\n&lt;p&gt;But just the fact that the file is so huge means that it&amp;#39;s going to require a lot of memory to zip it, as it holds larger and larger chunks of the file in memory in order to scan through them and look for similarities that it can obfuscate to save space.&lt;/p&gt;\n\n&lt;p&gt;In this case, it&amp;#39;s entirely likely you can get your 5GB database file zipped down to ... I dunno, 500 megs.  But if your computer only has 2GB of ram, the zip program is going to CHUG-A-LUG your computer while it&amp;#39;s zipping. &lt;/p&gt;\n\n&lt;p&gt;So applications like 7zip or gzip offer the choice of resource allocation to the user.  From the &lt;a href=\"https://linux.die.net/man/1/gzip\"&gt;gzip man page&lt;/a&gt;:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;-# --fast --best&lt;br/&gt;\nRegulate the speed of compression using the specified digit #, where -1 or --fast indicates the fastest compression method (less compression) and -9 or --best indicates the slowest compression method (best compression). The default compression level is -6 (that is, biased towards high compression at expense of speed).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;edit&lt;/strong&gt; changed word doc -&amp;gt; text doc, fixed spelling&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg639lc", "score_hidden": false, "stickied": false, "created": 1492042457.0, "created_utc": 1492013657.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 34}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg62p89", "gilded": 0, "archived": false, "score": 27, "report_reasons": null, "author": "Cadoc7", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "The final size of compression is related to how much time you are willing to spend on compressing the original. In the `RedddditRedddditRedddditRedddditReddddit` example, it took two passes to get to the optimal state of `5(Re4dit)`. That costs roughly 2x the time. But the difference between the compressed size of `5(Reddddit)` and `5(Re4dit)` is much smaller than the difference between `5(Reddddit)` and `RedddditRedddditRedddditRedddditReddddit`.\n\nSo at some point, the speed vs space tradeoff doesn't make sense. If you are willing to accept very good instead of perfect, you can shorten the amount of time that the compression takes.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The final size of compression is related to how much time you are willing to spend on compressing the original. In the &lt;code&gt;RedddditRedddditRedddditRedddditReddddit&lt;/code&gt; example, it took two passes to get to the optimal state of &lt;code&gt;5(Re4dit)&lt;/code&gt;. That costs roughly 2x the time. But the difference between the compressed size of &lt;code&gt;5(Reddddit)&lt;/code&gt; and &lt;code&gt;5(Re4dit)&lt;/code&gt; is much smaller than the difference between &lt;code&gt;5(Reddddit)&lt;/code&gt; and &lt;code&gt;RedddditRedddditRedddditRedddditReddddit&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;So at some point, the speed vs space tradeoff doesn&amp;#39;t make sense. If you are willing to accept very good instead of perfect, you can shorten the amount of time that the compression takes.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg62p89", "score_hidden": false, "stickied": false, "created": 1492041841.0, "created_utc": 1492013041.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 27}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg62erz", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "ericGraves", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Universal compression algorithms often employ sliding windows for the purposes of memory. In other words, they compress the next sequence based on a sliding window of the last k symbols. I can compress more by looking at a longer window, but it will take longer and use more computer resources.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Universal compression algorithms often employ sliding windows for the purposes of memory. In other words, they compress the next sequence based on a sliding window of the last k symbols. I can compress more by looking at a longer window, but it will take longer and use more computer resources.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg62erz", "score_hidden": false, "stickied": false, "created": 1492041531.0, "created_utc": 1492012731.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg62fv4", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "thegroundbelowme", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It's a balance of time vs compression level.  If, as a compression program, you use every single scheme, trick, and algorithm at your disposal, you might be able to get 70% compression, but it's going to be a lot of effort (i.e. CPU load) and the process will take a while.  If you only apply the easiest, more general tricks, you may only get 25% compression, but it won't take much effort and should be relatively quick.\n\nThen there's \"storing\" which is basically just using the compression format as a container to wrap multiple files, but without actually applying compression.  This is frequently what's used when you find a zip file full of .rar files.  The rar files provide the actual compression, while the zip file is simply used to reduce the number of files to distribute down to one.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a balance of time vs compression level.  If, as a compression program, you use every single scheme, trick, and algorithm at your disposal, you might be able to get 70% compression, but it&amp;#39;s going to be a lot of effort (i.e. CPU load) and the process will take a while.  If you only apply the easiest, more general tricks, you may only get 25% compression, but it won&amp;#39;t take much effort and should be relatively quick.&lt;/p&gt;\n\n&lt;p&gt;Then there&amp;#39;s &amp;quot;storing&amp;quot; which is basically just using the compression format as a container to wrap multiple files, but without actually applying compression.  This is frequently what&amp;#39;s used when you find a zip file full of .rar files.  The rar files provide the actual compression, while the zip file is simply used to reduce the number of files to distribute down to one.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg62fv4", "score_hidden": false, "stickied": false, "created": 1492041563.0, "created_utc": 1492012763.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg65n0x", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "the_crowz", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It takes longer. For very big files you may have to consider this. In the past when computers were very slow this mattered more.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It takes longer. For very big files you may have to consider this. In the past when computers were very slow this mattered more.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg65n0x", "score_hidden": false, "stickied": false, "created": 1492044934.0, "created_utc": 1492016134.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg67ckq", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "FreonTrip", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "There's a tradeoff between the computation needed to generate an efficiently compressed file, resulting time needed to decompress that file, memory usage during the process, and filesize. As an example that made more sense 20 years ago, if you're compressing a file that's *slightly* too big for a storage medium (like, say, a floppy) for a destination PC that's fairly pokey, it may be more convenient to apply less compression for everyone involved. These days it's less of a factor, but when you were compressing a batch of word processing documents to fit onto a floppy destined for a 486, that could make a noticeable difference in someone's work flow.", "edited": 1492018158.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a tradeoff between the computation needed to generate an efficiently compressed file, resulting time needed to decompress that file, memory usage during the process, and filesize. As an example that made more sense 20 years ago, if you&amp;#39;re compressing a file that&amp;#39;s &lt;em&gt;slightly&lt;/em&gt; too big for a storage medium (like, say, a floppy) for a destination PC that&amp;#39;s fairly pokey, it may be more convenient to apply less compression for everyone involved. These days it&amp;#39;s less of a factor, but when you were compressing a batch of word processing documents to fit onto a floppy destined for a 486, that could make a noticeable difference in someone&amp;#39;s work flow.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg67ckq", "score_hidden": false, "stickied": false, "created": 1492046665.0, "created_utc": 1492017865.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6gyyu", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "Em_Adespoton", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "See the comment by /u/ericGraves which covers this in detail.\n\nZip uses the deflate algorithm which is basically Lempel-Ziv from 1977/78.  The default for 7zip is LZMA, which stands for \"Lempel-Ziv Markov chain Algorithm\".  Basically, it uses the same algorithm as Zip, but attempts various block sizes and predicts the best compression window across the file using Markov chains.  It then changes the window size for different file parts.  Deflate, by comparison, only has manually set sizes you can pre-determine.\n\nThere are alternative compression algorithms that will produce a Zip-compatible archive which try all of the available sizes across the file and then only save the one providing the best compression, and some that will always compress the same file to the exact same byte sequence, sacrificing some compression by doing so.\n\nThen you have compression like lossless PNG, where the data space is well known, and multiple passes will actually improve the compression without degrading the data.  This is done by computing the best average window on each pass, chunking up the data each time.  So on a second pass, the best average window is only calculated for each sub-section of the file, and since different parts of the file might contain different distributions of informational data (eg, a white image with text at the top and a picture of a flower at the bottom), a different window size is appropriate.\n\n7zip LZMA also has a few other savings areas, in that it is specifically designed to be used across a set of similar files.  If you have two large files that only differ by a few bytes, it will compress the first file, and the second file will be saved as \"the same as the first file, but with the bytes at x being 'aaaa' instead of 'bbbb'\".  This means that if you're compressing a folder containing, say, a bunch of web pages, the common, repetitive HTML markup that is in every single file will only be stored once; with Deflate, there's only a 32 or 64 bit window (it only looks ahead that many bits to see if there's repeating data), and only for the file it's currently compressing.  So that data will be written in a compressed format for each file in the zip archive.\n\nAnother thing to note is that PK-Zip, 7zip, PDF, ISO, etc. are called \"container formats\".  The actual contents are defined in the file structure, and could be any number of things.  PK-Zip archives (commonly called zips after the Windows file extension) usually contain discrete \"file\" items, each with its own header information instructing the decompressor of the file's name, crc32 checksum, and compression algorithm (which is usually deflate, aka lv77).\n\nBy contrast, tgz \"tarball\" items found on Linux systems are a single data stream that's been created by serializing the file data into a \"tar\" (tape archive) file, which is then compressed using gnu-zip (similar to PK zip in that it uses deflate, but without the file header structure) over the entire new large file.  This means that tgz doesn't stop compression at the end of each internal file, and for small files, the file header information will likely be compressed itself across files.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;See the comment by &lt;a href=\"/u/ericGraves\"&gt;/u/ericGraves&lt;/a&gt; which covers this in detail.&lt;/p&gt;\n\n&lt;p&gt;Zip uses the deflate algorithm which is basically Lempel-Ziv from 1977/78.  The default for 7zip is LZMA, which stands for &amp;quot;Lempel-Ziv Markov chain Algorithm&amp;quot;.  Basically, it uses the same algorithm as Zip, but attempts various block sizes and predicts the best compression window across the file using Markov chains.  It then changes the window size for different file parts.  Deflate, by comparison, only has manually set sizes you can pre-determine.&lt;/p&gt;\n\n&lt;p&gt;There are alternative compression algorithms that will produce a Zip-compatible archive which try all of the available sizes across the file and then only save the one providing the best compression, and some that will always compress the same file to the exact same byte sequence, sacrificing some compression by doing so.&lt;/p&gt;\n\n&lt;p&gt;Then you have compression like lossless PNG, where the data space is well known, and multiple passes will actually improve the compression without degrading the data.  This is done by computing the best average window on each pass, chunking up the data each time.  So on a second pass, the best average window is only calculated for each sub-section of the file, and since different parts of the file might contain different distributions of informational data (eg, a white image with text at the top and a picture of a flower at the bottom), a different window size is appropriate.&lt;/p&gt;\n\n&lt;p&gt;7zip LZMA also has a few other savings areas, in that it is specifically designed to be used across a set of similar files.  If you have two large files that only differ by a few bytes, it will compress the first file, and the second file will be saved as &amp;quot;the same as the first file, but with the bytes at x being &amp;#39;aaaa&amp;#39; instead of &amp;#39;bbbb&amp;#39;&amp;quot;.  This means that if you&amp;#39;re compressing a folder containing, say, a bunch of web pages, the common, repetitive HTML markup that is in every single file will only be stored once; with Deflate, there&amp;#39;s only a 32 or 64 bit window (it only looks ahead that many bits to see if there&amp;#39;s repeating data), and only for the file it&amp;#39;s currently compressing.  So that data will be written in a compressed format for each file in the zip archive.&lt;/p&gt;\n\n&lt;p&gt;Another thing to note is that PK-Zip, 7zip, PDF, ISO, etc. are called &amp;quot;container formats&amp;quot;.  The actual contents are defined in the file structure, and could be any number of things.  PK-Zip archives (commonly called zips after the Windows file extension) usually contain discrete &amp;quot;file&amp;quot; items, each with its own header information instructing the decompressor of the file&amp;#39;s name, crc32 checksum, and compression algorithm (which is usually deflate, aka lv77).&lt;/p&gt;\n\n&lt;p&gt;By contrast, tgz &amp;quot;tarball&amp;quot; items found on Linux systems are a single data stream that&amp;#39;s been created by serializing the file data into a &amp;quot;tar&amp;quot; (tape archive) file, which is then compressed using gnu-zip (similar to PK zip in that it uses deflate, but without the file header structure) over the entire new large file.  This means that tgz doesn&amp;#39;t stop compression at the end of each internal file, and for small files, the file header information will likely be compressed itself across files.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6gyyu", "score_hidden": false, "stickied": false, "created": 1492056506.0, "created_utc": 1492027706.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg691bt", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Ellendar001", "parent_id": "t1_dg66p32", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "That depends a lot on the specific implementation of the algorithm / tuning parameters and even more so the file being compressed. If the program is switching algorithms on different settings, a file could be close to a degenerate case for either algorithm, causing a big swing in compression ratio either way. If the option is changing some tuning parameter within the algorithm (usually limiting search space / complexity to some upper bound), the faster version may find the same or a similarly good solution because a good solution happened to exist early in the search space. It's also possible that a much better optimization existed just beyond the set limit of the search space, and that a higher compression setting has a much better result. In general there is diminishing returns on the achieved compression ratio, but it's HIGHLY variable on the specific implementation and data used.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That depends a lot on the specific implementation of the algorithm / tuning parameters and even more so the file being compressed. If the program is switching algorithms on different settings, a file could be close to a degenerate case for either algorithm, causing a big swing in compression ratio either way. If the option is changing some tuning parameter within the algorithm (usually limiting search space / complexity to some upper bound), the faster version may find the same or a similarly good solution because a good solution happened to exist early in the search space. It&amp;#39;s also possible that a much better optimization existed just beyond the set limit of the search space, and that a higher compression setting has a much better result. In general there is diminishing returns on the achieved compression ratio, but it&amp;#39;s HIGHLY variable on the specific implementation and data used.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg691bt", "score_hidden": false, "stickied": false, "created": 1492048354.0, "created_utc": 1492019554.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg66p32", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "dragonnyxx", "parent_id": "t1_dg61vua", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It's not going to be 5% longer for 5% better compression, though. It's more like 3x as long for 1% better compression.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not going to be 5% longer for 5% better compression, though. It&amp;#39;s more like 3x as long for 1% better compression.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg66p32", "score_hidden": false, "stickied": false, "created": 1492046010.0, "created_utc": 1492017210.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 7}}], "after": null, "before": null}}, "user_reports": [], "id": "dg61vua", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Ellendar001", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "There is a time-space tradeoff where finding the optimal way to compress a file may take a lot of processing time. You may be willing to accept a compression ratio that is 5% worse if it speeds up the compression/decompression time by 5%. Which is better depends on the cost ratio for processor/memory vs file transfer, so it's left for the user to select.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There is a time-space tradeoff where finding the optimal way to compress a file may take a lot of processing time. You may be willing to accept a compression ratio that is 5% worse if it speeds up the compression/decompression time by 5%. Which is better depends on the cost ratio for processor/memory vs file transfer, so it&amp;#39;s left for the user to select.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61vua", "score_hidden": false, "stickied": false, "created": 1492040963.0, "created_utc": 1492012163.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg623i7", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "mooseable", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "These settings usually determine the type of algorithms used. Higher compression will use more complex algorithms at the expense of time, so they take longer to compress.\n\nIn most cases, choosing the highest compression will only have a tiny reduction in size, but might take twice as long to compute.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;These settings usually determine the type of algorithms used. Higher compression will use more complex algorithms at the expense of time, so they take longer to compress.&lt;/p&gt;\n\n&lt;p&gt;In most cases, choosing the highest compression will only have a tiny reduction in size, but might take twice as long to compute.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg623i7", "score_hidden": false, "stickied": false, "created": 1492041196.0, "created_utc": 1492012396.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg61zsq", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "butlertd", "parent_id": "t1_dg61ajc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It's a trade off with computing time.\n\nIf you want to compress as fast as possible, you might be lazy and not achieve optimal compression.\n\nIf you want the file size as small as possible, you need to be more thorough, and that takes time.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a trade off with computing time.&lt;/p&gt;\n\n&lt;p&gt;If you want to compress as fast as possible, you might be lazy and not achieve optimal compression.&lt;/p&gt;\n\n&lt;p&gt;If you want the file size as small as possible, you need to be more thorough, and that takes time.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61zsq", "score_hidden": false, "stickied": false, "created": 1492041083.0, "created_utc": 1492012283.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg61ajc", "gilded": 0, "archived": false, "score": 43, "report_reasons": null, "author": "Lumpyyyyy", "parent_id": "t1_dg5wt44", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Why is it that in some compression software it has settings for amount of compression (i.e. 7zip)?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why is it that in some compression software it has settings for amount of compression (i.e. 7zip)?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61ajc", "score_hidden": false, "stickied": false, "created": 1492040311.0, "created_utc": 1492011511.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 43}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg61o8r", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "jmite", "parent_id": "t1_dg5wt44", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Well, as small as you can with that particular algorithm. Kolmogorov complexity, the maximum compression of a string, is undecidable.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, as small as you can with that particular algorithm. Kolmogorov complexity, the maximum compression of a string, is undecidable.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61o8r", "score_hidden": false, "stickied": false, "created": 1492040731.0, "created_utc": 1492011931.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 9}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg64pjx", "gilded": 0, "archived": false, "score": 12, "report_reasons": null, "author": "zeidrich", "parent_id": "t1_dg636dd", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "You misread.  I didn't say it makes it as small as possible, I said as small as it can.  This means that it reaches it's limit, and then that's as small as it can compress it. \n\nI do mention that you can compress it multiple times and see multiple reductions in some cases, but in those situations, you could also write a compression utility that attempts the same algorithm multiple times until the file no longer sees a size reduction. \n\nI mean, if it's a common scenario that you can compress a file 4 times before it stops getting smaller, you just write a tool that compresses the file 4 times.   Then you won't be able to compress it with that algorithm any more. \n\nIt's just the case that most files aren't 1TB worth of zeros, and there's little to gain by compressing most files multiple times with zip compression, so the compression/decompression tools don't implement that, not because they can't, but because of processing tradeoffs. \n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You misread.  I didn&amp;#39;t say it makes it as small as possible, I said as small as it can.  This means that it reaches it&amp;#39;s limit, and then that&amp;#39;s as small as it can compress it. &lt;/p&gt;\n\n&lt;p&gt;I do mention that you can compress it multiple times and see multiple reductions in some cases, but in those situations, you could also write a compression utility that attempts the same algorithm multiple times until the file no longer sees a size reduction. &lt;/p&gt;\n\n&lt;p&gt;I mean, if it&amp;#39;s a common scenario that you can compress a file 4 times before it stops getting smaller, you just write a tool that compresses the file 4 times.   Then you won&amp;#39;t be able to compress it with that algorithm any more. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s just the case that most files aren&amp;#39;t 1TB worth of zeros, and there&amp;#39;s little to gain by compressing most files multiple times with zip compression, so the compression/decompression tools don&amp;#39;t implement that, not because they can&amp;#39;t, but because of processing tradeoffs. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg64pjx", "score_hidden": false, "stickied": false, "created": 1492043969.0, "created_utc": 1492015169.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 12}}], "after": null, "before": null}}, "user_reports": [], "id": "dg636dd", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "Dascandy", "parent_id": "t1_dg5wt44", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt; Because the first time you compress it, it makes it as small as it can. \n\nNot true. Each compression algorithm has limits for how far it will look, and it will compress as much as possible, usually reducing by a factor of 100000 on very compressible files. Normally this doesn't leave much redundancy to further compress.\n\nTake a 1TB file filled with only zero-bytes though. This, after a 100000 reduction, is still a 10MB file, only saying \"a million zeroes, and then another, and then another, ...\". This *is* compressible. Experimentally, this compresses slightly until the 4th time you compress it, and will expand on the 5th run.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Because the first time you compress it, it makes it as small as it can. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Not true. Each compression algorithm has limits for how far it will look, and it will compress as much as possible, usually reducing by a factor of 100000 on very compressible files. Normally this doesn&amp;#39;t leave much redundancy to further compress.&lt;/p&gt;\n\n&lt;p&gt;Take a 1TB file filled with only zero-bytes though. This, after a 100000 reduction, is still a 10MB file, only saying &amp;quot;a million zeroes, and then another, and then another, ...&amp;quot;. This &lt;em&gt;is&lt;/em&gt; compressible. Experimentally, this compresses slightly until the 4th time you compress it, and will expand on the 5th run.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg636dd", "score_hidden": false, "stickied": false, "created": 1492042361.0, "created_utc": 1492013561.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 6}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6dw9p", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "UncleMeat11", "parent_id": "t1_dg63drk", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Importantly, no system would ever produce 42.zip naturally. It is a specifically crafted file that takes advantage of the recursive unfolding process to exponentially increase file size. It is trivial to see how this might work with xml macros.\n\nThe reason why 42.zip was such a problem was because virus scanners used to *automatically* unzip it to check for malicious content so you didn't actually need to open it. Simply running a virus scanner on your filesystem would hang your computer.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Importantly, no system would ever produce 42.zip naturally. It is a specifically crafted file that takes advantage of the recursive unfolding process to exponentially increase file size. It is trivial to see how this might work with xml macros.&lt;/p&gt;\n\n&lt;p&gt;The reason why 42.zip was such a problem was because virus scanners used to &lt;em&gt;automatically&lt;/em&gt; unzip it to check for malicious content so you didn&amp;#39;t actually need to open it. Simply running a virus scanner on your filesystem would hang your computer.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6dw9p", "score_hidden": false, "stickied": false, "created": 1492053352.0, "created_utc": 1492024552.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg63drk", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "iBoMbY", "parent_id": "t1_dg5wt44", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "If you know what you are doing, you can do some [crazy stuff](http://www.unforgettable.dk/) with compression though (some Virus Scanners will detect that 42.zip as threat, because it may crash their Engine.).", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you know what you are doing, you can do some &lt;a href=\"http://www.unforgettable.dk/\"&gt;crazy stuff&lt;/a&gt; with compression though (some Virus Scanners will detect that 42.zip as threat, because it may crash their Engine.).&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg63drk", "score_hidden": false, "stickied": false, "created": 1492042579.0, "created_utc": 1492013779.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg77fp9", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "marcan42", "parent_id": "t1_dg5wt44", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It's worth noting that your \"RedddditRedddditRedddditRedddditReddddit\" example is actually compressed efficiently in one pass in most LZ-derived algorithms (like ZIP/gzip). Those algorithms work by referring back to previous data. It would look like this:\n\nRed(1;3)it(8;32)\n\nThat means \"Red\", then \"go back 1 character and copy 3 characters\", then \"it\", then \"go back 8 characters and copy 32 characters\". That might sound silly, since you're copying more data than you have, but the decompressor works byte by byte and is allowed to copy data that it has just output, so you end up with:\n\n* Red\n* Re(d)-&gt;[d] (1/3 characters)\n* Red(d)-&gt;[d] (2/3 characters)\n* Redd(d)-&gt;[d] (3/3 characters)\n* Reddddit\n* (Reddddit)-&gt;[Reddddit] (8/32 characters)\n* Reddddit(Reddddit)-&gt;[Reddddit] (16/32 characters)\n\n...and so on and so forth. Note that even though (1;3) is longer than the 3 characters it represents, in reality it would be represented in a more compact format that is shorter.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s worth noting that your &amp;quot;RedddditRedddditRedddditRedddditReddddit&amp;quot; example is actually compressed efficiently in one pass in most LZ-derived algorithms (like ZIP/gzip). Those algorithms work by referring back to previous data. It would look like this:&lt;/p&gt;\n\n&lt;p&gt;Red(1;3)it(8;32)&lt;/p&gt;\n\n&lt;p&gt;That means &amp;quot;Red&amp;quot;, then &amp;quot;go back 1 character and copy 3 characters&amp;quot;, then &amp;quot;it&amp;quot;, then &amp;quot;go back 8 characters and copy 32 characters&amp;quot;. That might sound silly, since you&amp;#39;re copying more data than you have, but the decompressor works byte by byte and is allowed to copy data that it has just output, so you end up with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Red&lt;/li&gt;\n&lt;li&gt;Re(d)-&amp;gt;[d] (1/3 characters)&lt;/li&gt;\n&lt;li&gt;Red(d)-&amp;gt;[d] (2/3 characters)&lt;/li&gt;\n&lt;li&gt;Redd(d)-&amp;gt;[d] (3/3 characters)&lt;/li&gt;\n&lt;li&gt;Reddddit&lt;/li&gt;\n&lt;li&gt;(Reddddit)-&amp;gt;[Reddddit] (8/32 characters)&lt;/li&gt;\n&lt;li&gt;Reddddit(Reddddit)-&amp;gt;[Reddddit] (16/32 characters)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;...and so on and so forth. Note that even though (1;3) is longer than the 3 characters it represents, in reality it would be represented in a more compact format that is shorter.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg77fp9", "score_hidden": false, "stickied": false, "created": 1492090590.0, "created_utc": 1492061790.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg5wt44", "gilded": 0, "archived": false, "score": 348, "report_reasons": null, "author": "zeidrich", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Because the first time you compress it, it makes it as small as it can. \n\nImagine you could zip it a second time and it would get smaller.  Why wouldn't you just have your compression software zip it twice automatically with one \"zip\" action.  And in some cases this is sort of what happens.  In some software you can change the level of compression, to change how thorough it is, at the expense of speed of compression and decompression. \n\nBut you can think of it mathematically too.  You are essentially factoring the contents.  RedditRedditRedditRedditReddit can be represented at 5(Reddit).  But now Reddit and 5 are essentially irreducible, \"prime\" almost. You could do 5(Re2dit) but this doesn't save any space. \n\nOn the other hand, RedddditRedddditRedddditRedddditReddddit might be 5(Reddddit), but Reddddit might be Re4dit, so one level of compression might give you 5(Reddddit), but being more thorough might give you 5(Re4dit)\n\nBut at this point, you can't do anything more to reduce it. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because the first time you compress it, it makes it as small as it can. &lt;/p&gt;\n\n&lt;p&gt;Imagine you could zip it a second time and it would get smaller.  Why wouldn&amp;#39;t you just have your compression software zip it twice automatically with one &amp;quot;zip&amp;quot; action.  And in some cases this is sort of what happens.  In some software you can change the level of compression, to change how thorough it is, at the expense of speed of compression and decompression. &lt;/p&gt;\n\n&lt;p&gt;But you can think of it mathematically too.  You are essentially factoring the contents.  RedditRedditRedditRedditReddit can be represented at 5(Reddit).  But now Reddit and 5 are essentially irreducible, &amp;quot;prime&amp;quot; almost. You could do 5(Re2dit) but this doesn&amp;#39;t save any space. &lt;/p&gt;\n\n&lt;p&gt;On the other hand, RedddditRedddditRedddditRedddditReddddit might be 5(Reddddit), but Reddddit might be Re4dit, so one level of compression might give you 5(Reddddit), but being more thorough might give you 5(Re4dit)&lt;/p&gt;\n\n&lt;p&gt;But at this point, you can&amp;#39;t do anything more to reduce it. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5wt44", "score_hidden": false, "stickied": false, "created": 1492035209.0, "created_utc": 1492006409.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 348}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5zaju", "gilded": 0, "archived": false, "score": 20, "report_reasons": null, "author": "TalkingBackAgain", "parent_id": "t1_dg5wgxo", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "This is true. Also: if you're trying to compress formats that already are compressed file formats you're not going to get s smaller file. In fact it will now be slightly larger because it now also has to add the information of the compression software applied to the file.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is true. Also: if you&amp;#39;re trying to compress formats that already are compressed file formats you&amp;#39;re not going to get s smaller file. In fact it will now be slightly larger because it now also has to add the information of the compression software applied to the file.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5zaju", "score_hidden": false, "stickied": false, "created": 1492038101.0, "created_utc": 1492009301.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 20}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6uraq", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "TheSteganographer", "parent_id": "t1_dg61nny", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "a=999999999X, b=999999999a, c=999999999b, d=999999999c, e=999999999d, f=999999999e, g=999999999f, h=999999999g, i=999999999h, j=999999999i, k=999999999j, l=999999999k, m=999999999l, n=999999999m, o=999999999n, p=999999999o, q=999999999p, r=999999999q, s=999999999r, t=999999999s, u=999999999t, v=999999999u, w=999999999v, x=999999999w, y=999999999x, z=999999999y! 999999999z\n\n^^^^^^^^^^^^^^^^^^^^^^^^ninja_edit", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;a=999999999X, b=999999999a, c=999999999b, d=999999999c, e=999999999d, f=999999999e, g=999999999f, h=999999999g, i=999999999h, j=999999999i, k=999999999j, l=999999999k, m=999999999l, n=999999999m, o=999999999n, p=999999999o, q=999999999p, r=999999999q, s=999999999r, t=999999999s, u=999999999t, v=999999999u, w=999999999v, x=999999999w, y=999999999x, z=999999999y! 999999999z&lt;/p&gt;\n\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6uraq", "score_hidden": false, "stickied": false, "created": 1492072789.0, "created_utc": 1492043989.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg61nny", "gilded": 0, "archived": false, "score": 27, "report_reasons": null, "author": "account_destroyed", "parent_id": "t1_dg60zrw", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "A zip bomb is basically a set of files and folders crafted knowing the exact rules that the compression software uses so that they can create the largest possible size with the smallest possible compressed output. In the example given previously, it would be like writing Reddit a million times, which would yield a file of 6 million characters uncompressed, but just something closer to 17 compressed, because the compressed file would just say \"a=Reddit!1000000a\".\n\nthere is a similar type of nefarious file manipulation trick in networking called a reflection attack, where I pretend to be your computer and ask someone for some data using the smallest request that yields the largest payload, such as what are all of the addresses for computers belonging to google and any of their subdomains and the person on the other end gets info about the servers for google.com, mail.google.com, calendar.google.com, etc.", "edited": 1492012179.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A zip bomb is basically a set of files and folders crafted knowing the exact rules that the compression software uses so that they can create the largest possible size with the smallest possible compressed output. In the example given previously, it would be like writing Reddit a million times, which would yield a file of 6 million characters uncompressed, but just something closer to 17 compressed, because the compressed file would just say &amp;quot;a=Reddit!1000000a&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;there is a similar type of nefarious file manipulation trick in networking called a reflection attack, where I pretend to be your computer and ask someone for some data using the smallest request that yields the largest payload, such as what are all of the addresses for computers belonging to google and any of their subdomains and the person on the other end gets info about the servers for google.com, mail.google.com, calendar.google.com, etc.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61nny", "score_hidden": false, "stickied": false, "created": 1492040714.0, "created_utc": 1492011914.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 27}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg61uv4", "gilded": 0, "archived": false, "score": 11, "report_reasons": null, "author": "FriendlyDespot", "parent_id": "t1_dg60zrw", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Take his explanation of \"20a\" to replace a string of 20 consecutive \"a\"s. That would inflate to 20 bytes of ASCII. If you put 1000000a instead, that would inflate to one megabyte of ASCII. If you put 100000000000a, it would inflate to 100 gigabytes of ASCII, which would leave the application stuck either trying to fit 100 gigabytes of data into your memory, or writing 100 gigabytes of data to your storage device, depending on implementation, all from trying to inflate a compressed file that's a handful of bytes in length. The zip bombs that target stuff like anti-virus usually nest multiple zip files meaning that the anti-virus has no choice but to try to store all of the data in memory, since it needs the full data of each nesting layer to decompress the nesting layer below.", "edited": 1492012319.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Take his explanation of &amp;quot;20a&amp;quot; to replace a string of 20 consecutive &amp;quot;a&amp;quot;s. That would inflate to 20 bytes of ASCII. If you put 1000000a instead, that would inflate to one megabyte of ASCII. If you put 100000000000a, it would inflate to 100 gigabytes of ASCII, which would leave the application stuck either trying to fit 100 gigabytes of data into your memory, or writing 100 gigabytes of data to your storage device, depending on implementation, all from trying to inflate a compressed file that&amp;#39;s a handful of bytes in length. The zip bombs that target stuff like anti-virus usually nest multiple zip files meaning that the anti-virus has no choice but to try to store all of the data in memory, since it needs the full data of each nesting layer to decompress the nesting layer below.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61uv4", "score_hidden": false, "stickied": false, "created": 1492040932.0, "created_utc": 1492012132.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 11}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg62301", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "Cintax", "parent_id": "t1_dg60zrw", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Zip bombs aren't zips all the way down, they're usually several discrete layers of zips with a number of repetitive easily compressed files zipped together. \n\nImagine you have the letter A repeated a billion times. Compressed with the simple algorithm above, it'd be 1000000000000A, which isn't very long. But decompressed, it's significantly larger. \n\nIt's zipped multiple times because it's not just one file, it's, for example, 5 files, separately zipped, then those 5 zips are zipped together. Then you copy that zip 4 times and zip the original and the copies together, etc. Zipping one file multiple times doesn't yield any benefits, but zipping multiple files or copies will. This makes it possible for the file contents to quickly grow out of control from a very tiny compressed seed. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Zip bombs aren&amp;#39;t zips all the way down, they&amp;#39;re usually several discrete layers of zips with a number of repetitive easily compressed files zipped together. &lt;/p&gt;\n\n&lt;p&gt;Imagine you have the letter A repeated a billion times. Compressed with the simple algorithm above, it&amp;#39;d be 1000000000000A, which isn&amp;#39;t very long. But decompressed, it&amp;#39;s significantly larger. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s zipped multiple times because it&amp;#39;s not just one file, it&amp;#39;s, for example, 5 files, separately zipped, then those 5 zips are zipped together. Then you copy that zip 4 times and zip the original and the copies together, etc. Zipping one file multiple times doesn&amp;#39;t yield any benefits, but zipping multiple files or copies will. This makes it possible for the file contents to quickly grow out of control from a very tiny compressed seed. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg62301", "score_hidden": false, "stickied": false, "created": 1492041181.0, "created_utc": 1492012381.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 6}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6272i", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "MGorak", "parent_id": "t1_dg60zrw", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "A zip bomb: a very small file that uncompresses to something so large the program/system crashes because it's not designed to handle so large a file.\n\nRe9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999dit.\n\n\nOnce you write that many d, you find that your drive is completely filled and it's not even close to be finished uncompressing the file.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A zip bomb: a very small file that uncompresses to something so large the program/system crashes because it&amp;#39;s not designed to handle so large a file.&lt;/p&gt;\n\n&lt;p&gt;Re9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999dit.&lt;/p&gt;\n\n&lt;p&gt;Once you write that many d, you find that your drive is completely filled and it&amp;#39;s not even close to be finished uncompressing the file.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6272i", "score_hidden": false, "stickied": false, "created": 1492041305.0, "created_utc": 1492012505.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg61wdg", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "CMDR_Pete", "parent_id": "t1_dg60zrw", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Think about the examples provided in the top level post, how you can use a number to repeat something a number of times. Imagine using that compression but you make a large dictionary entry such as: $a={huge data}\n\nNow imagine your compressed file is:  \n999999999a\n\nNow you have a compressed file that will expand to a hundred million times its size.  Of course just add numbers to make it even bigger!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Think about the examples provided in the top level post, how you can use a number to repeat something a number of times. Imagine using that compression but you make a large dictionary entry such as: $a={huge data}&lt;/p&gt;\n\n&lt;p&gt;Now imagine your compressed file is:&lt;br/&gt;\n999999999a&lt;/p&gt;\n\n&lt;p&gt;Now you have a compressed file that will expand to a hundred million times its size.  Of course just add numbers to make it even bigger!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61wdg", "score_hidden": false, "stickied": false, "created": 1492040979.0, "created_utc": 1492012179.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg62nd5", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "Got_Tiger", "parent_id": "t1_dg60zrw", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "a zip bomb is different from normal zip files in that it was specifically constructed to produce a large output. in the format of the top example, it would be something like $=t!9999999t. an expression like this is incredibly small, but it can produce output exponentially larger than its size. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;a zip bomb is different from normal zip files in that it was specifically constructed to produce a large output. in the format of the top example, it would be something like $=t!9999999t. an expression like this is incredibly small, but it can produce output exponentially larger than its size. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg62nd5", "score_hidden": false, "stickied": false, "created": 1492041787.0, "created_utc": 1492012987.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg61wa9", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Superpickle18", "parent_id": "t1_dg60zrw", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Basically millions of files with similar data inside, so the compression algorithm just compresses one copy of the file and shares that copy with all files.\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Basically millions of files with similar data inside, so the compression algorithm just compresses one copy of the file and shares that copy with all files.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61wa9", "score_hidden": false, "stickied": false, "created": 1492040976.0, "created_utc": 1492012176.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg60zrw", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "Galaghan", "parent_id": "t1_dg5wgxo", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "So what's the data inside a zip bomb? Isn't that zips all the way down? \n\nCan you explain a zip bomb for me because damn your explaining is top notch. \n\nP.s. ok I get it, thanks guys", "edited": 1492013203.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So what&amp;#39;s the data inside a zip bomb? Isn&amp;#39;t that zips all the way down? &lt;/p&gt;\n\n&lt;p&gt;Can you explain a zip bomb for me because damn your explaining is top notch. &lt;/p&gt;\n\n&lt;p&gt;P.s. ok I get it, thanks guys&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg60zrw", "score_hidden": false, "stickied": false, "created": 1492039981.0, "created_utc": 1492011181.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 9}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg656uw", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "dewiniaid", "parent_id": "t1_dg6147g", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "This is true of .zip because the catalog\u200b (which says which files are in the archive, where they're located, etc.) Isn't compressed IIRC.\n\nCompare to .tar.gz, where .tar is a solely an archive format, and .gz is solely compression (it doesn't even store the input filename)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is true of .zip because the catalog\u200b (which says which files are in the archive, where they&amp;#39;re located, etc.) Isn&amp;#39;t compressed IIRC.&lt;/p&gt;\n\n&lt;p&gt;Compare to .tar.gz, where .tar is a solely an archive format, and .gz is solely compression (it doesn&amp;#39;t even store the input filename)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg656uw", "score_hidden": false, "stickied": false, "created": 1492044465.0, "created_utc": 1492015665.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6147g", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "kanuut", "parent_id": "t1_dg5wgxo", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "End Note: Although there's very few instances where it's the best option, you can compress a collection of compressed files and enjoy a further reduction of data, best when the same compression method is used, but still usually functional when multiple are used. It's usually better to have all the files in a single compression though, you'll find the greatest reduction of size through that.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;End Note: Although there&amp;#39;s very few instances where it&amp;#39;s the best option, you can compress a collection of compressed files and enjoy a further reduction of data, best when the same compression method is used, but still usually functional when multiple are used. It&amp;#39;s usually better to have all the files in a single compression though, you&amp;#39;ll find the greatest reduction of size through that.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6147g", "score_hidden": false, "stickied": false, "created": 1492040116.0, "created_utc": 1492011316.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg5wgxo", "gilded": 0, "archived": false, "score": 95, "report_reasons": null, "author": "Captcha142", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "The main reason that you can't compress the zip file is that the zip file is already, by design, as compressed as it can be. The zip file compresses all of its data to the smallest size it can be without losing data, so putting that zip file into another zip file would do nothing. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The main reason that you can&amp;#39;t compress the zip file is that the zip file is already, by design, as compressed as it can be. The zip file compresses all of its data to the smallest size it can be without losing data, so putting that zip file into another zip file would do nothing. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5wgxo", "score_hidden": false, "stickied": false, "created": 1492034783.0, "created_utc": 1492005983.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 95}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6a3j4", "gilded": 0, "archived": false, "score": 11, "report_reasons": null, "author": "hobbycollector", "parent_id": "t1_dg5zpil", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "This is an important detail often left out of (lossless) compression discussions. It's not just that you didn't try hard enough or take long enough to get more compression, it's that the pigeonhole principle makes it theoretically impossible to create an algorithm that compresses everything. There is always some case where the algorithm makes it bigger.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is an important detail often left out of (lossless) compression discussions. It&amp;#39;s not just that you didn&amp;#39;t try hard enough or take long enough to get more compression, it&amp;#39;s that the pigeonhole principle makes it theoretically impossible to create an algorithm that compresses everything. There is always some case where the algorithm makes it bigger.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6a3j4", "score_hidden": false, "stickied": false, "created": 1492049446.0, "created_utc": 1492020646.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 11}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg5zpil", "gilded": 0, "archived": false, "score": 18, "report_reasons": null, "author": "ymgve", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It can be proven - suppose you have a file 4 bytes in size - there are 256\\*256\\*256\\*256 possible such files.\n\nNow suppose you have a program that *always* compresses a 4 byte file into a new file with 3 bytes. There are 256\\*256\\*256 possible files after compression.\n\nBut this is a much smaller number than the previous one - so there must be some different 4 byte files that compress to the same 3 byte file, which also means you can't uniquely decompress that file again.\n\nThis proves you cannot make a program that always compresses a 4 byte file into a 3 byte file, and can be generalized to prove that you can't make a program that always generates smaller output than its input.\n\nIf you're interested in more detailed info: https://en.wikipedia.org/wiki/Lossless_compression#Limitations", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It can be proven - suppose you have a file 4 bytes in size - there are 256*256*256*256 possible such files.&lt;/p&gt;\n\n&lt;p&gt;Now suppose you have a program that &lt;em&gt;always&lt;/em&gt; compresses a 4 byte file into a new file with 3 bytes. There are 256*256*256 possible files after compression.&lt;/p&gt;\n\n&lt;p&gt;But this is a much smaller number than the previous one - so there must be some different 4 byte files that compress to the same 3 byte file, which also means you can&amp;#39;t uniquely decompress that file again.&lt;/p&gt;\n\n&lt;p&gt;This proves you cannot make a program that always compresses a 4 byte file into a 3 byte file, and can be generalized to prove that you can&amp;#39;t make a program that always generates smaller output than its input.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested in more detailed info: &lt;a href=\"https://en.wikipedia.org/wiki/Lossless_compression#Limitations\"&gt;https://en.wikipedia.org/wiki/Lossless_compression#Limitations&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5zpil", "score_hidden": false, "stickied": false, "created": 1492038567.0, "created_utc": 1492009767.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 18}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5wd7v", "gilded": 0, "archived": false, "score": 12, "report_reasons": null, "author": "its_not_appropriate_", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "The first .zip is so 'complex' in sense of 'not having repetitive parts', that there is possibly no way to simplify this. I have a feeling that maybe we could find some veeery complex patterns though, but we should then write a special algorithm, made specially for this case - which would probably not be used anywhere else at all.\n\nAt some point the file is too random to be compressed efficiently.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The first .zip is so &amp;#39;complex&amp;#39; in sense of &amp;#39;not having repetitive parts&amp;#39;, that there is possibly no way to simplify this. I have a feeling that maybe we could find some veeery complex patterns though, but we should then write a special algorithm, made specially for this case - which would probably not be used anywhere else at all.&lt;/p&gt;\n\n&lt;p&gt;At some point the file is too random to be compressed efficiently.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5wd7v", "score_hidden": false, "stickied": false, "created": 1492034653.0, "created_utc": 1492005853.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 12}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5zpnh", "gilded": 0, "archived": false, "score": 12, "report_reasons": null, "author": "SinisterMJ", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Talking from an Entropy point of view, the uncompressed file includes a lot of patterns (low Entropy), so you can compress it via the earlier mentioned methods (instead of 'aaaaa' make it '5a'). But the higher the entropy, the better the compression, the more it looks like random data. And you can't compress randomness, since there is no pattern to it. Thus a compressed (random) file can't be compressed anymore with today's understanding.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Talking from an Entropy point of view, the uncompressed file includes a lot of patterns (low Entropy), so you can compress it via the earlier mentioned methods (instead of &amp;#39;aaaaa&amp;#39; make it &amp;#39;5a&amp;#39;). But the higher the entropy, the better the compression, the more it looks like random data. And you can&amp;#39;t compress randomness, since there is no pattern to it. Thus a compressed (random) file can&amp;#39;t be compressed anymore with today&amp;#39;s understanding.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5zpnh", "score_hidden": false, "stickied": false, "created": 1492038571.0, "created_utc": 1492009771.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 12}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg7mekp", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "ThwompThwomp", "parent_id": "t1_dg6rdeh", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Yes, but its also not stated that we are discussing lossless compression. For instance, I could take a DFT of your 1 GB file, and just store the first X peaks. That would compress the data, and you would have some loss of data. Would that suffice? Maybe.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, but its also not stated that we are discussing lossless compression. For instance, I could take a DFT of your 1 GB file, and just store the first X peaks. That would compress the data, and you would have some loss of data. Would that suffice? Maybe.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg7mekp", "score_hidden": false, "stickied": false, "created": 1492122753.0, "created_utc": 1492093953.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6rdeh", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "HighRelevancy", "parent_id": "t1_dg60u9d", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It's kinda implied that we're talking about algorithms that aren't entirely meaningless though.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s kinda implied that we&amp;#39;re talking about algorithms that aren&amp;#39;t entirely meaningless though.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6rdeh", "score_hidden": false, "stickied": false, "created": 1492068642.0, "created_utc": 1492039842.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg60u9d", "gilded": 0, "archived": false, "score": 18, "report_reasons": null, "author": "ThwompThwomp", "parent_id": "t1_dg5x3gz", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "That's for lossless compression. Sure, I can take _any_ 1GB file and compress it down to 1 bit for you. You might lose some data when I expand it out, though.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s for lossless compression. Sure, I can take &lt;em&gt;any&lt;/em&gt; 1GB file and compress it down to 1 bit for you. You might lose some data when I expand it out, though.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg60u9d", "score_hidden": false, "stickied": false, "created": 1492039814.0, "created_utc": 1492011014.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 18}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg5x3gz", "gilded": 0, "archived": false, "score": 16, "report_reasons": null, "author": "rollie82", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "So here's a question: do you think there is an algorithm that can take *any* 1GB of data, and compress it? Can your algorithm be guaranteed to be able to compress a chunk of data that size?\n\nThe answer is no. Most compression relies on pattern inside the data; a byte has 256 possible values, but there are only 72 English characters; you can exploit this to compression the document. But what if all values are equally likely? The \"A26\" example suddenly doesn't seem useful.  This can be proven impossible with a little thought. \n\nCompression is all about taking advantage of what you know about the data. If you the data you are compressing *tends* to have some sort of pattern, you can compress in the average case.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So here&amp;#39;s a question: do you think there is an algorithm that can take &lt;em&gt;any&lt;/em&gt; 1GB of data, and compress it? Can your algorithm be guaranteed to be able to compress a chunk of data that size?&lt;/p&gt;\n\n&lt;p&gt;The answer is no. Most compression relies on pattern inside the data; a byte has 256 possible values, but there are only 72 English characters; you can exploit this to compression the document. But what if all values are equally likely? The &amp;quot;A26&amp;quot; example suddenly doesn&amp;#39;t seem useful.  This can be proven impossible with a little thought. &lt;/p&gt;\n\n&lt;p&gt;Compression is all about taking advantage of what you know about the data. If you the data you are compressing &lt;em&gt;tends&lt;/em&gt; to have some sort of pattern, you can compress in the average case.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5x3gz", "score_hidden": false, "stickied": false, "created": 1492035558.0, "created_utc": 1492006758.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 16}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5xeyt", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "flyingapples15", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Sort of like reducing a fraction down to the lowest denominator, it's already in it's simplest form, or near it, trying to \"simplify\" the file further would likely make it larger instead ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sort of like reducing a fraction down to the lowest denominator, it&amp;#39;s already in it&amp;#39;s simplest form, or near it, trying to &amp;quot;simplify&amp;quot; the file further would likely make it larger instead &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5xeyt", "score_hidden": false, "stickied": false, "created": 1492035937.0, "created_utc": 1492007137.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 7}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5wgux", "gilded": 0, "archived": false, "score": 12, "report_reasons": null, "author": "EtherCJ", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "All compression algorithms have some inputs which take more space and some that take less space.  However, they design the compression algorithm to make the inputs which take less the ones that occur in practice.\n\nOnce you compress there is less redundancy to take advantage of and it's more likely your input will be one of the ones that take more space after compression.\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;All compression algorithms have some inputs which take more space and some that take less space.  However, they design the compression algorithm to make the inputs which take less the ones that occur in practice.&lt;/p&gt;\n\n&lt;p&gt;Once you compress there is less redundancy to take advantage of and it&amp;#39;s more likely your input will be one of the ones that take more space after compression.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5wgux", "score_hidden": false, "stickied": false, "created": 1492034779.0, "created_utc": 1492005979.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 12}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5zecc", "gilded": 0, "archived": false, "score": 11, "report_reasons": null, "author": "-Tesserex-", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Many other replies have explained why correctly, but I want to point out that if you're talking about lossless compression, it's mathematical fact that there can be no algorithm that compresses ANY input into a smaller output. Here's why:\n\nImagine our input strings are 10 lowercase alpha characters. I claim to have a lossless algorithm that can make ANY input smaller. The output I give is only 5 characters. This is impossible. For 10 character inputs, there are 26^10 or about 140 trillion possible combinations. For 5 characters, there are only 26^5, or 11,881,376 possible outputs. Since this should be lossless, it means that every output matches *exactly* one input. But that means that we can only represent 11,881,376 inputs. What happened to the other trillions of inputs? The only options are that the output values are reused, meaning the algorithm is lossy (you can't know which input was the one you were given) or some inputs actually get larger.\n\nNote that even if I said my method gave outputs of 9 characters or less, that's still fewer possible combinations than 10 characters. Also note that the metadata, the part where you're describing the substitutions and such, all counts toward the output file size. If you have 1 kb of \"data\" and 100 mb of \"metadata\" you haven't really accomplished anything.\n\nThere have been people in the past who fraudulently claimed they had developed such a mathematically impossible algorithm and tried to dupe companies into buying them or incorporating them into hardware. This is one of those things that used to get argued about endlessly in the earlier days of the internet.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Many other replies have explained why correctly, but I want to point out that if you&amp;#39;re talking about lossless compression, it&amp;#39;s mathematical fact that there can be no algorithm that compresses ANY input into a smaller output. Here&amp;#39;s why:&lt;/p&gt;\n\n&lt;p&gt;Imagine our input strings are 10 lowercase alpha characters. I claim to have a lossless algorithm that can make ANY input smaller. The output I give is only 5 characters. This is impossible. For 10 character inputs, there are 26&lt;sup&gt;10&lt;/sup&gt; or about 140 trillion possible combinations. For 5 characters, there are only 26&lt;sup&gt;5,&lt;/sup&gt; or 11,881,376 possible outputs. Since this should be lossless, it means that every output matches &lt;em&gt;exactly&lt;/em&gt; one input. But that means that we can only represent 11,881,376 inputs. What happened to the other trillions of inputs? The only options are that the output values are reused, meaning the algorithm is lossy (you can&amp;#39;t know which input was the one you were given) or some inputs actually get larger.&lt;/p&gt;\n\n&lt;p&gt;Note that even if I said my method gave outputs of 9 characters or less, that&amp;#39;s still fewer possible combinations than 10 characters. Also note that the metadata, the part where you&amp;#39;re describing the substitutions and such, all counts toward the output file size. If you have 1 kb of &amp;quot;data&amp;quot; and 100 mb of &amp;quot;metadata&amp;quot; you haven&amp;#39;t really accomplished anything.&lt;/p&gt;\n\n&lt;p&gt;There have been people in the past who fraudulently claimed they had developed such a mathematically impossible algorithm and tried to dupe companies into buying them or incorporating them into hardware. This is one of those things that used to get argued about endlessly in the earlier days of the internet.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5zecc", "score_hidden": false, "stickied": false, "created": 1492038220.0, "created_utc": 1492009420.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 11}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5wolv", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "Rannasha", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "I have a generic understanding of the process, I don't know the particular details of specific algorithms such as ZIP. But the purpose of compression is to take structured, repetitive data and transform it into something that looks much more \"random\" (between quotes since the output isn't actually random at all). If an algorithm is well designed, then it will already exploit almost all of the opportunities to reduce the structure and repetition in the data by \"shorthands\" and that means that applying another algorithm (or the same one a second time) will not be very effective.\n\nOn the other hand, if the algorithm used is very basic, then there may still be plenty of room for further compression and applying another basic algorithm, but one that works in a different way, will be useful. However, the algorithms used by popular compression applications are far from basic and leave little room for further optimization.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a generic understanding of the process, I don&amp;#39;t know the particular details of specific algorithms such as ZIP. But the purpose of compression is to take structured, repetitive data and transform it into something that looks much more &amp;quot;random&amp;quot; (between quotes since the output isn&amp;#39;t actually random at all). If an algorithm is well designed, then it will already exploit almost all of the opportunities to reduce the structure and repetition in the data by &amp;quot;shorthands&amp;quot; and that means that applying another algorithm (or the same one a second time) will not be very effective.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, if the algorithm used is very basic, then there may still be plenty of room for further compression and applying another basic algorithm, but one that works in a different way, will be useful. However, the algorithms used by popular compression applications are far from basic and leave little room for further optimization.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5wolv", "score_hidden": false, "stickied": false, "created": 1492035054.0, "created_utc": 1492006254.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5zqn5", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "Iron_Pencil", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "You may want to look up [Kolgomorov complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity) it's a mathematical description of the shortest way you can describe a particular piece of information.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You may want to look up &lt;a href=\"https://en.wikipedia.org/wiki/Kolmogorov_complexity\"&gt;Kolgomorov complexity&lt;/a&gt; it&amp;#39;s a mathematical description of the shortest way you can describe a particular piece of information.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5zqn5", "score_hidden": false, "stickied": false, "created": 1492038601.0, "created_utc": 1492009801.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg62azv", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "orbitaldan", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "As a thought experiment, imagine you had an algorithm that *could* always make it smaller (without data loss).  As you keep applying it over and over, the arbitrary test file you start with gets smaller and smaller until it is only a single bit in size.  Now you have a problem, because that would mean that only two files could ever exist: the one that reduces to 0 and the one that reduces to 1.  Since that's obviously absurd, we can conclude that there has to be a limit for how small a file can be compressed.\n\nAll compression algorithms come from finding patterns in the data and writing those patterns in more compact forms - once those forms are written as compact as possible, you can't compress it any further.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As a thought experiment, imagine you had an algorithm that &lt;em&gt;could&lt;/em&gt; always make it smaller (without data loss).  As you keep applying it over and over, the arbitrary test file you start with gets smaller and smaller until it is only a single bit in size.  Now you have a problem, because that would mean that only two files could ever exist: the one that reduces to 0 and the one that reduces to 1.  Since that&amp;#39;s obviously absurd, we can conclude that there has to be a limit for how small a file can be compressed.&lt;/p&gt;\n\n&lt;p&gt;All compression algorithms come from finding patterns in the data and writing those patterns in more compact forms - once those forms are written as compact as possible, you can&amp;#39;t compress it any further.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg62azv", "score_hidden": false, "stickied": false, "created": 1492041420.0, "created_utc": 1492012620.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5yif0", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "Kimundi", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "The basic problem is that that a compression algorithm searches for redunancy in the input and stores them in a output without such redundancies.\n\nLook at the `\"aaaaaaaa\" -&gt; \"8a\"` example - after the first compression you reduced the size from 8 to 2, but afterwards there is nothing to shrink anymore.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The basic problem is that that a compression algorithm searches for redunancy in the input and stores them in a output without such redundancies.&lt;/p&gt;\n\n&lt;p&gt;Look at the &lt;code&gt;&amp;quot;aaaaaaaa&amp;quot; -&amp;gt; &amp;quot;8a&amp;quot;&lt;/code&gt; example - after the first compression you reduced the size from 8 to 2, but afterwards there is nothing to shrink anymore.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5yif0", "score_hidden": false, "stickied": false, "created": 1492037214.0, "created_utc": 1492008414.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5z751", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "Happy_Bridge", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Compression relies on there being patterns in the data that you want to compress.  If you look at data compressed by a good algorithm, you will see that there are no patterns left - it appears to be a bunch of random bytes with no pattern. \n\nThis is for lossless compression like a zip file. Lossy compression algorithms like JPEG manipulate the source data to create patterns that can be compressed.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Compression relies on there being patterns in the data that you want to compress.  If you look at data compressed by a good algorithm, you will see that there are no patterns left - it appears to be a bunch of random bytes with no pattern. &lt;/p&gt;\n\n&lt;p&gt;This is for lossless compression like a zip file. Lossy compression algorithms like JPEG manipulate the source data to create patterns that can be compressed.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5z751", "score_hidden": false, "stickied": false, "created": 1492037993.0, "created_utc": 1492009193.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg612ox", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "reven80", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Assuming you are dealing with \"lossless\" compression there is an ultimate limit to how much you can compress something and most good compression algorithms there days reach that limit in one pass. Sometimes these algorithms are actually implemented in multiple passes of different algorithms run one after another.\n\nNow why can you not compress something smaller and smaller forever? Well there is no guaranteed ways to do this for all data. Imagine if you tried to compress an email into a single number between 1 and 100. Think of converting that number back to the original email content. There are way more than 100 email in existence so you cannot uniquely identify the original email content.\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Assuming you are dealing with &amp;quot;lossless&amp;quot; compression there is an ultimate limit to how much you can compress something and most good compression algorithms there days reach that limit in one pass. Sometimes these algorithms are actually implemented in multiple passes of different algorithms run one after another.&lt;/p&gt;\n\n&lt;p&gt;Now why can you not compress something smaller and smaller forever? Well there is no guaranteed ways to do this for all data. Imagine if you tried to compress an email into a single number between 1 and 100. Think of converting that number back to the original email content. There are way more than 100 email in existence so you cannot uniquely identify the original email content.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg612ox", "score_hidden": false, "stickied": false, "created": 1492040070.0, "created_utc": 1492011270.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg61fu9", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "ericGraves", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Uncompressed files have a large amount of redundancy. For example a file like 00010010000100010000 may have only 4 ones out of 20 possible symbols. Lossless compression these sequences to shorter ones, where the ratio is more even. In the above example it would map it to some sequence like 111010010101011, which is shorter and has a more even ratio. You can't reduce again, since the distribution of 0s and 1s is uniform. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Uncompressed files have a large amount of redundancy. For example a file like 00010010000100010000 may have only 4 ones out of 20 possible symbols. Lossless compression these sequences to shorter ones, where the ratio is more even. In the above example it would map it to some sequence like 111010010101011, which is shorter and has a more even ratio. You can&amp;#39;t reduce again, since the distribution of 0s and 1s is uniform. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61fu9", "score_hidden": false, "stickied": false, "created": 1492040473.0, "created_utc": 1492011673.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg68r6j", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "as_one_does", "parent_id": "t1_dg61etc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt; bzip2 then bzip2\t710955\t20.2%\n\nAlmost certainly bigger because you're re-compressing the block-headers/checksums and adding new ones.  ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;bzip2 then bzip2  710955  20.2%&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Almost certainly bigger because you&amp;#39;re re-compressing the block-headers/checksums and adding new ones.  &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg68r6j", "score_hidden": false, "stickied": false, "created": 1492048070.0, "created_utc": 1492019270.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg61etc", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "djimbob", "parent_id": "t1_dg5ywpy", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "While you may try different (lossless) compression algorithms that compress redundant data at different success rates, you generally will have poor results when running compression algorithms on already compressed data.\n\nReal data, like plain text or images or video, compresses well, because it has obvious redundancies in the data (e.g., in text some character patterns occur more frequently and can be given a short code; in images and video the color of adjacent pixels is very often related to its neighbors).  \n\nBut after the data has been compressed by a good algorithm, the data generally looks like random noise and in general random noise does not compress well.\n\nFor example, if I take some random CSV spreadsheet (text) and compress it with:\n\nCompression|File Size|% of original\n---|---|---\nOriginal file|  3516615 | 100%\ngzip|1136637|32.3%\nbzip2|707325|20.1%\ngzip then bzip2 | 1142542|32.5%\nbzip2 then gzip|707446|20.1%\nbzip2 then bzip2|710955|20.2%\n\nThis shows that running bzip2 or gzip on an already compressed file doesn't further shrink the file (in some cases with some algorithms you make get some slight further compression).", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;While you may try different (lossless) compression algorithms that compress redundant data at different success rates, you generally will have poor results when running compression algorithms on already compressed data.&lt;/p&gt;\n\n&lt;p&gt;Real data, like plain text or images or video, compresses well, because it has obvious redundancies in the data (e.g., in text some character patterns occur more frequently and can be given a short code; in images and video the color of adjacent pixels is very often related to its neighbors).  &lt;/p&gt;\n\n&lt;p&gt;But after the data has been compressed by a good algorithm, the data generally looks like random noise and in general random noise does not compress well.&lt;/p&gt;\n\n&lt;p&gt;For example, if I take some random CSV spreadsheet (text) and compress it with:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Compression&lt;/th&gt;\n&lt;th&gt;File Size&lt;/th&gt;\n&lt;th&gt;% of original&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;Original file&lt;/td&gt;\n&lt;td&gt;3516615&lt;/td&gt;\n&lt;td&gt;100%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;gzip&lt;/td&gt;\n&lt;td&gt;1136637&lt;/td&gt;\n&lt;td&gt;32.3%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;bzip2&lt;/td&gt;\n&lt;td&gt;707325&lt;/td&gt;\n&lt;td&gt;20.1%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;gzip then bzip2&lt;/td&gt;\n&lt;td&gt;1142542&lt;/td&gt;\n&lt;td&gt;32.5%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;bzip2 then gzip&lt;/td&gt;\n&lt;td&gt;707446&lt;/td&gt;\n&lt;td&gt;20.1%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;bzip2 then bzip2&lt;/td&gt;\n&lt;td&gt;710955&lt;/td&gt;\n&lt;td&gt;20.2%&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;This shows that running bzip2 or gzip on an already compressed file doesn&amp;#39;t further shrink the file (in some cases with some algorithms you make get some slight further compression).&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61etc", "score_hidden": false, "stickied": false, "created": 1492040440.0, "created_utc": 1492011640.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 9}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5ywpy", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "krazytekn0", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It is possible to apply a slightly different alogorithm and get more compression but eventually you will end up with a file with no repetitions and no longer be able to compress. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is possible to apply a slightly different alogorithm and get more compression but eventually you will end up with a file with no repetitions and no longer be able to compress. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5ywpy", "score_hidden": false, "stickied": false, "created": 1492037663.0, "created_utc": 1492008863.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg60zv6", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "gqcwwjtg", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Imagine that you have a compression algorithm C, where C(x) gives you a compressed version of x.\n\nFor this to actually be a compressed version of, the data, there must be some algorithm C' such that C'(C(x)) = x.  If you can't unzip the file, then zipping it might as well just be deleting it. \n\nLet's say that we are compressing a file that has n bits. There are, then, 2^n different things that our file, x, could be. Since C(x) must be smaller than x, there are less than 2^n things that C(x) could be. When you try to fit 2^n things in less than 2^n slots, at least one of the slots has more than one thing in it, so there is some n bit file x and some different n bit file y, C(x)=C(y). Applying C' to each side, we get C'(C(x))=C'(C(y)). Simplifying, we see that x=y, which is not true.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Imagine that you have a compression algorithm C, where C(x) gives you a compressed version of x.&lt;/p&gt;\n\n&lt;p&gt;For this to actually be a compressed version of, the data, there must be some algorithm C&amp;#39; such that C&amp;#39;(C(x)) = x.  If you can&amp;#39;t unzip the file, then zipping it might as well just be deleting it. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say that we are compressing a file that has n bits. There are, then, 2&lt;sup&gt;n&lt;/sup&gt; different things that our file, x, could be. Since C(x) must be smaller than x, there are less than 2&lt;sup&gt;n&lt;/sup&gt; things that C(x) could be. When you try to fit 2&lt;sup&gt;n&lt;/sup&gt; things in less than 2&lt;sup&gt;n&lt;/sup&gt; slots, at least one of the slots has more than one thing in it, so there is some n bit file x and some different n bit file y, C(x)=C(y). Applying C&amp;#39; to each side, we get C&amp;#39;(C(x))=C&amp;#39;(C(y)). Simplifying, we see that x=y, which is not true.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg60zv6", "score_hidden": false, "stickied": false, "created": 1492039984.0, "created_utc": 1492011184.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg65joc", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "the_crowz", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Because at some point pretty quickly there is no further interesting pattern in the data to take advantage of and compress.\n\nA totally structureless file (very random) will hardly compress because there's no pattern to describe.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because at some point pretty quickly there is no further interesting pattern in the data to take advantage of and compress.&lt;/p&gt;\n\n&lt;p&gt;A totally structureless file (very random) will hardly compress because there&amp;#39;s no pattern to describe.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg65joc", "score_hidden": false, "stickied": false, "created": 1492044836.0, "created_utc": 1492016036.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg65tj1", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "chadmill3r", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Consider the final case. Do you end up with a file of zero size?\n\nIf your compression can make a file that is more compressible with another run, then you have a bad compression idea. Every good compression makes something that is almost random noise, which is incompressible because nothing can efficiently represent that exact randomness except that randomness.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Consider the final case. Do you end up with a file of zero size?&lt;/p&gt;\n\n&lt;p&gt;If your compression can make a file that is more compressible with another run, then you have a bad compression idea. Every good compression makes something that is almost random noise, which is incompressible because nothing can efficiently represent that exact randomness except that randomness.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg65tj1", "score_hidden": false, "stickied": false, "created": 1492045119.0, "created_utc": 1492016319.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg621u0", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "butlertd", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "You can create examples where zipping a ZIP file actually does greatly reduce file size.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can create examples where zipping a ZIP file actually does greatly reduce file size.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg621u0", "score_hidden": false, "stickied": false, "created": 1492041146.0, "created_utc": 1492012346.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg63msn", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "conall88", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "some data types are inherently unable to be compressed. If the computer cannot find the right way to describe the data in such a manner that it can replicate it via an algorithm, it won't be compressed. Obviously an already compressed dataset would be a good example of this. Its like trying to fold paper more than 6 times!\n\n\n also on a loosely related note, if the minimum size of data to be stored in computer memory is a \"bit\", then data cannot be smaller than this as you can't store it.\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;some data types are inherently unable to be compressed. If the computer cannot find the right way to describe the data in such a manner that it can replicate it via an algorithm, it won&amp;#39;t be compressed. Obviously an already compressed dataset would be a good example of this. Its like trying to fold paper more than 6 times!&lt;/p&gt;\n\n&lt;p&gt;also on a loosely related note, if the minimum size of data to be stored in computer memory is a &amp;quot;bit&amp;quot;, then data cannot be smaller than this as you can&amp;#39;t store it.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg63msn", "score_hidden": false, "stickied": false, "created": 1492042843.0, "created_utc": 1492014043.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg64mq1", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "techiemikey", "parent_id": "t1_dg5w1bb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "I will give you an example of why things usually can't be made smaller.  Let's say you are compressing just numbers.  You are compressing 88888554444446666698999999444444422222288888866644444444422 simply by saying how many instances of a number there are.\nThe first time you run it through the compression algorithm, you would get \"58256456191869746268369422\".  If you run the same exact algorithm a second time, it would turn into \"15181215161415161119111816191714161216181316191422\"\nThe next time you ran it, it would be \"111511181112111511161114111511163119311811161119111711141116111211161118111311161119111422\" which is even longer that the original string.  If you run it again, it might shrink a little bit, but in essence compression algorithms are designed to work on uncompressed information, rather than compressed information.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I will give you an example of why things usually can&amp;#39;t be made smaller.  Let&amp;#39;s say you are compressing just numbers.  You are compressing 88888554444446666698999999444444422222288888866644444444422 simply by saying how many instances of a number there are.\nThe first time you run it through the compression algorithm, you would get &amp;quot;58256456191869746268369422&amp;quot;.  If you run the same exact algorithm a second time, it would turn into &amp;quot;15181215161415161119111816191714161216181316191422&amp;quot;\nThe next time you ran it, it would be &amp;quot;111511181112111511161114111511163119311811161119111711141116111211161118111311161119111422&amp;quot; which is even longer that the original string.  If you run it again, it might shrink a little bit, but in essence compression algorithms are designed to work on uncompressed information, rather than compressed information.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg64mq1", "score_hidden": false, "stickied": false, "created": 1492043890.0, "created_utc": 1492015090.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg5w1bb", "gilded": 0, "archived": false, "score": 120, "report_reasons": null, "author": "disord3r", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Since you have a good understanding of the process, can you explain why it's not possible to keep applying more and more such reduction algorithms to the output of the preivous one and keep squashing the source smaller and smaller? It's commonly known that zipping a zip, for example, doesn't enjoy the same benefits as the first compression.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Since you have a good understanding of the process, can you explain why it&amp;#39;s not possible to keep applying more and more such reduction algorithms to the output of the preivous one and keep squashing the source smaller and smaller? It&amp;#39;s commonly known that zipping a zip, for example, doesn&amp;#39;t enjoy the same benefits as the first compression.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5w1bb", "score_hidden": false, "stickied": false, "created": 1492034229.0, "created_utc": 1492005429.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 120}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg5zd2o", "gilded": 0, "archived": false, "score": 10, "report_reasons": null, "author": "drinkmorecoffee", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Fantastic explanation! Thank you.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fantastic explanation! Thank you.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5zd2o", "score_hidden": false, "stickied": false, "created": 1492038180.0, "created_utc": 1492009380.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 10}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg62d0d", "gilded": 0, "archived": false, "score": 10, "report_reasons": null, "author": "Malcmodnar", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "A fun bit of related trivia: because different languages are structured differently (with varying word lengths and patterns), some languages compress more efficiently than others. Translating text into another language can change its size!\n\nSource: my middle school science fair project.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A fun bit of related trivia: because different languages are structured differently (with varying word lengths and patterns), some languages compress more efficiently than others. Translating text into another language can change its size!&lt;/p&gt;\n\n&lt;p&gt;Source: my middle school science fair project.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg62d0d", "score_hidden": false, "stickied": false, "created": 1492041480.0, "created_utc": 1492012680.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 10}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg635i5", "gilded": 0, "archived": false, "score": 18, "report_reasons": null, "author": "CrimsonMoose", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Partially:\n\n{1}is a way to {2}{3}store {4}. It's best explained {5} a simplified {6}.\n{7} I have a {8} {9}{10}the string \"aaaaaaaaaaaaaaaaaaaa\" (w/o quotes). {11}is 20 {12} of {4} (or 20 bytes using basic ASCII encoding). If I know {9}{8}s of {11}type rarely {14} anything other than letters, I {18}replace {11}string by \"20a\" and program my software to read {11}as an instruction to create a string of 20 a's.\nBut {13}happens if I want to use the same function for something {9}does {14} a {15}? Well, I {18}decide {9}any {15} {9}is to be {16} literally, rather than as part of an instruction is preceded by a -symbol (and then add the special case of using \\ whenever I want to represent a single ).\nIn certain cases, where the {8} doesn't {14} many {15}s or slashes, the size of the {8} can be reduced by {11}{17} notation. In some special cases, the {8}size actually increases due to the rules I introduced to properly represent {15}s and slashes.\nNext up a {1}tool might replace recurring pieces of {4} by a symbol {9}takes up {19} less space. {7} a piece of text {10}many instances of a certain word, then the software {18}replace {9}word by a single character/symbol. In order to ensure {9}the de{1}software knows what's going on, the {8} format {18}be such {9}it first includes a list of these substitutions, followed by a specific symbol (or combination thereof) {9}marks the actual content.\nA practical {6}. Lets use both of the previous concepts (replacement of repeated {4} in a sequence by a {15} and a single instance of {9}{4} and replacement of freqeuntly occurring {4} by a special symbol and a \"dictionary\" at the start of the {8}). We use the format \"X=word\" at the start of the text to define a substitution of \"word\" by symbol \"X\", {5} the actual text starting {5} a !. We use the \\ to indicate {9}the following character has no special meaning and should be {16} literally.\nThe text is:\nI'm going to write Reddit 5 times (RedditRedditRedditRedditReddit) and post it on Reddit.\n{11}line has 90 {12}. Applying our {1}algorithm, we get:\n$=Reddit!I'm going to write $ \\5 times (5$) and post it on $.\n{11}line has 62 {12}. A reduction of a third. Note {9}{11}algorithm is very simplistic and {18}still be improved.\nAnother technique {9}can be used is reducing the size of the alphabet. Using standard ASCII encoding, 1 character uses 1 byte of space, but {11}1 byte allows for 256 different {12} to be expressed. If I know {9}a {8} only {10}lowercase letters, I only need 26 different {12}, which can be covered {5} just 5 out of the 8 bits {9}make up a byte. So for the first character, I don't use the full byte, but rather just the first 5 bits and for the next character, I use 3 remaining bits of the first byte and 2 bits from the next byte, etc...\nNow a {8} like {11}can only be {16} correctly if the software on the other end knows it's dealing {5} a {8} {9}uses 5 bits to encode a lowercase letter. {11}is rather inflexible. So {13}I can do is to include a special header in the {8}, a small piece of {4} {9}{10}the details of the encoding used, in {11}case it will mention {9}each character uses 5 bits and then has a list of all the {12} {9}are used. {11}header takes up some space, so reduces the efficiency of the {20}ion, but it allows the {1}software to use any type of character-set {9}it likes, making it useable for any {8}.\nIn reality, ZIP and other {1}techniques are {19} {2}complex than the {6}s I've demonstrated above. But the basic concepts remains the same: {1}is achieved by storing existing {4} in a {2}efficient way using some form of {17} notation. {11}{17} notation is part of the official standard for the {20}ion-system, so developers can create software to follow these rules and correctly de{20} a {20}ed {8}, recreating the original {4}.\nJust like in my {6}s, {1}works better on some {8}s than on others. A simple text {8} {5} a lot of repetition will be very easy to {20}, the reduction in {8} size can be quite large in these cases. On the other hand, a {8} {9}{10}{4} {9}is apparently random in nature will benefit very little, if anything, from {20}ion.\nA final remark. All of the above is about \"lossless\" {20}ion. {11}form of {1}means {9}the no information is lost during the {20}ion/de{1}process. If you {20} a {8} and then de{20} it using a lossless algorithm, then the two {8}s will be exactly the same, bit by bit.\nAnother form of {1}is \"lossy\" {20}ion. Where lossless {1}tries to figure out how {4} can be stored {2}efficiently, lossy {1}tries to figure out {13}{4} can be safely discarded {5}out it affecting the purpose of the {8}. Well known {6}s of lossy {1}are various {8} formats for images, sound or video.\nIn the case of images, the JPEG format will try to discard nuances in the image {9}are not noticable by a regular human observer. For {6}, if two neighbouring pixels are almost exactly the same colour, you {18}set both to the same colour value. Most lossy formats have the option to set how aggressive {11}{1}is, which is {13}the \"quality\" setting is for when saving JPEG {8}s {5} any reasonably sophisticated image editor. The {2}aggressive the {20}ion, the greater the reduction in {8}size, but the {2}{4} {9}is discarded. And at some point, {11}can lead to visible degradation of the image. So-called \"JPEG artifacts\" are an {6} of image degradation due to the application of aggressive lossy {1}(or the repeat application thereof, the image quality decreases every time a JPEG {8} is saved).\n\n{{1='compression '},{2='more ',{3='efficiently '},{4='data'},{5='with'},{6='example'},{7='suppose'},{8='file'},{9='that '},{10='contains '},{11='this '},{12='characters'},{13='what '},{14='contain'},{15='number'},{16='interpreted'},{17='shorthand'},{18='could '},{19='considerably'},{20='compress'}", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Partially:&lt;/p&gt;\n\n&lt;p&gt;{1}is a way to {2}{3}store {4}. It&amp;#39;s best explained {5} a simplified {6}.\n{7} I have a {8} {9}{10}the string &amp;quot;aaaaaaaaaaaaaaaaaaaa&amp;quot; (w/o quotes). {11}is 20 {12} of {4} (or 20 bytes using basic ASCII encoding). If I know {9}{8}s of {11}type rarely {14} anything other than letters, I {18}replace {11}string by &amp;quot;20a&amp;quot; and program my software to read {11}as an instruction to create a string of 20 a&amp;#39;s.\nBut {13}happens if I want to use the same function for something {9}does {14} a {15}? Well, I {18}decide {9}any {15} {9}is to be {16} literally, rather than as part of an instruction is preceded by a -symbol (and then add the special case of using \\ whenever I want to represent a single ).\nIn certain cases, where the {8} doesn&amp;#39;t {14} many {15}s or slashes, the size of the {8} can be reduced by {11}{17} notation. In some special cases, the {8}size actually increases due to the rules I introduced to properly represent {15}s and slashes.\nNext up a {1}tool might replace recurring pieces of {4} by a symbol {9}takes up {19} less space. {7} a piece of text {10}many instances of a certain word, then the software {18}replace {9}word by a single character/symbol. In order to ensure {9}the de{1}software knows what&amp;#39;s going on, the {8} format {18}be such {9}it first includes a list of these substitutions, followed by a specific symbol (or combination thereof) {9}marks the actual content.\nA practical {6}. Lets use both of the previous concepts (replacement of repeated {4} in a sequence by a {15} and a single instance of {9}{4} and replacement of freqeuntly occurring {4} by a special symbol and a &amp;quot;dictionary&amp;quot; at the start of the {8}). We use the format &amp;quot;X=word&amp;quot; at the start of the text to define a substitution of &amp;quot;word&amp;quot; by symbol &amp;quot;X&amp;quot;, {5} the actual text starting {5} a !. We use the \\ to indicate {9}the following character has no special meaning and should be {16} literally.\nThe text is:\nI&amp;#39;m going to write Reddit 5 times (RedditRedditRedditRedditReddit) and post it on Reddit.\n{11}line has 90 {12}. Applying our {1}algorithm, we get:\n$=Reddit!I&amp;#39;m going to write $ \\5 times (5$) and post it on $.\n{11}line has 62 {12}. A reduction of a third. Note {9}{11}algorithm is very simplistic and {18}still be improved.\nAnother technique {9}can be used is reducing the size of the alphabet. Using standard ASCII encoding, 1 character uses 1 byte of space, but {11}1 byte allows for 256 different {12} to be expressed. If I know {9}a {8} only {10}lowercase letters, I only need 26 different {12}, which can be covered {5} just 5 out of the 8 bits {9}make up a byte. So for the first character, I don&amp;#39;t use the full byte, but rather just the first 5 bits and for the next character, I use 3 remaining bits of the first byte and 2 bits from the next byte, etc...\nNow a {8} like {11}can only be {16} correctly if the software on the other end knows it&amp;#39;s dealing {5} a {8} {9}uses 5 bits to encode a lowercase letter. {11}is rather inflexible. So {13}I can do is to include a special header in the {8}, a small piece of {4} {9}{10}the details of the encoding used, in {11}case it will mention {9}each character uses 5 bits and then has a list of all the {12} {9}are used. {11}header takes up some space, so reduces the efficiency of the {20}ion, but it allows the {1}software to use any type of character-set {9}it likes, making it useable for any {8}.\nIn reality, ZIP and other {1}techniques are {19} {2}complex than the {6}s I&amp;#39;ve demonstrated above. But the basic concepts remains the same: {1}is achieved by storing existing {4} in a {2}efficient way using some form of {17} notation. {11}{17} notation is part of the official standard for the {20}ion-system, so developers can create software to follow these rules and correctly de{20} a {20}ed {8}, recreating the original {4}.\nJust like in my {6}s, {1}works better on some {8}s than on others. A simple text {8} {5} a lot of repetition will be very easy to {20}, the reduction in {8} size can be quite large in these cases. On the other hand, a {8} {9}{10}{4} {9}is apparently random in nature will benefit very little, if anything, from {20}ion.\nA final remark. All of the above is about &amp;quot;lossless&amp;quot; {20}ion. {11}form of {1}means {9}the no information is lost during the {20}ion/de{1}process. If you {20} a {8} and then de{20} it using a lossless algorithm, then the two {8}s will be exactly the same, bit by bit.\nAnother form of {1}is &amp;quot;lossy&amp;quot; {20}ion. Where lossless {1}tries to figure out how {4} can be stored {2}efficiently, lossy {1}tries to figure out {13}{4} can be safely discarded {5}out it affecting the purpose of the {8}. Well known {6}s of lossy {1}are various {8} formats for images, sound or video.\nIn the case of images, the JPEG format will try to discard nuances in the image {9}are not noticable by a regular human observer. For {6}, if two neighbouring pixels are almost exactly the same colour, you {18}set both to the same colour value. Most lossy formats have the option to set how aggressive {11}{1}is, which is {13}the &amp;quot;quality&amp;quot; setting is for when saving JPEG {8}s {5} any reasonably sophisticated image editor. The {2}aggressive the {20}ion, the greater the reduction in {8}size, but the {2}{4} {9}is discarded. And at some point, {11}can lead to visible degradation of the image. So-called &amp;quot;JPEG artifacts&amp;quot; are an {6} of image degradation due to the application of aggressive lossy {1}(or the repeat application thereof, the image quality decreases every time a JPEG {8} is saved).&lt;/p&gt;\n\n&lt;p&gt;{{1=&amp;#39;compression &amp;#39;},{2=&amp;#39;more &amp;#39;,{3=&amp;#39;efficiently &amp;#39;},{4=&amp;#39;data&amp;#39;},{5=&amp;#39;with&amp;#39;},{6=&amp;#39;example&amp;#39;},{7=&amp;#39;suppose&amp;#39;},{8=&amp;#39;file&amp;#39;},{9=&amp;#39;that &amp;#39;},{10=&amp;#39;contains &amp;#39;},{11=&amp;#39;this &amp;#39;},{12=&amp;#39;characters&amp;#39;},{13=&amp;#39;what &amp;#39;},{14=&amp;#39;contain&amp;#39;},{15=&amp;#39;number&amp;#39;},{16=&amp;#39;interpreted&amp;#39;},{17=&amp;#39;shorthand&amp;#39;},{18=&amp;#39;could &amp;#39;},{19=&amp;#39;considerably&amp;#39;},{20=&amp;#39;compress&amp;#39;}&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg635i5", "score_hidden": false, "stickied": false, "created": 1492042335.0, "created_utc": 1492013535.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 18}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg607mv", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "ZombieAlpacaLips", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "You touched on it a bit with your discussion of images, but it's important to note that there are numerous different ways to compress things that are designed/optimized for different types of data. If you know what your data is like, you can pick the best way to compress it.\n\nImages with large fields of the same color (for example, clipart of a stop sign) can be stored more efficiently as in PNG or GIF format, while a photograph is better stored as JPEG. A cartoon video could be best stored as instructions to the computer on what to draw where instead of actually storing the frames of the video, but then you need to also make sure that any device that is used to play back your cartoon knows how to execute those instructions.\n\nVideo compression keeps track of the pixels that stay the same between frames too, instead of just analyzing each frame individually. A movie with still camera shots and little motion will compress much more than one with a lot of activity. See [Tom Scott on video compression](https://www.youtube.com/watch?v=r6Rp-uo6HmI).\n\nAttempting to put images/sound/video into a ZIP file isn't going to help you, because you're effectively trying to compress the same thing with two different algorithms. The first algorithm is doing the lossy compression, and the second algorithm is going to attempt to compress that binary data without much success.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You touched on it a bit with your discussion of images, but it&amp;#39;s important to note that there are numerous different ways to compress things that are designed/optimized for different types of data. If you know what your data is like, you can pick the best way to compress it.&lt;/p&gt;\n\n&lt;p&gt;Images with large fields of the same color (for example, clipart of a stop sign) can be stored more efficiently as in PNG or GIF format, while a photograph is better stored as JPEG. A cartoon video could be best stored as instructions to the computer on what to draw where instead of actually storing the frames of the video, but then you need to also make sure that any device that is used to play back your cartoon knows how to execute those instructions.&lt;/p&gt;\n\n&lt;p&gt;Video compression keeps track of the pixels that stay the same between frames too, instead of just analyzing each frame individually. A movie with still camera shots and little motion will compress much more than one with a lot of activity. See &lt;a href=\"https://www.youtube.com/watch?v=r6Rp-uo6HmI\"&gt;Tom Scott on video compression&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Attempting to put images/sound/video into a ZIP file isn&amp;#39;t going to help you, because you&amp;#39;re effectively trying to compress the same thing with two different algorithms. The first algorithm is doing the lossy compression, and the second algorithm is going to attempt to compress that binary data without much success.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg607mv", "score_hidden": false, "stickied": false, "created": 1492039127.0, "created_utc": 1492010327.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 6}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6kq39", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "TheRaven1", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Thank you so much for answering! This is a great reply! I actually do understand this much better. Very appreciated. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you so much for answering! This is a great reply! I actually do understand this much better. Very appreciated. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6kq39", "score_hidden": false, "stickied": false, "created": 1492060515.0, "created_utc": 1492031715.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg63ub8", "gilded": 0, "archived": false, "score": 13, "report_reasons": null, "author": "Rannasha", "parent_id": "t1_dg62cs7", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It takes time to decompress a file. Especially for large files, this can add up considerably. And in many cases it is difficult to selectively decompress only sections of the file, since the data needed to compress a certain section may be spread out across the file, making it very inefficient to access specific parts of a file (with SSDs this problem has become less severe though).", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It takes time to decompress a file. Especially for large files, this can add up considerably. And in many cases it is difficult to selectively decompress only sections of the file, since the data needed to compress a certain section may be spread out across the file, making it very inefficient to access specific parts of a file (with SSDs this problem has become less severe though).&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg63ub8", "score_hidden": false, "stickied": false, "created": 1492043063.0, "created_utc": 1492014263.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 13}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg644mr", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "milkysniper", "parent_id": "t1_dg62cs7", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Yes, it takes sometimes a very significant amount of processing power to decompress", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, it takes sometimes a very significant amount of processing power to decompress&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg644mr", "score_hidden": false, "stickied": false, "created": 1492043361.0, "created_utc": 1492014561.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg644r8", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "dahimi", "parent_id": "t1_dg62cs7", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt; Does it take extra time/memory to decompress these files?\n\nYes.\n\nAdditionally, different kinds of data compress better using different methods.  For example audio compression works quite differently than video compression.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Does it take extra time/memory to decompress these files?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Yes.&lt;/p&gt;\n\n&lt;p&gt;Additionally, different kinds of data compress better using different methods.  For example audio compression works quite differently than video compression.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg644r8", "score_hidden": false, "stickied": false, "created": 1492043364.0, "created_utc": 1492014564.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg62cs7", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "olivertoad", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Could you explain why file compression isn't standard for all file formats or wherever possible? Does it take extra time/memory to decompress these files?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Could you explain why file compression isn&amp;#39;t standard for all file formats or wherever possible? Does it take extra time/memory to decompress these files?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg62cs7", "score_hidden": false, "stickied": false, "created": 1492041474.0, "created_utc": 1492012674.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg68kqm", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "ccooffee", "parent_id": "t1_dg66k9s", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "To expand on that for those that are curious - JPG is a \"lossy\" compression format compared to ZIP (a \"lossless\" format). A zip file unzips back to the original data, byte for byte. A JPG file will actually throw out data that you are unlikely to notice (the quality setting for creating a jpg basically tells it how careful to be when choosing what data to throw out). This results in a smaller file than what you would get from a zip file. But if you examine it close enough you could see where the quality is reduced. \nMP3 files are another example of lossy compression. Parts of the audio that you are unlikely to hear are thrown out in order to make the file even smaller.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;To expand on that for those that are curious - JPG is a &amp;quot;lossy&amp;quot; compression format compared to ZIP (a &amp;quot;lossless&amp;quot; format). A zip file unzips back to the original data, byte for byte. A JPG file will actually throw out data that you are unlikely to notice (the quality setting for creating a jpg basically tells it how careful to be when choosing what data to throw out). This results in a smaller file than what you would get from a zip file. But if you examine it close enough you could see where the quality is reduced. \nMP3 files are another example of lossy compression. Parts of the audio that you are unlikely to hear are thrown out in order to make the file even smaller.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg68kqm", "score_hidden": false, "stickied": false, "created": 1492047894.0, "created_utc": 1492019094.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 6}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg68tnw", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "frezik", "parent_id": "t1_dg66k9s", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "That's basically right. You also want to avoid using lossy compression again on something that's already lossy. The errors build up on top of each other, much like copying a copy of a VHS tape or a paper document.  A graphics artist might be resaving the file several times during the workflow, or copying from one image into another, so they want to use RAW as much as possible.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s basically right. You also want to avoid using lossy compression again on something that&amp;#39;s already lossy. The errors build up on top of each other, much like copying a copy of a VHS tape or a paper document.  A graphics artist might be resaving the file several times during the workflow, or copying from one image into another, so they want to use RAW as much as possible.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg68tnw", "score_hidden": false, "stickied": false, "created": 1492048138.0, "created_utc": 1492019338.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg66k9s", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "QueenoftheWaterways2", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Very well done!\n\nTo those who may have skimmed it, I will say this regarding images:\n\nA compressed image (JPG, png, etc.) is a LOT smaller than a RAW file.  To those only mildly interested in seeing a photo or drawing and perhaps posting it or emailing it to a friend - that's okay and no big whoop = most people online.\n\nA RAW file is HUGE and so it's not practical to email or upload on the common websites used for that sort of thing.  However, the RAW file is rather amazing in its detail and, therefore, the capabilities to fiddle with it regarding lighting, etc.\n\nOkay, professional graphic artists and photographers!  Correct me if I'm wrong!  I only work with you guys and this is how I understand it as far as their explaining it to me.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Very well done!&lt;/p&gt;\n\n&lt;p&gt;To those who may have skimmed it, I will say this regarding images:&lt;/p&gt;\n\n&lt;p&gt;A compressed image (JPG, png, etc.) is a LOT smaller than a RAW file.  To those only mildly interested in seeing a photo or drawing and perhaps posting it or emailing it to a friend - that&amp;#39;s okay and no big whoop = most people online.&lt;/p&gt;\n\n&lt;p&gt;A RAW file is HUGE and so it&amp;#39;s not practical to email or upload on the common websites used for that sort of thing.  However, the RAW file is rather amazing in its detail and, therefore, the capabilities to fiddle with it regarding lighting, etc.&lt;/p&gt;\n\n&lt;p&gt;Okay, professional graphic artists and photographers!  Correct me if I&amp;#39;m wrong!  I only work with you guys and this is how I understand it as far as their explaining it to me.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg66k9s", "score_hidden": false, "stickied": false, "created": 1492045876.0, "created_utc": 1492017076.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg64z9v", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "nelsonia", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Great explanation , I will now always picture in my mind how compression works in its basic form whenever I zip files or optimize an image for the web \ud83d\ude00", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Great explanation , I will now always picture in my mind how compression works in its basic form whenever I zip files or optimize an image for the web \ud83d\ude00&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg64z9v", "score_hidden": false, "stickied": false, "created": 1492044244.0, "created_utc": 1492015444.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg69j1g", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "adequate-zoo", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "An example of a compression algorithm that will almost always result in size increases would be PiFS. It's a sort of joke project on GitHub using pi as a kind of file system.  \n\nFor anyone unfamiliar the idea is that you convert your file to a sequence of numbers, locate that sequence in pi (since it theoretically contains every possible combination) and store the location and length of the sequence as your compressed file. \n\nSo if your file was this sentence and you converted it to a number. \n\nLike 4159. Your file would be represented by 3,4. Starts at the third digit and is 4 digits long. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;An example of a compression algorithm that will almost always result in size increases would be PiFS. It&amp;#39;s a sort of joke project on GitHub using pi as a kind of file system.  &lt;/p&gt;\n\n&lt;p&gt;For anyone unfamiliar the idea is that you convert your file to a sequence of numbers, locate that sequence in pi (since it theoretically contains every possible combination) and store the location and length of the sequence as your compressed file. &lt;/p&gt;\n\n&lt;p&gt;So if your file was this sentence and you converted it to a number. &lt;/p&gt;\n\n&lt;p&gt;Like 4159. Your file would be represented by 3,4. Starts at the third digit and is 4 digits long. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg69j1g", "score_hidden": false, "stickied": false, "created": 1492048856.0, "created_utc": 1492020056.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg624hq", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "Rannasha", "parent_id": "t1_dg61nak", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Most internet communication these days runs over HTTPS, which is encrypted. Encrypted data is much harder to compress, because in many ways it appears random.\n\nIf you and me both request a copy of this Reddit-thread at the same time (so without any intermediate changes to the actual content), then the datastream that we each receive will be completely different. Only after our machine has decrypted it, will they be the same.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Most internet communication these days runs over HTTPS, which is encrypted. Encrypted data is much harder to compress, because in many ways it appears random.&lt;/p&gt;\n\n&lt;p&gt;If you and me both request a copy of this Reddit-thread at the same time (so without any intermediate changes to the actual content), then the datastream that we each receive will be completely different. Only after our machine has decrypted it, will they be the same.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg624hq", "score_hidden": false, "stickied": false, "created": 1492041226.0, "created_utc": 1492012426.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg61nak", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "[deleted]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61nak", "score_hidden": false, "stickied": false, "created": 1492040703.0, "created_utc": 1492011903.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6tgqx", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "brainiac256", "parent_id": "t1_dg6i09w", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Maybe UUEncodes the machine code then prepends an interpreter? Then all you have to do is hack together a UUDecode that only uses printable characters in the machine code. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe UUEncodes the machine code then prepends an interpreter? Then all you have to do is hack together a UUDecode that only uses printable characters in the machine code. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6tgqx", "score_hidden": false, "stickied": false, "created": 1492071247.0, "created_utc": 1492042447.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6i09w", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "hobbycollector", "parent_id": "t1_dg6e5e0", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Hah! And I guess it would be more compressible than usual, but it would probably be initially bigger too by having to avoid certain instructions and optimizations.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hah! And I guess it would be more compressible than usual, but it would probably be initially bigger too by having to avoid certain instructions and optimizations.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6i09w", "score_hidden": false, "stickied": false, "created": 1492057564.0, "created_utc": 1492028764.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6e5e0", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "UncleMeat11", "parent_id": "t1_dg6ai5z", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Interestingly, somebody at this year's sigbovik wrote a compiler that produces binaries using only the printable bytes.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interestingly, somebody at this year&amp;#39;s sigbovik wrote a compiler that produces binaries using only the printable bytes.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6e5e0", "score_hidden": false, "stickied": false, "created": 1492053616.0, "created_utc": 1492024816.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 4}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6ai5z", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "hobbycollector", "parent_id": "t1_dg644jl", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Well, for example an executable file might contain machine code instructions, requiring all 256 8-bit values per byte. Not ascii per se, but a different file format that will not be as compressible.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, for example an executable file might contain machine code instructions, requiring all 256 8-bit values per byte. Not ascii per se, but a different file format that will not be as compressible.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6ai5z", "score_hidden": false, "stickied": false, "created": 1492049867.0, "created_utc": 1492021067.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 7}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg644jl", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "darexinfinity", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "What there's a file that takes all 256 ASCII values?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What there&amp;#39;s a file that takes all 256 ASCII values?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg644jl", "score_hidden": false, "stickied": false, "created": 1492043359.0, "created_utc": 1492014559.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg64zxx", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "lolwat_is_dis", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "This post is very good. Thanks!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This post is very good. Thanks!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg64zxx", "score_hidden": false, "stickied": false, "created": 1492044264.0, "created_utc": 1492015464.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg66ilh", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "mfukar", "parent_id": "t1_dg65tx1", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "ASCII defines no 'error-correcting bit', or any other means of error control.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ASCII defines no &amp;#39;error-correcting bit&amp;#39;, or any other means of error control.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg66ilh", "score_hidden": false, "stickied": false, "created": 1492045828.0, "created_utc": 1492017028.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 8}}], "after": null, "before": null}}, "user_reports": [], "id": "dg65tx1", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Grigorios", "parent_id": "t1_dg5t4zn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "How common is it to compress text by removing the error-correcting bit from ASCII?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How common is it to compress text by removing the error-correcting bit from ASCII?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg65tx1", "score_hidden": false, "stickied": false, "created": 1492045131.0, "created_utc": 1492016331.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg5t4zn", "gilded": 1, "archived": false, "score": 8863, "report_reasons": null, "author": "Rannasha", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Compression is a way to more efficiently store data. It's best explained with a simplified example.\n\nSuppose I have a file that contains the string \"aaaaaaaaaaaaaaaaaaaa\" (w/o quotes). This is 20 characters of data (or 20 bytes using basic ASCII encoding). If I know that files of this type rarely contain anything other than letters, I could replace this string by \"20a\" and program my software to read this as an instruction to create a string of 20 a's.\n\nBut what happens if I want to use the same function for something that does contain a number? Well, I could decide that any number that is to be interpreted literally, rather than as part of an instruction is preceded by a \\-symbol (and then add the special case of using \\\\ whenever I want to represent a single \\).\n\nIn certain cases, where the file doesn't contain many numbers or slashes, the size of the file can be reduced by this shorthand notation. In some special cases, the filesize actually increases due to the rules I introduced to properly represent numbers and slashes.\n\nNext up a compression tool might replace recurring pieces of data by a symbol that takes up considerably less space. Suppose a piece of text contains many instances of a certain word, then the software could replace that word by a single character/symbol. In order to ensure that the decompression software knows what's going on, the file format could be such that it first includes a list of these substitutions, followed by a specific symbol (or combination thereof) that marks the actual content.\n\nA practical example. Lets use both of the previous concepts (replacement of repeated data in a sequence by a number and a single instance of that data and replacement of freqeuntly occurring data by a special symbol and a \"dictionary\" at the start of the file). We use the format \"X=word\" at the start of the text to define a substitution of \"word\" by symbol \"X\", with the actual text starting with a !. We use the \\ to indicate that the following character has no special meaning and should be interpreted literally.\n\nThe text is:\n\nI'm going to write Reddit 5 times (RedditRedditRedditRedditReddit) and post it on Reddit.\n\nThis line has 90 characters. Applying our compression algorithm, we get:\n\n$=Reddit!I'm going to write $ \\5 times (5$) and post it on $.\n\nThis line has 62 characters. A reduction of a third. Note that this algorithm is very simplistic and could still be improved.\n\nAnother technique that can be used is reducing the size of the alphabet. Using standard ASCII encoding, 1 character uses 1 byte of space, but this 1 byte allows for 256 different characters to be expressed. If I know that a file only contains lowercase letters, I only need 26 different characters, which can be covered with just 5 out of the 8 bits that make up a byte. So for the first character, I don't use the full byte, but rather just the first 5 bits and for the next character, I use 3 remaining bits of the first byte and 2 bits from the next byte, etc...\n\nNow a file like this can only be interpreted correctly if the software on the other end knows it's dealing with a file that uses 5 bits to encode a lowercase letter. This is rather inflexible. So what I can do is to include a special header in the file, a small piece of data that contains the details of the encoding used, in this case it will mention that each character uses 5 bits and then has a list of all the characters that are used. This header takes up some space, so reduces the efficiency of the compression, but it allows the compression software to use any type of character-set that it likes, making it useable for any file.\n\nIn reality, ZIP and other compression techniques are considerably more complex than the examples I've demonstrated above. But the basic concepts remains the same: Compression is achieved by storing existing data in a more efficient way using some form of shorthand notation. This shorthand notation is part of the official standard for the compression-system, so developers can create software to follow these rules and correctly decompress a compressed file, recreating the original data.\n\nJust like in my examples, compression works better on some files than on others. A simple text file with a lot of repetition will be very easy to compress, the reduction in file size can be quite large in these cases. On the other hand, a file that contains data that is apparently random in nature will benefit very little, if anything, from compression.\n\nA final remark. All of the above is about \"lossless\" compression. This form of compression means that the no information is lost during the compression/decompression process. If you compress a file and then decompress it using a lossless algorithm, then the two files will be exactly the same, bit by bit.\n\nAnother form of compression is \"lossy\" compression. Where lossless compression tries to figure out how data can be stored more efficiently, lossy compression tries to figure out what data can be safely discarded without it affecting the purpose of the file. Well known examples of lossy compression are various file formats for images, sound or video.\n\nIn the case of images, the JPEG format will try to discard nuances in the image that are not noticable by a regular human observer. For example, if two neighbouring pixels are almost exactly the same colour, you could set both to the same colour value. Most lossy formats have the option to set how aggressive this compression is, which is what the \"quality\" setting is for when saving JPEG files with any reasonably sophisticated image editor. The more aggressive the compression, the greater the reduction in filesize, but the more data that is discarded. And at some point, this can lead to visible degradation of the image. So-called \"JPEG artifacts\" are an example of image degradation due to the application of aggressive lossy compression (or the repeat application thereof, the image quality decreases every time a JPEG file is saved).\n\nedit:\nFor a more detailed overview of the compression often used in ZIP files, see [this comment](https://www.reddit.com/r/askscience/comments/64xl01/what_is_a_zip_file_or_compressed_file_how_does/dg60pjr/) by /u/ericGraves ", "edited": 1492018360.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Compression is a way to more efficiently store data. It&amp;#39;s best explained with a simplified example.&lt;/p&gt;\n\n&lt;p&gt;Suppose I have a file that contains the string &amp;quot;aaaaaaaaaaaaaaaaaaaa&amp;quot; (w/o quotes). This is 20 characters of data (or 20 bytes using basic ASCII encoding). If I know that files of this type rarely contain anything other than letters, I could replace this string by &amp;quot;20a&amp;quot; and program my software to read this as an instruction to create a string of 20 a&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;But what happens if I want to use the same function for something that does contain a number? Well, I could decide that any number that is to be interpreted literally, rather than as part of an instruction is preceded by a -symbol (and then add the special case of using \\ whenever I want to represent a single ).&lt;/p&gt;\n\n&lt;p&gt;In certain cases, where the file doesn&amp;#39;t contain many numbers or slashes, the size of the file can be reduced by this shorthand notation. In some special cases, the filesize actually increases due to the rules I introduced to properly represent numbers and slashes.&lt;/p&gt;\n\n&lt;p&gt;Next up a compression tool might replace recurring pieces of data by a symbol that takes up considerably less space. Suppose a piece of text contains many instances of a certain word, then the software could replace that word by a single character/symbol. In order to ensure that the decompression software knows what&amp;#39;s going on, the file format could be such that it first includes a list of these substitutions, followed by a specific symbol (or combination thereof) that marks the actual content.&lt;/p&gt;\n\n&lt;p&gt;A practical example. Lets use both of the previous concepts (replacement of repeated data in a sequence by a number and a single instance of that data and replacement of freqeuntly occurring data by a special symbol and a &amp;quot;dictionary&amp;quot; at the start of the file). We use the format &amp;quot;X=word&amp;quot; at the start of the text to define a substitution of &amp;quot;word&amp;quot; by symbol &amp;quot;X&amp;quot;, with the actual text starting with a !. We use the \\ to indicate that the following character has no special meaning and should be interpreted literally.&lt;/p&gt;\n\n&lt;p&gt;The text is:&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to write Reddit 5 times (RedditRedditRedditRedditReddit) and post it on Reddit.&lt;/p&gt;\n\n&lt;p&gt;This line has 90 characters. Applying our compression algorithm, we get:&lt;/p&gt;\n\n&lt;p&gt;$=Reddit!I&amp;#39;m going to write $ \\5 times (5$) and post it on $.&lt;/p&gt;\n\n&lt;p&gt;This line has 62 characters. A reduction of a third. Note that this algorithm is very simplistic and could still be improved.&lt;/p&gt;\n\n&lt;p&gt;Another technique that can be used is reducing the size of the alphabet. Using standard ASCII encoding, 1 character uses 1 byte of space, but this 1 byte allows for 256 different characters to be expressed. If I know that a file only contains lowercase letters, I only need 26 different characters, which can be covered with just 5 out of the 8 bits that make up a byte. So for the first character, I don&amp;#39;t use the full byte, but rather just the first 5 bits and for the next character, I use 3 remaining bits of the first byte and 2 bits from the next byte, etc...&lt;/p&gt;\n\n&lt;p&gt;Now a file like this can only be interpreted correctly if the software on the other end knows it&amp;#39;s dealing with a file that uses 5 bits to encode a lowercase letter. This is rather inflexible. So what I can do is to include a special header in the file, a small piece of data that contains the details of the encoding used, in this case it will mention that each character uses 5 bits and then has a list of all the characters that are used. This header takes up some space, so reduces the efficiency of the compression, but it allows the compression software to use any type of character-set that it likes, making it useable for any file.&lt;/p&gt;\n\n&lt;p&gt;In reality, ZIP and other compression techniques are considerably more complex than the examples I&amp;#39;ve demonstrated above. But the basic concepts remains the same: Compression is achieved by storing existing data in a more efficient way using some form of shorthand notation. This shorthand notation is part of the official standard for the compression-system, so developers can create software to follow these rules and correctly decompress a compressed file, recreating the original data.&lt;/p&gt;\n\n&lt;p&gt;Just like in my examples, compression works better on some files than on others. A simple text file with a lot of repetition will be very easy to compress, the reduction in file size can be quite large in these cases. On the other hand, a file that contains data that is apparently random in nature will benefit very little, if anything, from compression.&lt;/p&gt;\n\n&lt;p&gt;A final remark. All of the above is about &amp;quot;lossless&amp;quot; compression. This form of compression means that the no information is lost during the compression/decompression process. If you compress a file and then decompress it using a lossless algorithm, then the two files will be exactly the same, bit by bit.&lt;/p&gt;\n\n&lt;p&gt;Another form of compression is &amp;quot;lossy&amp;quot; compression. Where lossless compression tries to figure out how data can be stored more efficiently, lossy compression tries to figure out what data can be safely discarded without it affecting the purpose of the file. Well known examples of lossy compression are various file formats for images, sound or video.&lt;/p&gt;\n\n&lt;p&gt;In the case of images, the JPEG format will try to discard nuances in the image that are not noticable by a regular human observer. For example, if two neighbouring pixels are almost exactly the same colour, you could set both to the same colour value. Most lossy formats have the option to set how aggressive this compression is, which is what the &amp;quot;quality&amp;quot; setting is for when saving JPEG files with any reasonably sophisticated image editor. The more aggressive the compression, the greater the reduction in filesize, but the more data that is discarded. And at some point, this can lead to visible degradation of the image. So-called &amp;quot;JPEG artifacts&amp;quot; are an example of image degradation due to the application of aggressive lossy compression (or the repeat application thereof, the image quality decreases every time a JPEG file is saved).&lt;/p&gt;\n\n&lt;p&gt;edit:\nFor a more detailed overview of the compression often used in ZIP files, see &lt;a href=\"https://www.reddit.com/r/askscience/comments/64xl01/what_is_a_zip_file_or_compressed_file_how_does/dg60pjr/\"&gt;this comment&lt;/a&gt; by &lt;a href=\"/u/ericGraves\"&gt;/u/ericGraves&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5t4zn", "score_hidden": false, "stickied": false, "created": 1492030034.0, "created_utc": 1492001234.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 8863}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg66fdf", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "mfukar", "parent_id": "t1_dg61hkc", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "`ar` archives are in use in more than libraries, most prominently packages, e.g. for [ipkg](https://en.wikipedia.org/wiki/Ipkg) or opkg.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;code&gt;ar&lt;/code&gt; archives are in use in more than libraries, most prominently packages, e.g. for &lt;a href=\"https://en.wikipedia.org/wiki/Ipkg\"&gt;ipkg&lt;/a&gt; or opkg.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg66fdf", "score_hidden": false, "stickied": false, "created": 1492045735.0, "created_utc": 1492016935.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg61hkc", "gilded": 0, "archived": false, "score": 17, "report_reasons": null, "author": "bg-j38", "parent_id": "t1_dg5zwch", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "These days you'll mostly only see .a files as static libraries for programs. The program to access them is called ar and dates back to the very earliest days of Unix. It's mentioned in the Unix Programmer's Manual which Thompson and Ritchie published in 1971.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;These days you&amp;#39;ll mostly only see .a files as static libraries for programs. The program to access them is called ar and dates back to the very earliest days of Unix. It&amp;#39;s mentioned in the Unix Programmer&amp;#39;s Manual which Thompson and Ritchie published in 1971.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61hkc", "score_hidden": false, "stickied": false, "created": 1492040527.0, "created_utc": 1492011727.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 17}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6l5sp", "gilded": 0, "archived": false, "score": 11, "report_reasons": null, "author": "TheRaven1", "parent_id": "t1_dg5zwch", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "*her\nBut when making the post I didn't know there was a difference! Now I do! And I understand the basics of both now :)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;*her\nBut when making the post I didn&amp;#39;t know there was a difference! Now I do! And I understand the basics of both now :)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6l5sp", "score_hidden": false, "stickied": false, "created": 1492061020.0, "created_utc": 1492032220.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 11}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5zwch", "gilded": 0, "archived": false, "score": 27, "report_reasons": null, "author": "DatsButterBoo", "parent_id": "t1_dg5z6ts", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt;There are already some good explanations of compression. But that isn't the whole answer to, \"what is a zip file\".\n\nTrue but based on the rest of his question it seems to be more of  \"What is a compressed file\" than \"what is a zip file\".\n\nBut your response has interesting information as well. \n\nI know tarballs but I was unfamiliar with `.a` files.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;There are already some good explanations of compression. But that isn&amp;#39;t the whole answer to, &amp;quot;what is a zip file&amp;quot;.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;True but based on the rest of his question it seems to be more of  &amp;quot;What is a compressed file&amp;quot; than &amp;quot;what is a zip file&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;But your response has interesting information as well. &lt;/p&gt;\n\n&lt;p&gt;I know tarballs but I was unfamiliar with &lt;code&gt;.a&lt;/code&gt; files.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5zwch", "score_hidden": false, "stickied": false, "created": 1492038779.0, "created_utc": 1492009979.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 27}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6ashn", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Epistaxis", "parent_id": "t1_dg5z6ts", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt; Compression is a common feature for archive file formats, but is not required. Nor is an archive format required for compression. Classic Unix 'tar' and '.a' files are examples of archive formats that didn't include compression. And there are a variety of tools in Unix/Linux environments for compressing a single file without any kind of archive file format.\n\nIndeed, archiving and compressing are treated as separate tasks in Unix/Linux. If you only want to compress a single file, you might only compress it and not archive it. If you want to compress a few files, you might just compress each one individually rather than put them all together into an archive and then compress that. It's easier to stream through them in pipes that way. Archiving only really becomes necessary when you want to preserve a whole directory structure. However, if you have a large group of files that are already compressed, e.g. JPEG images, and you just want to make them into a single file for ease of storage/transport, you might actually put them together into an archive that isn't itself compressed in any way. (I have an instrument that generates tens of thousands of JPEGs every time it runs, and the mere act of creating all those inodes makes it a lot slower to copy them piecemeal than to copy one big file that contains them all.)\n\nSo since compression is separate from archiving, most Unix/Linux compressors have an interesting property: if you concatenate two compressed files and then decompress the concatenated file, you get the same thing as if you concatenated the original uncompressed files. That is, if `smallfile1` contains the text\n\n    hello\n\nand `smallfile2` contains the text\n\n    world\n\nyou can compress them individually to `smallfile1.gz` and `smallfile2.gz`, then concatenate those into a single `bigfile.gz`; when you decompress `bigfile.gz` you will get\n\n    hello\n    world", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Compression is a common feature for archive file formats, but is not required. Nor is an archive format required for compression. Classic Unix &amp;#39;tar&amp;#39; and &amp;#39;.a&amp;#39; files are examples of archive formats that didn&amp;#39;t include compression. And there are a variety of tools in Unix/Linux environments for compressing a single file without any kind of archive file format.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Indeed, archiving and compressing are treated as separate tasks in Unix/Linux. If you only want to compress a single file, you might only compress it and not archive it. If you want to compress a few files, you might just compress each one individually rather than put them all together into an archive and then compress that. It&amp;#39;s easier to stream through them in pipes that way. Archiving only really becomes necessary when you want to preserve a whole directory structure. However, if you have a large group of files that are already compressed, e.g. JPEG images, and you just want to make them into a single file for ease of storage/transport, you might actually put them together into an archive that isn&amp;#39;t itself compressed in any way. (I have an instrument that generates tens of thousands of JPEGs every time it runs, and the mere act of creating all those inodes makes it a lot slower to copy them piecemeal than to copy one big file that contains them all.)&lt;/p&gt;\n\n&lt;p&gt;So since compression is separate from archiving, most Unix/Linux compressors have an interesting property: if you concatenate two compressed files and then decompress the concatenated file, you get the same thing as if you concatenated the original uncompressed files. That is, if &lt;code&gt;smallfile1&lt;/code&gt; contains the text&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;hello\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and &lt;code&gt;smallfile2&lt;/code&gt; contains the text&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;world\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;you can compress them individually to &lt;code&gt;smallfile1.gz&lt;/code&gt; and &lt;code&gt;smallfile2.gz&lt;/code&gt;, then concatenate those into a single &lt;code&gt;bigfile.gz&lt;/code&gt;; when you decompress &lt;code&gt;bigfile.gz&lt;/code&gt; you will get&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;hello\nworld\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6ashn", "score_hidden": false, "stickied": false, "created": 1492050163.0, "created_utc": 1492021363.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg5z6ts", "gilded": 0, "archived": false, "score": 161, "report_reasons": null, "author": "ramennoodle", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "There are already some good explanations of compression.  But that isn't the whole answer to, \"what is a zip file\".  \n\nA zip file is  an *archive file* that also supports compression.  An archive file is a way of storing a collection of files (and directories, file names, and other data necessary to reproduce a subset of a file system) in a single file.  It is a convenient  way of packaging a bunch of files for archiving, sharing, etc.   \n\nCompression is a common feature for archive file formats, but is not required.  Nor is an archive format required for compression.  Classic Unix 'tar' and '.a' files are examples of archive formats that didn't include compression.  And there are a variety of tools in Unix/Linux environments for compressing a single file without any kind of archive file format.\n\n.zip files were the file format for a \"shareware\" DOS program from *1989* called pkzip, from a company named pkware.  Because it was functional and effectively free (and maybe because of the catchy name) it quickly grew to be a defacto standard.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There are already some good explanations of compression.  But that isn&amp;#39;t the whole answer to, &amp;quot;what is a zip file&amp;quot;.  &lt;/p&gt;\n\n&lt;p&gt;A zip file is  an &lt;em&gt;archive file&lt;/em&gt; that also supports compression.  An archive file is a way of storing a collection of files (and directories, file names, and other data necessary to reproduce a subset of a file system) in a single file.  It is a convenient  way of packaging a bunch of files for archiving, sharing, etc.   &lt;/p&gt;\n\n&lt;p&gt;Compression is a common feature for archive file formats, but is not required.  Nor is an archive format required for compression.  Classic Unix &amp;#39;tar&amp;#39; and &amp;#39;.a&amp;#39; files are examples of archive formats that didn&amp;#39;t include compression.  And there are a variety of tools in Unix/Linux environments for compressing a single file without any kind of archive file format.&lt;/p&gt;\n\n&lt;p&gt;.zip files were the file format for a &amp;quot;shareware&amp;quot; DOS program from &lt;em&gt;1989&lt;/em&gt; called pkzip, from a company named pkware.  Because it was functional and effectively free (and maybe because of the catchy name) it quickly grew to be a defacto standard.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5z6ts", "score_hidden": false, "stickied": false, "created": 1492037982.0, "created_utc": 1492009182.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 161}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg68e78", "gilded": 0, "archived": false, "score": 19, "report_reasons": null, "author": "SnowdensOfYesteryear", "parent_id": "t1_dg61kpz", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Yes technically right but not really relevant to the basic understanding of compression.  Also symbols are often reassigned, e.g.\n\nA -&gt; Pizza\n\n%7(A)Linguini %3(A)Sub would be the next step of \"compression\".\n\nReally though, most of the effort is usually in identifying patterns rather than the \"compression\" aspect of things.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes technically right but not really relevant to the basic understanding of compression.  Also symbols are often reassigned, e.g.&lt;/p&gt;\n\n&lt;p&gt;A -&amp;gt; Pizza&lt;/p&gt;\n\n&lt;p&gt;%7(A)Linguini %3(A)Sub would be the next step of &amp;quot;compression&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Really though, most of the effort is usually in identifying patterns rather than the &amp;quot;compression&amp;quot; aspect of things.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg68e78", "score_hidden": false, "stickied": false, "created": 1492047711.0, "created_utc": 1492018911.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 19}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg61kpz", "gilded": 0, "archived": false, "score": 13, "report_reasons": null, "author": "Ikinoki", "parent_id": "t1_dg5y2rq", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Shouldn't zip write a dictionary (array mapping) and then use that dictionary for compression?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Shouldn&amp;#39;t zip write a dictionary (array mapping) and then use that dictionary for compression?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61kpz", "score_hidden": false, "stickied": false, "created": 1492040623.0, "created_utc": 1492011823.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 13}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5y2rq", "gilded": 0, "archived": false, "score": 142, "report_reasons": null, "author": "_Mr-Skeltal_", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Say you have the following sequence of data:\n\n`Pizza Pizza Pizza Pizza Pizza Pizza Pizza Linguini Pizza Pizza Pizza Sub`\n\nLet's shorten it by making a code.  We agree that in addition to the regular text we can use % to \"signal\" that the next value is a number that tells us how many times to repeat the following text in parentheses.  So now we have:\n\n`%7(Pizza )Linguini %3(Pizza )Sub`\n\nMuch shorter!  We know to repeat Pizza 7 times, then Linguini once, and then another signal to repeat Pizza 3 times.\n\nWe can also do:\n\n`%7(Pizza )%1Linguini %3(Pizza )Sub`\n\nThat's pretty much the concept compression in a nutshell (and I've written code for decoding bzip data).  it is basically agreeing on code forms to shorten stuff, and implementing those tricks and techniques as part of a compression algorithm.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Say you have the following sequence of data:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Pizza Pizza Pizza Pizza Pizza Pizza Pizza Linguini Pizza Pizza Pizza Sub&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s shorten it by making a code.  We agree that in addition to the regular text we can use % to &amp;quot;signal&amp;quot; that the next value is a number that tells us how many times to repeat the following text in parentheses.  So now we have:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;%7(Pizza )Linguini %3(Pizza )Sub&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Much shorter!  We know to repeat Pizza 7 times, then Linguini once, and then another signal to repeat Pizza 3 times.&lt;/p&gt;\n\n&lt;p&gt;We can also do:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;%7(Pizza )%1Linguini %3(Pizza )Sub&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s pretty much the concept compression in a nutshell (and I&amp;#39;ve written code for decoding bzip data).  it is basically agreeing on code forms to shorten stuff, and implementing those tricks and techniques as part of a compression algorithm.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5y2rq", "score_hidden": false, "stickied": false, "created": 1492036714.0, "created_utc": 1492007914.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 142}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6pps1", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dg6ekz7", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "[removed]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6pps1", "score_hidden": false, "stickied": false, "created": 1492066583.0, "created_utc": 1492037783.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6ekz7", "gilded": 0, "archived": false, "score": 22, "report_reasons": null, "author": "[deleted]", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "[removed]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6ekz7", "score_hidden": false, "stickied": false, "created": 1492054066.0, "created_utc": 1492025266.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 22}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg76pzd", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "vijeno", "parent_id": "t1_dg61udm", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "    *ing;!hat;% is ;&amp;compress;\u00a7 file;$ it :W!%a \"zip\u00a7\" or \"&amp;ed\u00a7?\" How does formatt*$t! way &amp;$and w!%&amp;*?\n\nHey, you didn't count the index in your demo! You villain you!\n\nDamnit. Now I want to mess around with a simple compression algorightm.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;pre&gt;&lt;code&gt;*ing;!hat;% is ;&amp;amp;compress;\u00a7 file;$ it :W!%a &amp;quot;zip\u00a7&amp;quot; or &amp;quot;&amp;amp;ed\u00a7?&amp;quot; How does formatt*$t! way &amp;amp;$and w!%&amp;amp;*?\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Hey, you didn&amp;#39;t count the index in your demo! You villain you!&lt;/p&gt;\n\n&lt;p&gt;Damnit. Now I want to mess around with a simple compression algorightm.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg76pzd", "score_hidden": false, "stickied": false, "created": 1492089168.0, "created_utc": 1492060368.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dg61udm", "gilded": 0, "archived": false, "score": 30, "report_reasons": null, "author": "frowawayduh", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Let's compress OP's question by replacing certain 3 and 4 character sequences with a single character:  \n* = \"ing\"  \n! = \"hat\"  \n% = \" is \"  \nCompressed:\nW!%a \"zip file\" or \"compressed file?\" How does formatt* it t! way compress it and w!%compress*? \n\nOriginal:\nWhat is a \"zip file\" or \"compressed file?\" How does formatting it that way compress it and what is compressing?\n\nIn this compression, 23 characters were replaced by 7, a saving of 16 characters.  Because we chose a single character that never appears in the original text, our compression can be reversed without error.  There is some overhead, however, because we also need to send the translation key along with the message.  In very long messages, there are often large and frequent repetitions that can be squeezed down enough to be worth the overhead.  If the compression / decompression rules are built in to the software, there is no need to transmit the compression key.  ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s compress OP&amp;#39;s question by replacing certain 3 and 4 character sequences with a single character:&lt;br/&gt;\n* = &amp;quot;ing&amp;quot;&lt;br/&gt;\n! = &amp;quot;hat&amp;quot;&lt;br/&gt;\n% = &amp;quot; is &amp;quot;&lt;br/&gt;\nCompressed:\nW!%a &amp;quot;zip file&amp;quot; or &amp;quot;compressed file?&amp;quot; How does formatt* it t! way compress it and w!%compress*? &lt;/p&gt;\n\n&lt;p&gt;Original:\nWhat is a &amp;quot;zip file&amp;quot; or &amp;quot;compressed file?&amp;quot; How does formatting it that way compress it and what is compressing?&lt;/p&gt;\n\n&lt;p&gt;In this compression, 23 characters were replaced by 7, a saving of 16 characters.  Because we chose a single character that never appears in the original text, our compression can be reversed without error.  There is some overhead, however, because we also need to send the translation key along with the message.  In very long messages, there are often large and frequent repetitions that can be squeezed down enough to be worth the overhead.  If the compression / decompression rules are built in to the software, there is no need to transmit the compression key.  &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61udm", "score_hidden": false, "stickied": false, "created": 1492040917.0, "created_utc": 1492012117.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 30}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6ml7b", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "scarabic", "parent_id": "t1_dg68o5g", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Yes! Thank you for adding this. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes! Thank you for adding this. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6ml7b", "score_hidden": false, "stickied": false, "created": 1492062719.0, "created_utc": 1492033919.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg68o5g", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "kRkthOr", "parent_id": "t1_dg61hcn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "OP: Note that the second example is lossy compression. It's compression that takes away *some* data that it deems unnecessary. For another example: \n\nSay you have these graph values: 1,4,4,3,4,2,9,8,9,8,4,5\n\nThis wouldn't compress well with lossless compression: `1(1), 2(4), 1(3), 1(4), 1(2), 1(9), 1(8), 1(9), 1(8), 1(4), 1(5)`. This is longer than the original content. No good.\n\nBut when using lossy compression, you can decide to average out any data that doesn't vary by more than 1, **for example**.\n\nYou end up with: 1,4,4,(4),4,2,9,(9),9,(9),4,(4) - values in brackets have been averaged out. This string of numbers would print out a graph that's basically ALMOST the same as the original, and while not exactly the same it now allows us to much more neatly compress to: `1(1), 4(4), 1(2), 4(9), 2(4)`.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;OP: Note that the second example is lossy compression. It&amp;#39;s compression that takes away &lt;em&gt;some&lt;/em&gt; data that it deems unnecessary. For another example: &lt;/p&gt;\n\n&lt;p&gt;Say you have these graph values: 1,4,4,3,4,2,9,8,9,8,4,5&lt;/p&gt;\n\n&lt;p&gt;This wouldn&amp;#39;t compress well with lossless compression: &lt;code&gt;1(1), 2(4), 1(3), 1(4), 1(2), 1(9), 1(8), 1(9), 1(8), 1(4), 1(5)&lt;/code&gt;. This is longer than the original content. No good.&lt;/p&gt;\n\n&lt;p&gt;But when using lossy compression, you can decide to average out any data that doesn&amp;#39;t vary by more than 1, &lt;strong&gt;for example&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;You end up with: 1,4,4,(4),4,2,9,(9),9,(9),4,(4) - values in brackets have been averaged out. This string of numbers would print out a graph that&amp;#39;s basically ALMOST the same as the original, and while not exactly the same it now allows us to much more neatly compress to: &lt;code&gt;1(1), 4(4), 1(2), 4(9), 2(4)&lt;/code&gt;.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg68o5g", "score_hidden": false, "stickied": false, "created": 1492047989.0, "created_utc": 1492019189.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 6}}], "after": null, "before": null}}, "user_reports": [], "id": "dg61hcn", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "scarabic", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Here's a simple example. \n\nThe number 7777722333333 can be expressed in fewer characters, like this:\n\n7(5)223(6)\n\nWith the right \"decompressor\" program you could turn those parentheticals back into long strings of repetitive digits. \n\nAnother example: when you take an audio file and \"compress\" it into an MP3, one of the first things that happens is that all frequencies beyond the human range of hearing are discarded. Don't need 'em. Makes your file smaller.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s a simple example. &lt;/p&gt;\n\n&lt;p&gt;The number 7777722333333 can be expressed in fewer characters, like this:&lt;/p&gt;\n\n&lt;p&gt;7(5)223(6)&lt;/p&gt;\n\n&lt;p&gt;With the right &amp;quot;decompressor&amp;quot; program you could turn those parentheticals back into long strings of repetitive digits. &lt;/p&gt;\n\n&lt;p&gt;Another example: when you take an audio file and &amp;quot;compress&amp;quot; it into an MP3, one of the first things that happens is that all frequencies beyond the human range of hearing are discarded. Don&amp;#39;t need &amp;#39;em. Makes your file smaller.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg61hcn", "score_hidden": false, "stickied": false, "created": 1492040521.0, "created_utc": 1492011721.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg709qc", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "zynix", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "A toy example of compressing text using the first paragraph from the Data compression page in wikipedia as test material - https://en.wikipedia.org/wiki/Data_compression\n\n\n&gt;In signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. The process of reducing the size of a data file is referred to as data compression. In the context of data transmission, it is called source coding (encoding done at the source of the data before it is stored or transmitted) in opposition to channel coding.\n\n683 characters.\n\nThe top most common words that are atleast 3 characters and repeated at least twice:\n\n    [('data', 5), \n    ('the', 5), \n    ('compression', 4), \n    ('information', 3), \n    ('bits', 3), \n    ('source', 3), \n    ('lossless', 2), \n    ('reduces', 2), \n    ('coding', 2)]\n\nIf we go through and replace them we get\n\n&gt;'In signal processing, /1 /3, /6 /9, or bit-rate reduction involves en/9 /4 using fewer /5 than /2 original representation. Compression can be ei/2r lossy or /7. Lossless /3 /8 /5 by identifying and eliminating statistical redundancy. No /4 is lost in /7 /3. Lossy /3 /8 /5 by removing unnecessary or less important /4. The process of reducing /2 size of a /1 file is referred to as /1 /3. In /2 context of /1 transmission, it is called /6 /9 (en/9 done at /2 /6 of /2 /1 before it is stored or transmitted) in opposition to channel /9.'\n\nwhich is 535 characters PLUS the dictionary used to remember what those numbers mean.  That puts us at 621 characters or approximately 10% compression.\n\nAdvanced compression algorithms spend more time looking for repetition in the data you wish to compress so in the above example, the repeated character string \"ossy\" could be cut down to a number.   Same goes for the \"ssion\" in Compression and transmission.  Another opportunity would be the \"tion\" in representation and ~~transmission~~ opposition.\n\nAfter replacing these easy ones,  a compression algorithm could then look at the binary (0's and 1's) representation for the data to be compressed and see if there were any repeats in there as well.  With a lossless compression algorithm I believe the theoretical maximum savings is ~50% or a 1:2 ratio.  \n\n\nMeanwhile with ~~lossless~~ lossy compression, an extreme and crude example would be somewhat like the code paraphrasing the example paragraph above as \"Compression replaces commonly repeating pieces of data\".  Absolutely amazing 80% compression there but there may have been a few finer points lost in the compression process. \n\nOne last type of compression involves both the compressor and decompressor having a common dictionary shared between the two of them so that the compressor can replace commonly repeated pieces of data without having to include a dictionary along with it.\n\nedit: quick typo ~~fixes~~\n\n\n\n\n", "edited": 1492055506.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A toy example of compressing text using the first paragraph from the Data compression page in wikipedia as test material - &lt;a href=\"https://en.wikipedia.org/wiki/Data_compression\"&gt;https://en.wikipedia.org/wiki/Data_compression&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;In signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. The process of reducing the size of a data file is referred to as data compression. In the context of data transmission, it is called source coding (encoding done at the source of the data before it is stored or transmitted) in opposition to channel coding.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;683 characters.&lt;/p&gt;\n\n&lt;p&gt;The top most common words that are atleast 3 characters and repeated at least twice:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[(&amp;#39;data&amp;#39;, 5), \n(&amp;#39;the&amp;#39;, 5), \n(&amp;#39;compression&amp;#39;, 4), \n(&amp;#39;information&amp;#39;, 3), \n(&amp;#39;bits&amp;#39;, 3), \n(&amp;#39;source&amp;#39;, 3), \n(&amp;#39;lossless&amp;#39;, 2), \n(&amp;#39;reduces&amp;#39;, 2), \n(&amp;#39;coding&amp;#39;, 2)]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;If we go through and replace them we get&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&amp;#39;In signal processing, /1 /3, /6 /9, or bit-rate reduction involves en/9 /4 using fewer /5 than /2 original representation. Compression can be ei/2r lossy or /7. Lossless /3 /8 /5 by identifying and eliminating statistical redundancy. No /4 is lost in /7 /3. Lossy /3 /8 /5 by removing unnecessary or less important /4. The process of reducing /2 size of a /1 file is referred to as /1 /3. In /2 context of /1 transmission, it is called /6 /9 (en/9 done at /2 /6 of /2 /1 before it is stored or transmitted) in opposition to channel /9.&amp;#39;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;which is 535 characters PLUS the dictionary used to remember what those numbers mean.  That puts us at 621 characters or approximately 10% compression.&lt;/p&gt;\n\n&lt;p&gt;Advanced compression algorithms spend more time looking for repetition in the data you wish to compress so in the above example, the repeated character string &amp;quot;ossy&amp;quot; could be cut down to a number.   Same goes for the &amp;quot;ssion&amp;quot; in Compression and transmission.  Another opportunity would be the &amp;quot;tion&amp;quot; in representation and &lt;del&gt;transmission&lt;/del&gt; opposition.&lt;/p&gt;\n\n&lt;p&gt;After replacing these easy ones,  a compression algorithm could then look at the binary (0&amp;#39;s and 1&amp;#39;s) representation for the data to be compressed and see if there were any repeats in there as well.  With a lossless compression algorithm I believe the theoretical maximum savings is ~50% or a 1:2 ratio.  &lt;/p&gt;\n\n&lt;p&gt;Meanwhile with &lt;del&gt;lossless&lt;/del&gt; lossy compression, an extreme and crude example would be somewhat like the code paraphrasing the example paragraph above as &amp;quot;Compression replaces commonly repeating pieces of data&amp;quot;.  Absolutely amazing 80% compression there but there may have been a few finer points lost in the compression process. &lt;/p&gt;\n\n&lt;p&gt;One last type of compression involves both the compressor and decompressor having a common dictionary shared between the two of them so that the compressor can replace commonly repeated pieces of data without having to include a dictionary along with it.&lt;/p&gt;\n\n&lt;p&gt;edit: quick typo &lt;del&gt;fixes&lt;/del&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg709qc", "score_hidden": false, "stickied": false, "created": 1492079578.0, "created_utc": 1492050778.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6ly4u", "gilded": 0, "archived": false, "score": 14, "report_reasons": null, "author": "uber1337h4xx0r", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "So i imagine you already know that files are made up of just ones and zeroes and that the bigger the file, the more ones and zeroes are used. \n\nImagine a similar concept, but in real life.  You have to memorize 3 phone numbers -\n\n123-555-0632\n\n123-555-0555\n\n555-655-5123\n\nImagine your brain can't remember all that.  So what you can do is compress it. \n\nReplace any reference to 123 with o (for Onetwothree), f for 555 (Fivefivefive). \n\nNow you have\n\nof0632\n\nof0f\n\nf6fo\n\nThat should be a lot easier to remember because it takes up less space, but it's only highly compressed because of favorable data (lots of repeats).  Of course, this only works because you have a code to work with (you need to know f means 555), and you cannot do anything if you don't uncompress first (a phone can use 555,  but not f).  That's why you have to unzip. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So i imagine you already know that files are made up of just ones and zeroes and that the bigger the file, the more ones and zeroes are used. &lt;/p&gt;\n\n&lt;p&gt;Imagine a similar concept, but in real life.  You have to memorize 3 phone numbers -&lt;/p&gt;\n\n&lt;p&gt;123-555-0632&lt;/p&gt;\n\n&lt;p&gt;123-555-0555&lt;/p&gt;\n\n&lt;p&gt;555-655-5123&lt;/p&gt;\n\n&lt;p&gt;Imagine your brain can&amp;#39;t remember all that.  So what you can do is compress it. &lt;/p&gt;\n\n&lt;p&gt;Replace any reference to 123 with o (for Onetwothree), f for 555 (Fivefivefive). &lt;/p&gt;\n\n&lt;p&gt;Now you have&lt;/p&gt;\n\n&lt;p&gt;of0632&lt;/p&gt;\n\n&lt;p&gt;of0f&lt;/p&gt;\n\n&lt;p&gt;f6fo&lt;/p&gt;\n\n&lt;p&gt;That should be a lot easier to remember because it takes up less space, but it&amp;#39;s only highly compressed because of favorable data (lots of repeats).  Of course, this only works because you have a code to work with (you need to know f means 555), and you cannot do anything if you don&amp;#39;t uncompress first (a phone can use 555,  but not f).  That&amp;#39;s why you have to unzip. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6ly4u", "score_hidden": false, "stickied": false, "created": 1492061950.0, "created_utc": 1492033150.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 14}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg6cbno", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "MyNameIsZaxer2", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "To add to the previous answers, there is a website where you can demonstrate an example of text compression. You act as a program trying to compress the text as much as possible using a sort of \"symbol replacement\" method.  \n\nhttps://studio.code.org/s/text-compression/stage/1/puzzle/2", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;To add to the previous answers, there is a website where you can demonstrate an example of text compression. You act as a program trying to compress the text as much as possible using a sort of &amp;quot;symbol replacement&amp;quot; method.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://studio.code.org/s/text-compression/stage/1/puzzle/2\"&gt;https://studio.code.org/s/text-compression/stage/1/puzzle/2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6cbno", "score_hidden": false, "stickied": false, "created": 1492051718.0, "created_utc": 1492022918.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg607re", "gilded": 0, "archived": false, "score": 12, "report_reasons": null, "author": "wildjokers", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "As other commenters have pointed out compression takes advantage of redundancy. Here are some common compression algorithms:\n\n* https://en.wikipedia.org/wiki/Huffman_coding -- circa 1952, an old compression algorithm, modified forms of it was used in fax machines, has a drawback that the compression tree has to be included in the output so it can be decompressed on other end, this adds overhead to the resulting compressed file\n* https://en.wikipedia.org/wiki/Run-length_encoding -- circa 1967, pretty simplistic, not used on its own but sometimes used as a step of other algorithms\n* https://en.wikipedia.org/wiki/LZ77_and_LZ78 -- circa 1977/78, very important development in compression technology, used a dictionary table (or sliding window), dictionary _does not_ need to be included in output as the dictionary can be reconstructed on-the-fly on the other end.  \n* https://en.wikipedia.org/wiki/Lempel\u2013Ziv\u2013Welch -- circa 1984, An improvement on LZ77/78. Usually referred to as LZW.\n* https://en.wikipedia.org/wiki/Burrows\u2013Wheeler_transform -- circa 1994, used in the popular bzip2 unix utility, compresses better than dictionary algorithms at the cost of memory and CPU usage.\n\nThe dictionary algorithms are the basis for most of the popular tools today like WinZip and gzip. People made slight alterations to dictionary algorithms to get around patents associated with LZ77/78 and LZW.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As other commenters have pointed out compression takes advantage of redundancy. Here are some common compression algorithms:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://en.wikipedia.org/wiki/Huffman_coding\"&gt;https://en.wikipedia.org/wiki/Huffman_coding&lt;/a&gt; -- circa 1952, an old compression algorithm, modified forms of it was used in fax machines, has a drawback that the compression tree has to be included in the output so it can be decompressed on other end, this adds overhead to the resulting compressed file&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://en.wikipedia.org/wiki/Run-length_encoding\"&gt;https://en.wikipedia.org/wiki/Run-length_encoding&lt;/a&gt; -- circa 1967, pretty simplistic, not used on its own but sometimes used as a step of other algorithms&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://en.wikipedia.org/wiki/LZ77_and_LZ78\"&gt;https://en.wikipedia.org/wiki/LZ77_and_LZ78&lt;/a&gt; -- circa 1977/78, very important development in compression technology, used a dictionary table (or sliding window), dictionary &lt;em&gt;does not&lt;/em&gt; need to be included in output as the dictionary can be reconstructed on-the-fly on the other end.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch\"&gt;https://en.wikipedia.org/wiki/Lempel\u2013Ziv\u2013Welch&lt;/a&gt; -- circa 1984, An improvement on LZ77/78. Usually referred to as LZW.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform\"&gt;https://en.wikipedia.org/wiki/Burrows\u2013Wheeler_transform&lt;/a&gt; -- circa 1994, used in the popular bzip2 unix utility, compresses better than dictionary algorithms at the cost of memory and CPU usage.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The dictionary algorithms are the basis for most of the popular tools today like WinZip and gzip. People made slight alterations to dictionary algorithms to get around patents associated with LZ77/78 and LZW.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg607re", "score_hidden": false, "stickied": false, "created": 1492039130.0, "created_utc": 1492010330.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 12}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg5zvd1", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "drenp", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "A higher-level view of compression is the following: you're exploiting the fact that most data has *some sort of predictable structure* to it, and you're transforming the data to make use of this structure. This explains why it's impossible to find a compression algorithm that compresses all data (since arbitrary data is not predictable), or why zipping a zip file doesn't yield any improvements: the data format already fully exploits the structure that the zip file assumes.\n\nExample: on a computer, most data is stored in bytes, taking values 0 through 255. However, in regular text, most data is one of 26 characters, plus some uppercase and punctuation. In fact, some letters (e.g. 'e') also occur more often than others (e.g. 'q'). You can extend this to combinations of two letters, three letters, etc. In other words, text is highly repetitive.\n\nFile compression uses these structural predictabilities to output data in a shorter form, in a manner which /u/Rannasha described in more detail.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A higher-level view of compression is the following: you&amp;#39;re exploiting the fact that most data has &lt;em&gt;some sort of predictable structure&lt;/em&gt; to it, and you&amp;#39;re transforming the data to make use of this structure. This explains why it&amp;#39;s impossible to find a compression algorithm that compresses all data (since arbitrary data is not predictable), or why zipping a zip file doesn&amp;#39;t yield any improvements: the data format already fully exploits the structure that the zip file assumes.&lt;/p&gt;\n\n&lt;p&gt;Example: on a computer, most data is stored in bytes, taking values 0 through 255. However, in regular text, most data is one of 26 characters, plus some uppercase and punctuation. In fact, some letters (e.g. &amp;#39;e&amp;#39;) also occur more often than others (e.g. &amp;#39;q&amp;#39;). You can extend this to combinations of two letters, three letters, etc. In other words, text is highly repetitive.&lt;/p&gt;\n\n&lt;p&gt;File compression uses these structural predictabilities to output data in a shorter form, in a manner which &lt;a href=\"/u/Rannasha\"&gt;/u/Rannasha&lt;/a&gt; described in more detail.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg5zvd1", "score_hidden": false, "stickied": false, "created": 1492038748.0, "created_utc": 1492009948.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 7}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg652m8", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Sophira", "parent_id": "t1_dg62yqn", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "&gt; It stores everything as it comes in, but when it sees a sequence of 3 or more bytes it's already seen before, it inserts a special thing saying \"copy X bytes from Y bytes back\" instead of the actual data.\n\nIt's worth noting that due to this property, it's possible to [create a ZIP file which, when extracted, produces an exact copy of itself](https://research.swtch.com/zip)!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;It stores everything as it comes in, but when it sees a sequence of 3 or more bytes it&amp;#39;s already seen before, it inserts a special thing saying &amp;quot;copy X bytes from Y bytes back&amp;quot; instead of the actual data.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It&amp;#39;s worth noting that due to this property, it&amp;#39;s possible to &lt;a href=\"https://research.swtch.com/zip\"&gt;create a ZIP file which, when extracted, produces an exact copy of itself&lt;/a&gt;!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg652m8", "score_hidden": false, "stickied": false, "created": 1492044341.0, "created_utc": 1492015541.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg62yqn", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "Dascandy", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "The zip file format is actually a storage file format. It does not compress anything by itself. Look at it like a moving box - you put things into it and label it, and then given the list of things in the box (the so-called index) you can find them again. By itself, this makes a zip file slightly larger than the files in there. It also saves a bit of space, because your filesystem stores files in larger chunks (with a bit of unused space after it), while a zip file doesn't leave any space - much like how a moving box is packed with books, while a bookshelf usually has all of them standing side by side with the space above it wasted.\n\nThe files in there are actually compressed though - which is possible, because the Zip index has an entry to say \"this one is compressed with X, and this one with Y, and this one is uncompressed\". Your normal computer folders don't do this. The algorithm explained by /u/rannasha is first RLE, then a basic LZ77, and then a basic Huffman. Zip normally uses Deflate, which is based on LZ77 but improves on the basic algorithm. It stores everything as it comes in, but when it sees a sequence of 3 or more bytes it's already seen before, it inserts a special thing saying \"copy X bytes from Y bytes back\" instead of the actual data.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The zip file format is actually a storage file format. It does not compress anything by itself. Look at it like a moving box - you put things into it and label it, and then given the list of things in the box (the so-called index) you can find them again. By itself, this makes a zip file slightly larger than the files in there. It also saves a bit of space, because your filesystem stores files in larger chunks (with a bit of unused space after it), while a zip file doesn&amp;#39;t leave any space - much like how a moving box is packed with books, while a bookshelf usually has all of them standing side by side with the space above it wasted.&lt;/p&gt;\n\n&lt;p&gt;The files in there are actually compressed though - which is possible, because the Zip index has an entry to say &amp;quot;this one is compressed with X, and this one with Y, and this one is uncompressed&amp;quot;. Your normal computer folders don&amp;#39;t do this. The algorithm explained by &lt;a href=\"/u/rannasha\"&gt;/u/rannasha&lt;/a&gt; is first RLE, then a basic LZ77, and then a basic Huffman. Zip normally uses Deflate, which is based on LZ77 but improves on the basic algorithm. It stores everything as it comes in, but when it sees a sequence of 3 or more bytes it&amp;#39;s already seen before, it inserts a special thing saying &amp;quot;copy X bytes from Y bytes back&amp;quot; instead of the actual data.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg62yqn", "score_hidden": false, "stickied": false, "created": 1492042132.0, "created_utc": 1492013332.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 6}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg60gi6", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sebwiers", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "Besides the multiply mentioned reduction of repetition, you can do a lot with encoding &amp; dictionaries.  Most files contain a limited set of characters (encoded as clumps of bits usually 16 of them), and could more efficiently be encoded if not limited to using just those characters.  \n\nImagine for example that you had a very long text that was all in lowercase letters.   If you replaced some common letter combinations with uppercase letters, and included a \"dictionary\" of these replacements, the result would be a shorter text.  If those common letter combinations can be of arbitrary length, say replacing entire common words, they result can even be MUCH shorter.  \n\nComputer programs, for example, tend to contain the same words over and over.  They don't appear multiple times in a row, so the repetition reduction described in other posts won't apply, but encoding them as something shorter instead of using the full word will make the file shorter.\n\nThe encoding need not be via special characters.  It can just use an symbol that designates encoding - for example, if encodes sequances started with \"%\" then \"%p\" could represent the word \"print\".  You would need an \"escape\" character as well, to represent when you actually wanted to display \"%\" (as well as the escape character itself).", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Besides the multiply mentioned reduction of repetition, you can do a lot with encoding &amp;amp; dictionaries.  Most files contain a limited set of characters (encoded as clumps of bits usually 16 of them), and could more efficiently be encoded if not limited to using just those characters.  &lt;/p&gt;\n\n&lt;p&gt;Imagine for example that you had a very long text that was all in lowercase letters.   If you replaced some common letter combinations with uppercase letters, and included a &amp;quot;dictionary&amp;quot; of these replacements, the result would be a shorter text.  If those common letter combinations can be of arbitrary length, say replacing entire common words, they result can even be MUCH shorter.  &lt;/p&gt;\n\n&lt;p&gt;Computer programs, for example, tend to contain the same words over and over.  They don&amp;#39;t appear multiple times in a row, so the repetition reduction described in other posts won&amp;#39;t apply, but encoding them as something shorter instead of using the full word will make the file shorter.&lt;/p&gt;\n\n&lt;p&gt;The encoding need not be via special characters.  It can just use an symbol that designates encoding - for example, if encodes sequances started with &amp;quot;%&amp;quot; then &amp;quot;%p&amp;quot; could represent the word &amp;quot;print&amp;quot;.  You would need an &amp;quot;escape&amp;quot; character as well, to represent when you actually wanted to display &amp;quot;%&amp;quot; (as well as the escape character itself).&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg60gi6", "score_hidden": false, "stickied": false, "created": 1492039397.0, "created_utc": 1492010597.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6734h", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dg66sza", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "[removed]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6734h", "score_hidden": false, "stickied": false, "created": 1492046406.0, "created_utc": 1492017606.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg66sza", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dg66qjy", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "[removed]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg66sza", "score_hidden": false, "stickied": false, "created": 1492046122.0, "created_utc": 1492017322.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg66qjy", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dg66i26", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "[removed]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg66qjy", "score_hidden": false, "stickied": false, "created": 1492046052.0, "created_utc": 1492017252.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg66i26", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dg662cb", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "[removed]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg66i26", "score_hidden": false, "stickied": false, "created": 1492045813.0, "created_utc": 1492017013.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg662cb", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "[deleted]", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "[removed]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg662cb", "score_hidden": false, "stickied": false, "created": 1492045369.0, "created_utc": 1492016569.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg68l44", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "z8xbc4x3", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "u/sje46 had a [great explanation of compression](https://www.reddit.com/r/explainlikeimfive/comments/1dshk6/eli5_a_file_is_often_compressed_to_smaller_size/) that finally helped me understand it.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"/u/sje46\"&gt;u/sje46&lt;/a&gt; had a &lt;a href=\"https://www.reddit.com/r/explainlikeimfive/comments/1dshk6/eli5_a_file_is_often_compressed_to_smaller_size/\"&gt;great explanation of compression&lt;/a&gt; that finally helped me understand it.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg68l44", "score_hidden": false, "stickied": false, "created": 1492047905.0, "created_utc": 1492019105.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": "", "user_reports": [], "id": "dg7eg69", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Digletto", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "It's usually deploying a lot of little tricks to use fewer 'characters' (or bits or whatever it is you're compressing) to write the same thing. Example: 'hhhhhhhbbbbbbeeezz' takes a lot of space but can be written with fewer characters, compressed with: '7h6beeezz'.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s usually deploying a lot of little tricks to use fewer &amp;#39;characters&amp;#39; (or bits or whatever it is you&amp;#39;re compressing) to write the same thing. Example: &amp;#39;hhhhhhhbbbbbbeeezz&amp;#39; takes a lot of space but can be written with fewer characters, compressed with: &amp;#39;7h6beeezz&amp;#39;.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg7eg69", "score_hidden": false, "stickied": false, "created": 1492109976.0, "created_utc": 1492081176.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qm4e", "removal_reason": null, "link_id": "t3_64xl01", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dg6m1mv", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dg6k7ag", "subreddit_name_prefixed": "r/askscience", "controversiality": 0, "body": "[removed]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6m1mv", "score_hidden": false, "stickied": false, "created": 1492062068.0, "created_utc": 1492033268.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 6}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6k7ag", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "[deleted]", "parent_id": "t3_64xl01", "subreddit_name_prefixed": "r/askscience", "controversiality": 1, "body": "[removed]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "askscience", "name": "t1_dg6k7ag", "score_hidden": false, "stickied": false, "created": 1492059917.0, "created_utc": 1492031117.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "more"}], "after": null, "before": null}}]