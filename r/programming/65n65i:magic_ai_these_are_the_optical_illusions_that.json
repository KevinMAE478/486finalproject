[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "programming", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": null, "id": "65n65i", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 168, "report_reasons": null, "author": "ljluzjvert", "saved": false, "mod_reports": [], "name": "t3_65n65i", "subreddit_name_prefixed": "r/programming", "approved_by": null, "over_18": false, "domain": "theverge.com", "hidden": false, "thumbnail": "", "subreddit_id": "t5_2fwo", "edited": false, "link_flair_css_class": null, "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/programming/comments/65n65i/magic_ai_these_are_the_optical_illusions_that/", "num_reports": null, "locked": false, "stickied": false, "created": 1492341749.0, "url": "http://www.theverge.com/2017/4/12/15271874/ai-adversarial-images-fooling-attacks-artificial-intelligence", "author_flair_text": null, "quarantine": false, "title": "Magic AI: these are the optical illusions that trick, fool, and flummox computers", "created_utc": 1492312949.0, "distinguished": null, "media": null, "upvote_ratio": 0.83, "num_comments": 36, "visited": false, "subreddit_type": "public", "ups": 168}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgcyj9k", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "okmkz", "parent_id": "t1_dgctakc", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "No, that's different", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No, that&amp;#39;s different&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcyj9k", "score_hidden": false, "stickied": false, "created": 1492424118.0, "created_utc": 1492395318.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgctakc", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "paxromana96", "parent_id": "t1_dgcfatx", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "How bamboozling.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How bamboozling.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgctakc", "score_hidden": false, "stickied": false, "created": 1492417101.0, "created_utc": 1492388301.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcfatx", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "_Mardoxx", "parent_id": "t1_dgc2k1a", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "All three.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;All three.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcfatx", "score_hidden": false, "stickied": false, "created": 1492398493.0, "created_utc": 1492369693.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dgc2k1a", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "jokr004", "parent_id": "t3_65n65i", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Trick, fool *and* flummox??!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Trick, fool &lt;em&gt;and&lt;/em&gt; flummox??!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgc2k1a", "score_hidden": false, "stickied": false, "created": 1492381074.0, "created_utc": 1492352274.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 9}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgbtme8", "gilded": 0, "archived": false, "score": 23, "report_reasons": null, "author": "rlbond86", "parent_id": "t1_dgbrcrq", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "It's not just some simple vision system, they are using eigenfaces, which is an extremely common method of facial recognition. The article is fairly high-level but they are demonstrating an enormous weakness in industry-standard AI algorithms.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not just some simple vision system, they are using eigenfaces, which is an extremely common method of facial recognition. The article is fairly high-level but they are demonstrating an enormous weakness in industry-standard AI algorithms.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgbtme8", "score_hidden": false, "stickied": false, "created": 1492356959.0, "created_utc": 1492328159.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 23}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dgcpjvd", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Works_of_memercy", "parent_id": "t1_dgcot8e", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "&gt;&gt; Like, you might think that it shouldn't because it relies on fooling a particular trained instance of a particular algorithm, so the carefully crafted noise you add would just turn into random noise from the algorithm's point of view.\n\nIs that the reason you think it can't possibly work with printing a weird noisy picture over the number in a check, undetectable to the naked eye? If yes, then I explained how the state of the art moved __twice__ beyond that, very counter-intuitively if you don't know how those OCR algorithms work, and even if you do.\n\nThey not only fool the same neural net instance they used to train against, they fool instances of the same NN produced by the same algorithm, and they fool instances produced by __different algorithms__ with their images. Because their foolery doesn't rely on a particular weakness of some particular trained OCR algorithm, it apparently relies on the weakness of that entire class of algorithms and all their instances.\n\nReread [this](https://www.reddit.com/r/programming/comments/65n65i/magic_ai_these_are_the_optical_illusions_that/dgcne7l/) with that in mind.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Like, you might think that it shouldn&amp;#39;t because it relies on fooling a particular trained instance of a particular algorithm, so the carefully crafted noise you add would just turn into random noise from the algorithm&amp;#39;s point of view.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Is that the reason you think it can&amp;#39;t possibly work with printing a weird noisy picture over the number in a check, undetectable to the naked eye? If yes, then I explained how the state of the art moved &lt;strong&gt;twice&lt;/strong&gt; beyond that, very counter-intuitively if you don&amp;#39;t know how those OCR algorithms work, and even if you do.&lt;/p&gt;\n\n&lt;p&gt;They not only fool the same neural net instance they used to train against, they fool instances of the same NN produced by the same algorithm, and they fool instances produced by &lt;strong&gt;different algorithms&lt;/strong&gt; with their images. Because their foolery doesn&amp;#39;t rely on a particular weakness of some particular trained OCR algorithm, it apparently relies on the weakness of that entire class of algorithms and all their instances.&lt;/p&gt;\n\n&lt;p&gt;Reread &lt;a href=\"https://www.reddit.com/r/programming/comments/65n65i/magic_ai_these_are_the_optical_illusions_that/dgcne7l/\"&gt;this&lt;/a&gt; with that in mind.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcpjvd", "score_hidden": false, "stickied": false, "created": 1492411988.0, "created_utc": 1492383188.0, "depth": 9, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcot8e", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "queenkid1", "parent_id": "t1_dgcoivk", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "But I'm not arguing about the specific paper you're talking about, because I have no clue what the paper is. And as you said, no PoC actually exists. I'm talking about this article that talks about a low-level concept, and applies a basic problem to a real world solution, as if that somehow proves these stupid glasses actually work.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;But I&amp;#39;m not arguing about the specific paper you&amp;#39;re talking about, because I have no clue what the paper is. And as you said, no PoC actually exists. I&amp;#39;m talking about this article that talks about a low-level concept, and applies a basic problem to a real world solution, as if that somehow proves these stupid glasses actually work.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcot8e", "score_hidden": false, "stickied": false, "created": 1492410966.0, "created_utc": 1492382166.0, "depth": 8, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcoivk", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Works_of_memercy", "parent_id": "t1_dgco79e", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Try rereading the rest of comment where I explained why the belief that it only works with \"perfect, 100x100 pixel images\" is misguided. Then reply to that argument.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try rereading the rest of comment where I explained why the belief that it only works with &amp;quot;perfect, 100x100 pixel images&amp;quot; is misguided. Then reply to that argument.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcoivk", "score_hidden": false, "stickied": false, "created": 1492410569.0, "created_utc": 1492381769.0, "depth": 7, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgd9wrk", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "alexeyr", "parent_id": "t1_dgco79e", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "And it turns out that despite all of that, it still continues to work. Not always, but a decent amount of time. See the paper [\"Adversarial examples in the physical world\"](https://arxiv.org/pdf/1607.02533.pdf). ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And it turns out that despite all of that, it still continues to work. Not always, but a decent amount of time. See the paper &lt;a href=\"https://arxiv.org/pdf/1607.02533.pdf\"&gt;&amp;quot;Adversarial examples in the physical world&amp;quot;&lt;/a&gt;. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgd9wrk", "score_hidden": false, "stickied": false, "created": 1492445810.0, "created_utc": 1492417010.0, "depth": 7, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgco79e", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "queenkid1", "parent_id": "t1_dgcne7l", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "&gt;I've never heard about any proof of concept that actually does that.  \n  \nExactly. When you're dealing with perfect, 100x100 pixel images, then sure, it's easy to fool. But the second you stop giving it entirely artificial images, and start providing *real world* examples, it's not gonna work. There's no way to reliably create that noise, and know that it'll break the system reliably. Instead of modifying a bunch of pixels, you're printing those pixels onto a check. Then it's gonna be scanned, definitely not perfectly straight, and the computer will interpret it. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I&amp;#39;ve never heard about any proof of concept that actually does that.  &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Exactly. When you&amp;#39;re dealing with perfect, 100x100 pixel images, then sure, it&amp;#39;s easy to fool. But the second you stop giving it entirely artificial images, and start providing &lt;em&gt;real world&lt;/em&gt; examples, it&amp;#39;s not gonna work. There&amp;#39;s no way to reliably create that noise, and know that it&amp;#39;ll break the system reliably. Instead of modifying a bunch of pixels, you&amp;#39;re printing those pixels onto a check. Then it&amp;#39;s gonna be scanned, definitely not perfectly straight, and the computer will interpret it. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgco79e", "score_hidden": false, "stickied": false, "created": 1492410122.0, "created_utc": 1492381322.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcne7l", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "Works_of_memercy", "parent_id": "t1_dgcljoi", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Why not?\n\nLike, you might think that it shouldn't because it relies on fooling a particular trained instance of a particular algorithm, so the carefully crafted noise you add would just turn into random noise from the algorithm's point of view.\n\nBut from what I recall from reading one of those papers, the really surprising thing was that their images confounded not just same instances they were trained on, and not just other trained instances but also instances of completely different algorithms with probabilities between 50 and 80%.\n\nBecause, as I understand it, their confounding algorithm totally understood the properties that the usual OCR algorithms end up relying on in detecting numbers, those properties being the properties of training data (that is, handwritten numbers) and then found some orthogonal vector to them that was short in terms of each individual pixel but very long in total (because there's a lot of pixels), and reliably brought the combined result way off that number's detecting hyperplane.\n\nWhich means that it is an inherent weakness of that sort of OCR algorithms. If they can detect off-center, skewed characters, then the off-center, skewed image of the noise that you sprinkled on those characters on paper would lead them astray as well. Probably. I've never heard about any proof of concept that actually does that.", "edited": 1492380566.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why not?&lt;/p&gt;\n\n&lt;p&gt;Like, you might think that it shouldn&amp;#39;t because it relies on fooling a particular trained instance of a particular algorithm, so the carefully crafted noise you add would just turn into random noise from the algorithm&amp;#39;s point of view.&lt;/p&gt;\n\n&lt;p&gt;But from what I recall from reading one of those papers, the really surprising thing was that their images confounded not just same instances they were trained on, and not just other trained instances but also instances of completely different algorithms with probabilities between 50 and 80%.&lt;/p&gt;\n\n&lt;p&gt;Because, as I understand it, their confounding algorithm totally understood the properties that the usual OCR algorithms end up relying on in detecting numbers, those properties being the properties of training data (that is, handwritten numbers) and then found some orthogonal vector to them that was short in terms of each individual pixel but very long in total (because there&amp;#39;s a lot of pixels), and reliably brought the combined result way off that number&amp;#39;s detecting hyperplane.&lt;/p&gt;\n\n&lt;p&gt;Which means that it is an inherent weakness of that sort of OCR algorithms. If they can detect off-center, skewed characters, then the off-center, skewed image of the noise that you sprinkled on those characters on paper would lead them astray as well. Probably. I&amp;#39;ve never heard about any proof of concept that actually does that.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcne7l", "score_hidden": false, "stickied": false, "created": 1492409053.0, "created_utc": 1492380253.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 4}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcljoi", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "queenkid1", "parent_id": "t1_dgbutst", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "There's a difference between discerning between small images of letters, and reading real handwriting off of checks. A \"trick\" that relies on modifying the base of the image, eg. the coloured overlays in this article, would never work on a *real* check.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a difference between discerning between small images of letters, and reading real handwriting off of checks. A &amp;quot;trick&amp;quot; that relies on modifying the base of the image, eg. the coloured overlays in this article, would never work on a &lt;em&gt;real&lt;/em&gt; check.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcljoi", "score_hidden": false, "stickied": false, "created": 1492406617.0, "created_utc": 1492377817.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgbvbf6", "gilded": 0, "archived": false, "score": -1, "report_reasons": null, "author": "josefx", "parent_id": "t1_dgbutst", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "&gt;  such as classifiers that read handwritten digits can be fooled\n\nTry scanning a printed text with OCR enabled. For Xerox 8 and 0 were close enough for professional office use. Nobody is going to test it anyway so just ship it, while the code is still steaming. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;such as classifiers that read handwritten digits can be fooled&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Try scanning a printed text with OCR enabled. For Xerox 8 and 0 were close enough for professional office use. Nobody is going to test it anyway so just ship it, while the code is still steaming. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgbvbf6", "score_hidden": false, "stickied": false, "created": 1492362297.0, "created_utc": 1492333497.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": -1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbutst", "gilded": 0, "archived": false, "score": 13, "report_reasons": null, "author": "sickofthisshit", "parent_id": "t1_dgbsblt", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "I don't see why you draw such a distinction. Of course different models will have different weaknesses, but in both cases, the models have found what actually are only very thin hyperplanes in the space, and only very small perturbations make the classifier completely inaccurate.\n\nThere are other examples, such as classifiers that read handwritten digits can be fooled in such a way that the original digit can be made into any other digit. If my AI is reading handwritten checks, it really casts doubt on the whole project. \n\nWe understand roughly how humans can be fooled. These machines can be *completely* fooled, yet be highly confident, by something that doesn't perceptibly change what the human sees.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t see why you draw such a distinction. Of course different models will have different weaknesses, but in both cases, the models have found what actually are only very thin hyperplanes in the space, and only very small perturbations make the classifier completely inaccurate.&lt;/p&gt;\n\n&lt;p&gt;There are other examples, such as classifiers that read handwritten digits can be fooled in such a way that the original digit can be made into any other digit. If my AI is reading handwritten checks, it really casts doubt on the whole project. &lt;/p&gt;\n\n&lt;p&gt;We understand roughly how humans can be fooled. These machines can be &lt;em&gt;completely&lt;/em&gt; fooled, yet be highly confident, by something that doesn&amp;#39;t perceptibly change what the human sees.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgbutst", "score_hidden": false, "stickied": false, "created": 1492360674.0, "created_utc": 1492331874.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 13}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbsblt", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "queenkid1", "parent_id": "t1_dgbs92d", "subreddit_name_prefixed": "r/programming", "controversiality": 1, "body": "Yes, but you can't call a system broken because edge cases exist. Edge cases always exist. But overlaying a pseudorandom pixel image isn't feasible for an actual system.  \n  \nMy problem is that yes, these issues exist, but the glasses they show off have absolutely no relation. Covering your image with subtle random pixels that throw off a computer vision system isn't the same as a pair of glasses with some lines on them. This article is based on a false equivolency. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, but you can&amp;#39;t call a system broken because edge cases exist. Edge cases always exist. But overlaying a pseudorandom pixel image isn&amp;#39;t feasible for an actual system.  &lt;/p&gt;\n\n&lt;p&gt;My problem is that yes, these issues exist, but the glasses they show off have absolutely no relation. Covering your image with subtle random pixels that throw off a computer vision system isn&amp;#39;t the same as a pair of glasses with some lines on them. This article is based on a false equivolency. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgbsblt", "score_hidden": false, "stickied": false, "created": 1492353503.0, "created_utc": 1492324703.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbs92d", "gilded": 0, "archived": false, "score": 17, "report_reasons": null, "author": "sickofthisshit", "parent_id": "t1_dgbrcrq", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "I think you are misreading that part of the article. The point is that *making a model resistant to adversarial images* is expensive, because each additional training image is costly, but doesn't necessarily add much resistance, because there are so many possible adversarial images.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think you are misreading that part of the article. The point is that &lt;em&gt;making a model resistant to adversarial images&lt;/em&gt; is expensive, because each additional training image is costly, but doesn&amp;#39;t necessarily add much resistance, because there are so many possible adversarial images.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgbs92d", "score_hidden": false, "stickied": false, "created": 1492353329.0, "created_utc": 1492324529.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 17}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgc4vbf", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "yaemes", "parent_id": "t1_dgbrcrq", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Isn't adverserial neural networks being used wrong too? Adverserial applies to the nn, not the image. There can not be an \"adverserial image\"", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Isn&amp;#39;t adverserial neural networks being used wrong too? Adverserial applies to the nn, not the image. There can not be an &amp;quot;adverserial image&amp;quot;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgc4vbf", "score_hidden": false, "stickied": false, "created": 1492384581.0, "created_utc": 1492355781.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbrcrq", "gilded": 0, "archived": false, "score": 28, "report_reasons": null, "author": "queenkid1", "parent_id": "t3_65n65i", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "This article stinks of buzzword bullshit. It makes outlandish claims, and backs them up by explaining the *extreme* basics.   \n  \n&gt;Unfortunately, as Nicolas Papernot, a graduate student at Pennsylvania State University who\u2019s written a number of papers on adversarial attacks, explains, even this sort of training is weak against \u201ccomputationally intensive strategies\u201d (i.e, throw enough images at the system and it\u2019ll eventually fail).  \n  \nThis exactly shows the issues with these \"glasses\", they've built a simple vision system and then have found non-working edge cases. It would never work in a system actually implemented in the real world.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This article stinks of buzzword bullshit. It makes outlandish claims, and backs them up by explaining the &lt;em&gt;extreme&lt;/em&gt; basics.   &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Unfortunately, as Nicolas Papernot, a graduate student at Pennsylvania State University who\u2019s written a number of papers on adversarial attacks, explains, even this sort of training is weak against \u201ccomputationally intensive strategies\u201d (i.e, throw enough images at the system and it\u2019ll eventually fail).  &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This exactly shows the issues with these &amp;quot;glasses&amp;quot;, they&amp;#39;ve built a simple vision system and then have found non-working edge cases. It would never work in a system actually implemented in the real world.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgbrcrq", "score_hidden": false, "stickied": false, "created": 1492351200.0, "created_utc": 1492322400.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 28}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgcl67f", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "earthboundkid", "parent_id": "t1_dgbw2om", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "&gt; It may be news to people who haven't done their prior research but it shouldn't be news to anyone serious in the space.\n\nThat's why this is an article on the Verge and not in ACM. Knowledge moves from researchers to the public via the press.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;It may be news to people who haven&amp;#39;t done their prior research but it shouldn&amp;#39;t be news to anyone serious in the space.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;That&amp;#39;s why this is an article on the Verge and not in ACM. Knowledge moves from researchers to the public via the press.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcl67f", "score_hidden": false, "stickied": false, "created": 1492406139.0, "created_utc": 1492377339.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgce28x", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "RaionTategami", "parent_id": "t1_dgbw2om", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "I think it's a realistic goal to make them not be fooled in ways that humans wouldn't be. I think these are suggesting that ML works differently to human intelligence right now.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think it&amp;#39;s a realistic goal to make them not be fooled in ways that humans wouldn&amp;#39;t be. I think these are suggesting that ML works differently to human intelligence right now.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgce28x", "score_hidden": false, "stickied": false, "created": 1492396838.0, "created_utc": 1492368038.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgcumzz", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "addmoreice", "parent_id": "t1_dgbw2om", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "which should be a big eye roll to anyone in the ML space.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;which should be a big eye roll to anyone in the ML space.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcumzz", "score_hidden": false, "stickied": false, "created": 1492418899.0, "created_utc": 1492390099.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbw2om", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "SentientEnglishman", "parent_id": "t1_dgbvq3z", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Indeed, and anyone employing ML techniques should be aware of this. It may be news to people who haven't done their prior research but it shouldn't be news to anyone serious in the space.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Indeed, and anyone employing ML techniques should be aware of this. It may be news to people who haven&amp;#39;t done their prior research but it shouldn&amp;#39;t be news to anyone serious in the space.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgbw2om", "score_hidden": false, "stickied": false, "created": 1492364920.0, "created_utc": 1492336120.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 9}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbvq3z", "gilded": 0, "archived": false, "score": 26, "report_reasons": null, "author": "sickofthisshit", "parent_id": "t1_dgbs6ae", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "I think it is still important to understand what we mean by \"imperfect.\" These ML models turn out to be easily manipulated into making wrong, yet highly confident, classifications that are imperceptible to human classifiers.\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think it is still important to understand what we mean by &amp;quot;imperfect.&amp;quot; These ML models turn out to be easily manipulated into making wrong, yet highly confident, classifications that are imperceptible to human classifiers.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgbvq3z", "score_hidden": false, "stickied": false, "created": 1492363686.0, "created_utc": 1492334886.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 26}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgc375l", "gilded": 0, "archived": false, "score": 11, "report_reasons": null, "author": "darkmighty", "parent_id": "t1_dgbs6ae", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Everyone expects them to not be perfect, but also they expect that the perturbation necessary to get good classifiers to misclassify should be large -- it's absurd for a human that an image of a mouse could be classified, with barely distinguishable random noise, into an elephant, which is very semantically \"distant\". It would be more acceptable if it were mistaking subgenera of mice: say mistaking a lab mouse to a wild one. This shows a lack of high level logical/hierarchical behavior: it is probably differentiating between objects using small details (but doing so extremely well), not dividing them into high level classes like \"animals\", \"fish\", \"musical instruments\" or \"human faces\", and subsequently refining those classes into subclasses.", "edited": 1492367678.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Everyone expects them to not be perfect, but also they expect that the perturbation necessary to get good classifiers to misclassify should be large -- it&amp;#39;s absurd for a human that an image of a mouse could be classified, with barely distinguishable random noise, into an elephant, which is very semantically &amp;quot;distant&amp;quot;. It would be more acceptable if it were mistaking subgenera of mice: say mistaking a lab mouse to a wild one. This shows a lack of high level logical/hierarchical behavior: it is probably differentiating between objects using small details (but doing so extremely well), not dividing them into high level classes like &amp;quot;animals&amp;quot;, &amp;quot;fish&amp;quot;, &amp;quot;musical instruments&amp;quot; or &amp;quot;human faces&amp;quot;, and subsequently refining those classes into subclasses.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgc375l", "score_hidden": false, "stickied": false, "created": 1492382071.0, "created_utc": 1492353271.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 11}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgc1bdx", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "brian_1970", "parent_id": "t1_dgbs6ae", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Yea, it's a sign that you are perhaps not using the 'right' feature set in the classifier. Classification should be the same as compression, squeezing the information down while keeping the original signal recognizable at a human level.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yea, it&amp;#39;s a sign that you are perhaps not using the &amp;#39;right&amp;#39; feature set in the classifier. Classification should be the same as compression, squeezing the information down while keeping the original signal recognizable at a human level.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgc1bdx", "score_hidden": false, "stickied": false, "created": 1492378940.0, "created_utc": 1492350140.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgcfaim", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "_Mardoxx", "parent_id": "t1_dgbs6ae", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Lol what. Yeah because you'd mistake random noise over a panda for a fucking giraffe or whatever it was lol. Stop being a pedant.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Lol what. Yeah because you&amp;#39;d mistake random noise over a panda for a fucking giraffe or whatever it was lol. Stop being a pedant.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcfaim", "score_hidden": false, "stickied": false, "created": 1492398482.0, "created_utc": 1492369682.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbs6ae", "gilded": 0, "archived": false, "score": 12, "report_reasons": null, "author": "SentientEnglishman", "parent_id": "t3_65n65i", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Classifiers always have these sorts of problems, even humans do. Unless you have a perfect 1-1 mapping of input data to output classification (which you won't in reality) then you're going to have a classifier which isn't perfect. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Classifiers always have these sorts of problems, even humans do. Unless you have a perfect 1-1 mapping of input data to output classification (which you won&amp;#39;t in reality) then you&amp;#39;re going to have a classifier which isn&amp;#39;t perfect. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgbs6ae", "score_hidden": false, "stickied": false, "created": 1492353145.0, "created_utc": 1492324345.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 12}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgcvtx8", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "YM_Industries", "parent_id": "t1_dgcc52k", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "I think querying someone else's AI would require more than a few thousand attempts, because they almost certainly won't give you the raw percentage data of the results. This means you can't tell if you're getting closer or not, it becomes completely brute force.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think querying someone else&amp;#39;s AI would require more than a few thousand attempts, because they almost certainly won&amp;#39;t give you the raw percentage data of the results. This means you can&amp;#39;t tell if you&amp;#39;re getting closer or not, it becomes completely brute force.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcvtx8", "score_hidden": false, "stickied": false, "created": 1492420470.0, "created_utc": 1492391670.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcc52k", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "mike_bolt", "parent_id": "t1_dgc5ui5", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "That's exactly what I was thinking, and I think you're right.\n\nWhen the article says \"you often \u2014 but not always \u2014 need access to the internal code of the system you\u2019re trying to manipulate in order to generate the perturbation,\" I think it means that you need the ability to query the fully functional AI. You could do that using the source code and the entire training dataset, or you could do it against a \"secure\" AI hosted on a server that you could query a few thousand times.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s exactly what I was thinking, and I think you&amp;#39;re right.&lt;/p&gt;\n\n&lt;p&gt;When the article says &amp;quot;you often \u2014 but not always \u2014 need access to the internal code of the system you\u2019re trying to manipulate in order to generate the perturbation,&amp;quot; I think it means that you need the ability to query the fully functional AI. You could do that using the source code and the entire training dataset, or you could do it against a &amp;quot;secure&amp;quot; AI hosted on a server that you could query a few thousand times.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcc52k", "score_hidden": false, "stickied": false, "created": 1492394257.0, "created_utc": 1492365457.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgd9ksk", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "alexeyr", "parent_id": "t1_dgc5ui5", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Quotes from https://blog.acolyer.org/2017/02/28/when-dnns-go-wrong-adversarial-examples-and-what-we-can-learn-from-them/.\n\n&gt; The attack may succeed with some good luck, but I'd expect that any change in camera angle would prevent it, so with data accumulated over several frames of a video it may not work.\n\nAdversarial examples in the physical world: \"The authors print clean and adversarial images, take photos of the printed images, crop those photos to be the same size as the originals, and then pass these into the classifier. The procedure takes place with manual photography and no careful control of lighting, camera angle etc., thus in introduces nuisance variability with the potential to destroy adversarial perturbations depending on subtle changes.\"\n\n&gt; I was of the belief that generating adversarial images required specific knowledge of the algorithm and training data used to create the target neural net\n\n&gt; I think querying someone else's AI would require more than a few thousand attempts, because they almost certainly won't give you the raw percentage data of the results. This means you can't tell if you're getting closer or not, it becomes completely brute force.\n\nPractical black-box attacks against deep learning systems using adversarial examples: \"In fact, we instantiate our attack against classifiers served by MetaMind, Amazon, and Google. Models are automatically trained by the hosting platform. We are capable of making labeling prediction queries only after training is completed. Thus, we provide the first correctly blinded experiments concerning adversarial examples as a security risk.\"", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Quotes from &lt;a href=\"https://blog.acolyer.org/2017/02/28/when-dnns-go-wrong-adversarial-examples-and-what-we-can-learn-from-them/\"&gt;https://blog.acolyer.org/2017/02/28/when-dnns-go-wrong-adversarial-examples-and-what-we-can-learn-from-them/&lt;/a&gt;.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The attack may succeed with some good luck, but I&amp;#39;d expect that any change in camera angle would prevent it, so with data accumulated over several frames of a video it may not work.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Adversarial examples in the physical world: &amp;quot;The authors print clean and adversarial images, take photos of the printed images, crop those photos to be the same size as the originals, and then pass these into the classifier. The procedure takes place with manual photography and no careful control of lighting, camera angle etc., thus in introduces nuisance variability with the potential to destroy adversarial perturbations depending on subtle changes.&amp;quot;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;I was of the belief that generating adversarial images required specific knowledge of the algorithm and training data used to create the target neural net&lt;/p&gt;\n\n&lt;p&gt;I think querying someone else&amp;#39;s AI would require more than a few thousand attempts, because they almost certainly won&amp;#39;t give you the raw percentage data of the results. This means you can&amp;#39;t tell if you&amp;#39;re getting closer or not, it becomes completely brute force.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Practical black-box attacks against deep learning systems using adversarial examples: &amp;quot;In fact, we instantiate our attack against classifiers served by MetaMind, Amazon, and Google. Models are automatically trained by the hosting platform. We are capable of making labeling prediction queries only after training is completed. Thus, we provide the first correctly blinded experiments concerning adversarial examples as a security risk.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgd9ksk", "score_hidden": false, "stickied": false, "created": 1492444723.0, "created_utc": 1492415923.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgc5ui5", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "YM_Industries", "parent_id": "t3_65n65i", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "I don't have much knowledge around machine learning, but to me the threat seems overstated. In the caption of the example image, it refers to the glasses as \"simulated fooling glasses\". Closer inspection of the image reveals that the glasses don't really exist, they have been photoshopped in. This suggests to me that such an attack couldn't be replicated using real glasses, as my understanding is that adversarial images require placement of pixels in specific places and colours to manipulate the neural net. With physical glasses, the positioning, angle and lighting are going to change, which I would expect to render the attack extremely difficult, if not completely ineffective. The attack may succeed with some good luck, but I'd expect that any change in camera angle would prevent it, so with data accumulated over several frames of a video it may not work.\n\nAdditionally, I was of the belief that generating adversarial images required specific knowledge of the algorithm and training data used to create the target neural net. There's been a few articles about adversarial images posted here recently, but I'm not yet convinced that they are a practical exploit.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t have much knowledge around machine learning, but to me the threat seems overstated. In the caption of the example image, it refers to the glasses as &amp;quot;simulated fooling glasses&amp;quot;. Closer inspection of the image reveals that the glasses don&amp;#39;t really exist, they have been photoshopped in. This suggests to me that such an attack couldn&amp;#39;t be replicated using real glasses, as my understanding is that adversarial images require placement of pixels in specific places and colours to manipulate the neural net. With physical glasses, the positioning, angle and lighting are going to change, which I would expect to render the attack extremely difficult, if not completely ineffective. The attack may succeed with some good luck, but I&amp;#39;d expect that any change in camera angle would prevent it, so with data accumulated over several frames of a video it may not work.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I was of the belief that generating adversarial images required specific knowledge of the algorithm and training data used to create the target neural net. There&amp;#39;s been a few articles about adversarial images posted here recently, but I&amp;#39;m not yet convinced that they are a practical exploit.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgc5ui5", "score_hidden": false, "stickied": false, "created": 1492385929.0, "created_utc": 1492357129.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgcdiwn", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "cannibalsock", "parent_id": "t3_65n65i", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "&gt; hallucinogenic print\u00a0", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;hallucinogenic print\u00a0&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgcdiwn", "score_hidden": false, "stickied": false, "created": 1492396107.0, "created_utc": 1492367307.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2fwo", "removal_reason": null, "link_id": "t3_65n65i", "likes": null, "replies": "", "user_reports": [], "id": "dgd9ht7", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "alexeyr", "parent_id": "t3_65n65i", "subreddit_name_prefixed": "r/programming", "controversiality": 0, "body": "Some of the papers in this area explained on The Morning Paper: https://blog.acolyer.org/2017/02/28/when-dnns-go-wrong-adversarial-examples-and-what-we-can-learn-from-them/", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Some of the papers in this area explained on The Morning Paper: &lt;a href=\"https://blog.acolyer.org/2017/02/28/when-dnns-go-wrong-adversarial-examples-and-what-we-can-learn-from-them/\"&gt;https://blog.acolyer.org/2017/02/28/when-dnns-go-wrong-adversarial-examples-and-what-we-can-learn-from-them/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "programming", "name": "t1_dgd9ht7", "score_hidden": false, "stickied": false, "created": 1492444462.0, "created_utc": 1492415662.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]