[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "math", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Just starting branching my knowledge toward the world of statistician &lt;del&gt;and fun&lt;/del&gt;. I am currently stretching my head to figure out about what is the true significant of MCMC (that&amp;#39;s how it is normally abbreviated, right...?)&lt;/p&gt;\n\n&lt;p&gt;So things like:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How did it become such a huge deal? And why is it a huge deal for people in fields like data mining? Machine learning? Etc.&lt;/li&gt;\n&lt;li&gt;What is the real challenge to do it? I guess the main thing would be making it more efficient, since it&amp;#39;s like throwing random darts - you in real life want to be better at throwing them so you won&amp;#39;t have to throw a gazillion of time before you hit the bull eye (ok I am using bare minimum knowledge in probabilistic here...)&lt;/li&gt;\n&lt;li&gt;If it is inefficient, how do people, like, tell how inefficient it is? Like if I am a beginner in dart throwing, I would say I would waste 500 throws before hitting a bulls-eye. A pro may nail in 1 or 2 - that may be a good measurement of efficiency. But if you are sampling a distribution, say, you don&amp;#39;t know anything about it. So how can you know you have hit, I don&amp;#39;t know, the bull-eye?&lt;/li&gt;\n&lt;li&gt;If it is so inefficient (I learnt something about the curse of dimensions...?), are people trying to make it faster? More efficient? Something like that.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m sorry if this is not in the right sub - just a random mechanical guy stuck in the academic world who is willing to educate himself about this beautiful world of MCMC.&lt;/p&gt;\n\n&lt;p&gt;Ok I have used all my fancy words. I am sorry if you have to puke rainbow after reading this... &lt;/p&gt;\n\n&lt;p&gt;EDIT: Thank you reddits!!!! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "Hi,\n\nJust starting branching my knowledge toward the world of statistician ~~and fun~~. I am currently stretching my head to figure out about what is the true significant of MCMC (that's how it is normally abbreviated, right...?)\n\nSo things like:\n\n1. How did it become such a huge deal? And why is it a huge deal for people in fields like data mining? Machine learning? Etc.\n1. What is the real challenge to do it? I guess the main thing would be making it more efficient, since it's like throwing random darts - you in real life want to be better at throwing them so you won't have to throw a gazillion of time before you hit the bull eye (ok I am using bare minimum knowledge in probabilistic here...)\n1. If it is inefficient, how do people, like, tell how inefficient it is? Like if I am a beginner in dart throwing, I would say I would waste 500 throws before hitting a bulls-eye. A pro may nail in 1 or 2 - that may be a good measurement of efficiency. But if you are sampling a distribution, say, you don't know anything about it. So how can you know you have hit, I don't know, the bull-eye?\n1. If it is so inefficient (I learnt something about the curse of dimensions...?), are people trying to make it faster? More efficient? Something like that.\n\nI'm sorry if this is not in the right sub - just a random mechanical guy stuck in the academic world who is willing to educate himself about this beautiful world of MCMC.\n\nOk I have used all my fancy words. I am sorry if you have to puke rainbow after reading this... \n\nEDIT: Thank you reddits!!!! ", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": null, "id": "653idk", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 18, "report_reasons": null, "author": "QueueTee314", "saved": false, "mod_reports": [], "name": "t3_653idk", "subreddit_name_prefixed": "r/math", "approved_by": null, "over_18": false, "domain": "self.math", "hidden": false, "thumbnail": "", "subreddit_id": "t5_2qh0n", "edited": 1492266425.0, "link_flair_css_class": null, "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/math/comments/653idk/why_is_sampling_by_monte_carlomarkov_chain_such_a/", "num_reports": null, "locked": false, "stickied": false, "created": 1492088539.0, "url": "https://www.reddit.com/r/math/comments/653idk/why_is_sampling_by_monte_carlomarkov_chain_such_a/", "author_flair_text": null, "quarantine": false, "title": "Why is sampling by Monte Carlo-Markov Chain such a big thing? (plus other questions)", "created_utc": 1492059739.0, "distinguished": null, "media": null, "upvote_ratio": 1.0, "num_comments": 7, "visited": false, "subreddit_type": "public", "ups": 18}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qh0n", "removal_reason": null, "link_id": "t3_653idk", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qh0n", "removal_reason": null, "link_id": "t3_653idk", "likes": null, "replies": "", "user_reports": [], "id": "dg8pxo7", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "HM_D", "parent_id": "t1_dg77zjw", "subreddit_name_prefixed": "r/math", "controversiality": 0, "body": "I like this, but there a few small things I would emphasize:\n\n1. MCMC became a big deal because there was no other way to do lots of integrals, especially high-dimensional integrals (originally in physics, later in statistics). It remains a big deal because it is still the simplest general algorithm for doing these sorts of computations in moderate to high dimensions.\n\n2. Designing good chains is hard (there are lots of heuristics about this, as mentioned above).\n\n3. There are lots of diagnostics. Many start with the autocorrelation function. None are perfect.\n\n4. Most important - Monte Carlo methods can (sometimes) avoid the curse of dimensionality (to a degree), scaling polynomially rather than exponentially in the dimension. This is basically why they are used over the \"grid\" method described above. It is hard to know when this good thing happens, though there are lots of papers with sufficient conditions. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I like this, but there a few small things I would emphasize:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;MCMC became a big deal because there was no other way to do lots of integrals, especially high-dimensional integrals (originally in physics, later in statistics). It remains a big deal because it is still the simplest general algorithm for doing these sorts of computations in moderate to high dimensions.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Designing good chains is hard (there are lots of heuristics about this, as mentioned above).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There are lots of diagnostics. Many start with the autocorrelation function. None are perfect.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Most important - Monte Carlo methods can (sometimes) avoid the curse of dimensionality (to a degree), scaling polynomially rather than exponentially in the dimension. This is basically why they are used over the &amp;quot;grid&amp;quot; method described above. It is hard to know when this good thing happens, though there are lots of papers with sufficient conditions. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;", "subreddit": "math", "name": "t1_dg8pxo7", "score_hidden": false, "stickied": false, "created": 1492170665.0, "created_utc": 1492141865.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg77zjw", "gilded": 0, "archived": false, "score": 11, "report_reasons": null, "author": "Euthyphron_", "parent_id": "t3_653idk", "subreddit_name_prefixed": "r/math", "controversiality": 0, "body": "MCMC is the standard method for sampling from an arbitrary (absolutely continuous) probability distribution. Sampling from a uniform distribution is not difficult (using most pseudo-random number generators); sampling normal variables can be done with some arithmetic but whenever you have a statistical model with a pdf that's more complicated than these two, you'll probably want to use MCMC. Also, many types of MCMC don't need normalised pdfs; in practice, the normalisation of a given pdf can be very costly or intractable. MCMC is heavily used in Bayesian statistics and modelling.\n\nBy the way, there are many MCMC algorithms, and they are all very different, so I will concentrate on the Metropolis-Hastings algorithm which is the standard.\n\nMCMC has its drawbacks. For one, it has a burn-in phase, meaning that you have to throw away potentially lots of samples first to get a good approximation of the distribution. As MCMC is just a simulation of a Markov chain with a given *stationary* distribution, the result will converge to that stationary distribution, but it will differ in the beginning depending on the initial condition.\n\nNow, in the case of MH MCMC, are usually quite strongly correlated - MH gives you de facto a random walk in parameter space, meaning that samples will tend to cluster locally instead of being distributed across the whole space as you'd expect from independent draws. If you don't set your algorithms' parameters correctly you will 'see' the path the algorithm took - while independent samples would just look like a completely random collection of dots with density varying according to the pdf. This can be avoided by the use of e.g. Hamiltonian MCMC.\n\nAbout the efficiency part... It really depends. When people say it's inefficient, it really just means their simulations need too much time. You cannot canonically quantify that. However, in many cases, MH and Hamiltonian MH waste a lot of time because they have to evaluate the pdf at a proposed new point which might get rejected anyway, especially if the last point is a high-probability one, and for complicated models you have to run many simulations to find the pdf at any given point. \n\nThe curse of dimensions you speak generally is a general problem for many algorithms. Say you want to sample from a uniform NxNx...xN grid in d dimensions. In one dimension, you need to sample from N points. In two you need N\u00b2. In three you need N\u00b3 etc., so the number of samples required is exponential in the number of dimensions, because there is much more volume available. Similarly, in high dimensions, if you want to traverse parameter space in a balanced manner, you will need lots and lots of sample points to explore all that volume. This is often wasteful. In many cases one has, say, 100 parameters and wants to determine the parameters for a given model. If one knows, for example, that the likelihood of the parameters is concentrated on e.g. a hyperplane or a low-dimensional surface, then one will normally waste lots of time exploring implausible parameter values away from that hyperplane or surface. There are algorithms that avoid that.\n\nHope that helped! I'm sure others will find more to add.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MCMC is the standard method for sampling from an arbitrary (absolutely continuous) probability distribution. Sampling from a uniform distribution is not difficult (using most pseudo-random number generators); sampling normal variables can be done with some arithmetic but whenever you have a statistical model with a pdf that&amp;#39;s more complicated than these two, you&amp;#39;ll probably want to use MCMC. Also, many types of MCMC don&amp;#39;t need normalised pdfs; in practice, the normalisation of a given pdf can be very costly or intractable. MCMC is heavily used in Bayesian statistics and modelling.&lt;/p&gt;\n\n&lt;p&gt;By the way, there are many MCMC algorithms, and they are all very different, so I will concentrate on the Metropolis-Hastings algorithm which is the standard.&lt;/p&gt;\n\n&lt;p&gt;MCMC has its drawbacks. For one, it has a burn-in phase, meaning that you have to throw away potentially lots of samples first to get a good approximation of the distribution. As MCMC is just a simulation of a Markov chain with a given &lt;em&gt;stationary&lt;/em&gt; distribution, the result will converge to that stationary distribution, but it will differ in the beginning depending on the initial condition.&lt;/p&gt;\n\n&lt;p&gt;Now, in the case of MH MCMC, are usually quite strongly correlated - MH gives you de facto a random walk in parameter space, meaning that samples will tend to cluster locally instead of being distributed across the whole space as you&amp;#39;d expect from independent draws. If you don&amp;#39;t set your algorithms&amp;#39; parameters correctly you will &amp;#39;see&amp;#39; the path the algorithm took - while independent samples would just look like a completely random collection of dots with density varying according to the pdf. This can be avoided by the use of e.g. Hamiltonian MCMC.&lt;/p&gt;\n\n&lt;p&gt;About the efficiency part... It really depends. When people say it&amp;#39;s inefficient, it really just means their simulations need too much time. You cannot canonically quantify that. However, in many cases, MH and Hamiltonian MH waste a lot of time because they have to evaluate the pdf at a proposed new point which might get rejected anyway, especially if the last point is a high-probability one, and for complicated models you have to run many simulations to find the pdf at any given point. &lt;/p&gt;\n\n&lt;p&gt;The curse of dimensions you speak generally is a general problem for many algorithms. Say you want to sample from a uniform NxNx...xN grid in d dimensions. In one dimension, you need to sample from N points. In two you need N\u00b2. In three you need N\u00b3 etc., so the number of samples required is exponential in the number of dimensions, because there is much more volume available. Similarly, in high dimensions, if you want to traverse parameter space in a balanced manner, you will need lots and lots of sample points to explore all that volume. This is often wasteful. In many cases one has, say, 100 parameters and wants to determine the parameters for a given model. If one knows, for example, that the likelihood of the parameters is concentrated on e.g. a hyperplane or a low-dimensional surface, then one will normally waste lots of time exploring implausible parameter values away from that hyperplane or surface. There are algorithms that avoid that.&lt;/p&gt;\n\n&lt;p&gt;Hope that helped! I&amp;#39;m sure others will find more to add.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "math", "name": "t1_dg77zjw", "score_hidden": false, "stickied": false, "created": 1492091789.0, "created_utc": 1492062989.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 11}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qh0n", "removal_reason": null, "link_id": "t3_653idk", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qh0n", "removal_reason": null, "link_id": "t3_653idk", "likes": null, "replies": "", "user_reports": [], "id": "dg7wmna", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "Euthyphron_", "parent_id": "t1_dg7bizt", "subreddit_name_prefixed": "r/math", "controversiality": 0, "body": "Careful in the second paragraph! A Gaussian curve is clearly concentrated around the origin (in any dimension), but in high dimensions the volume of the central part becomes negligible compared to the volume of the tails, meaning that it actually makes more sense to concentrate on a spherical shell away from the centre. To see this in action, consider the chi squared distributions (which describe the norm of a k-dimensional normally distributed variable), whose pdfs shift away from the origin with increasing k.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Careful in the second paragraph! A Gaussian curve is clearly concentrated around the origin (in any dimension), but in high dimensions the volume of the central part becomes negligible compared to the volume of the tails, meaning that it actually makes more sense to concentrate on a spherical shell away from the centre. To see this in action, consider the chi squared distributions (which describe the norm of a k-dimensional normally distributed variable), whose pdfs shift away from the origin with increasing k.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "math", "name": "t1_dg7wmna", "score_hidden": false, "stickied": false, "created": 1492133787.0, "created_utc": 1492104987.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 6}}], "after": null, "before": null}}, "user_reports": [], "id": "dg7bizt", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "Ieafeator", "parent_id": "t3_653idk", "subreddit_name_prefixed": "r/math", "controversiality": 0, "body": "Sometimes you have a multidimensional integral that you can't solve symbolically, especially in Bayesian statistics. If you have a one-dimensional integral, you just throw it in the computer and have it sample a million points and there's your answer. But for a two-dimensional integral you need 10^6 \\* 10^6 = 10^12 points to get the same accuracy, and it only gets worse. That's basically the \"curse of dimensionality\": high dimensional space is really big.\n\nWhat people noticed is though that even if the space is big, most of the weight is actually in a very small portion, especially again in Bayesian statistics (look at a picture of a multivariate normal distribution until you understand). So the idea is to find where the weight is, sample it accurately and ignore all the empty space. MCMC is a method for doing that. It has nice guarantees that in the limit it will converge to the real answer, so you only have to worry about making it fast by tuning the parameters and the maybe method itself (using e.g. momentum methods like Hamiltonian MCMC). It's a big deal because it's simple, mathematically elegant and makes the impossible possible. It became so popular that Bayesian and MCMC started to become almost synonymous.\n\nIt's not perfect though. In practice, it's still pretty slow, even with all the right tricks. It can take hours, days, weeks to get it where you feel comfortable it has converged. Compared to Frequentist methods that were designed to be done by hand with a lookup table, it's no wonder people choose instant and wrong over slow and correct. That's why I'm interested in approximate methods like Variational Inference and Expectation Propagation, which are fast (on a computer) and approximately correct.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sometimes you have a multidimensional integral that you can&amp;#39;t solve symbolically, especially in Bayesian statistics. If you have a one-dimensional integral, you just throw it in the computer and have it sample a million points and there&amp;#39;s your answer. But for a two-dimensional integral you need 10&lt;sup&gt;6&lt;/sup&gt; * 10&lt;sup&gt;6&lt;/sup&gt; = 10&lt;sup&gt;12&lt;/sup&gt; points to get the same accuracy, and it only gets worse. That&amp;#39;s basically the &amp;quot;curse of dimensionality&amp;quot;: high dimensional space is really big.&lt;/p&gt;\n\n&lt;p&gt;What people noticed is though that even if the space is big, most of the weight is actually in a very small portion, especially again in Bayesian statistics (look at a picture of a multivariate normal distribution until you understand). So the idea is to find where the weight is, sample it accurately and ignore all the empty space. MCMC is a method for doing that. It has nice guarantees that in the limit it will converge to the real answer, so you only have to worry about making it fast by tuning the parameters and the maybe method itself (using e.g. momentum methods like Hamiltonian MCMC). It&amp;#39;s a big deal because it&amp;#39;s simple, mathematically elegant and makes the impossible possible. It became so popular that Bayesian and MCMC started to become almost synonymous.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not perfect though. In practice, it&amp;#39;s still pretty slow, even with all the right tricks. It can take hours, days, weeks to get it where you feel comfortable it has converged. Compared to Frequentist methods that were designed to be done by hand with a lookup table, it&amp;#39;s no wonder people choose instant and wrong over slow and correct. That&amp;#39;s why I&amp;#39;m interested in approximate methods like Variational Inference and Expectation Propagation, which are fast (on a computer) and approximately correct.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "math", "name": "t1_dg7bizt", "score_hidden": false, "stickied": false, "created": 1492101247.0, "created_utc": 1492072447.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qh0n", "removal_reason": null, "link_id": "t3_653idk", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qh0n", "removal_reason": null, "link_id": "t3_653idk", "likes": null, "replies": "", "user_reports": [], "id": "dg7cv4j", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "RadicalOlly", "parent_id": "t1_dg78tnx", "subreddit_name_prefixed": "r/math", "controversiality": 0, "body": "&gt;The probability of the top and bottom cards being aces is (1/13)*(12/51).\n\nThe probability is (1/13)\\*(3/51) = (1/13)\\*(1/17) = 1/221", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The probability of the top and bottom cards being aces is (1/13)*(12/51).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The probability is (1/13)*(3/51) = (1/13)*(1/17) = 1/221&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "math", "name": "t1_dg7cv4j", "score_hidden": false, "stickied": false, "created": 1492105363.0, "created_utc": 1492076563.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dg78tnx", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "boyobo", "parent_id": "t3_653idk", "subreddit_name_prefixed": "r/math", "controversiality": 0, "body": "The typical example is card shuffling. I will use this to (partially) answer some of your questions.\n\n1. Apparently there was once a card shuffling machine used in Vegas that was not actually very good at shuffling cards. This was demonstrated by Persi Diaoconis and someone else who I forget. I think this counts as a big deal in that particular application.\n\n2. You want to choose a method of shuffle that is guaranteed to mix the cards (this is usually easy) and furthermore, it should do this in as few shuffles as possible. Showing that this last part holds is the challenge.\n\n3. I will interpret your question as \"how do we measure how well mixed the cards are?\"\n\nthen the answer is \"total variation distance\". \n\nInstead of writing down the definition, let me give an example. Suppose we want to show that the card shuffle procedure works well to mix our cards. Let X be the deck that is shuffled according to our procedure. If it was truly well shuffled, the probability of the top card being an ace is 1/13. The probability of the top card being a heart is 1/4. The probability of the top and bottom cards being aces is (1/13)*(12/51).\n\nThere are lots of different probabilities we can compute.\n\n\nHowever, if our deck is shuffled using our procedure, maybe the probability of the top card being an ace is not 1/13 exactly. For example if you do a riffle shuffle on a sorted deck with the ace originally on top, the probability that the ace is on top is probably close to 0.5.\n\nBut lets say we do 10 riffle shuffles. Then perhaps  the probability is very close, say 1/13+0.001. If this is the case, and if for every other probability you can think of (like the ones I listed above), the probability is within 0.001 of the correct probability, then by definition we say that the distance between our shuffling procedure and the stationary distribution is less than 0.001.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The typical example is card shuffling. I will use this to (partially) answer some of your questions.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Apparently there was once a card shuffling machine used in Vegas that was not actually very good at shuffling cards. This was demonstrated by Persi Diaoconis and someone else who I forget. I think this counts as a big deal in that particular application.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;You want to choose a method of shuffle that is guaranteed to mix the cards (this is usually easy) and furthermore, it should do this in as few shuffles as possible. Showing that this last part holds is the challenge.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I will interpret your question as &amp;quot;how do we measure how well mixed the cards are?&amp;quot;&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;then the answer is &amp;quot;total variation distance&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;Instead of writing down the definition, let me give an example. Suppose we want to show that the card shuffle procedure works well to mix our cards. Let X be the deck that is shuffled according to our procedure. If it was truly well shuffled, the probability of the top card being an ace is 1/13. The probability of the top card being a heart is 1/4. The probability of the top and bottom cards being aces is (1/13)*(12/51).&lt;/p&gt;\n\n&lt;p&gt;There are lots of different probabilities we can compute.&lt;/p&gt;\n\n&lt;p&gt;However, if our deck is shuffled using our procedure, maybe the probability of the top card being an ace is not 1/13 exactly. For example if you do a riffle shuffle on a sorted deck with the ace originally on top, the probability that the ace is on top is probably close to 0.5.&lt;/p&gt;\n\n&lt;p&gt;But lets say we do 10 riffle shuffles. Then perhaps  the probability is very close, say 1/13+0.001. If this is the case, and if for every other probability you can think of (like the ones I listed above), the probability is within 0.001 of the correct probability, then by definition we say that the distance between our shuffling procedure and the stationary distribution is less than 0.001.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "math", "name": "t1_dg78tnx", "score_hidden": false, "stickied": false, "created": 1492093748.0, "created_utc": 1492064948.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qh0n", "removal_reason": null, "link_id": "t3_653idk", "likes": null, "replies": "", "user_reports": [], "id": "dg7zimp", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Oedipustrexeliot", "parent_id": "t3_653idk", "subreddit_name_prefixed": "r/math", "controversiality": 0, "body": "It basically makes Bayesian inference possible in situations where you don't have something really neat and tidy like a conjugate prior.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It basically makes Bayesian inference possible in situations where you don&amp;#39;t have something really neat and tidy like a conjugate prior.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "math", "name": "t1_dg7zimp", "score_hidden": false, "stickied": false, "created": 1492136851.0, "created_utc": 1492108051.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}]