[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "Foodforthought", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": null, "id": "65v0kw", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 237, "report_reasons": null, "author": "speckz", "saved": false, "mod_reports": [], "name": "t3_65v0kw", "subreddit_name_prefixed": "r/Foodforthought", "approved_by": null, "over_18": false, "domain": "theguardian.com", "hidden": false, "thumbnail": "", "subreddit_id": "t5_2rete", "edited": false, "link_flair_css_class": null, "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/Foodforthought/comments/65v0kw/ai_programs_exhibit_racial_and_gender_biases/", "num_reports": null, "locked": false, "stickied": false, "created": 1492457778.0, "url": "https://www.theguardian.com/technology/2017/apr/13/ai-programs-exhibit-racist-and-sexist-biases-research-reveals", "author_flair_text": null, "quarantine": false, "title": "AI programs exhibit racial and gender biases, research reveals", "created_utc": 1492428978.0, "distinguished": null, "media": null, "upvote_ratio": 0.89, "num_comments": 64, "visited": false, "subreddit_type": "public", "ups": 237}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdsv2n", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "Bartweiss", "parent_id": "t1_dgdpj3w", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; Maybe this is the mirror that we need to look into as humans to grasp how shitty we are to each other, or how careless and thoughtless our racism and other prejudices really are.\n\nThis is pretty much why I try to point out that these \"biased programs\" are accurately learning their inputs. FICO score predictors are 'biased', but FICO scores themselves are notoriously opaque and biased. De-biasing algorithms will undoubtedly have uses, but if we're really lucky we might see algorithms used to understand and combat the underlying problems.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Maybe this is the mirror that we need to look into as humans to grasp how shitty we are to each other, or how careless and thoughtless our racism and other prejudices really are.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is pretty much why I try to point out that these &amp;quot;biased programs&amp;quot; are accurately learning their inputs. FICO score predictors are &amp;#39;biased&amp;#39;, but FICO scores themselves are notoriously opaque and biased. De-biasing algorithms will undoubtedly have uses, but if we&amp;#39;re really lucky we might see algorithms used to understand and combat the underlying problems.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdsv2n", "score_hidden": false, "stickied": false, "created": 1492478978.0, "created_utc": 1492450178.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdpj3w", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "Jackmack65", "parent_id": "t1_dgdo68h", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt;only one Tech Review article caught the point that \"biased algorithms\" are sometimes what you want to reflect real world bias.\n\nThat struck me as the real food for thought here. Maybe this is the mirror that we need to look into as humans to grasp how shitty we are to each other, or how careless and thoughtless our racism and other prejudices really are. If machines learn to be assholes and they're learning from us, my hope is that we in turn might learn that we're really the assholes and ought to change our thinking. \n\nVery little remains of what optimism I once had. I'll try to hang on to this dim little spark for the rest of the day, anyway. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;only one Tech Review article caught the point that &amp;quot;biased algorithms&amp;quot; are sometimes what you want to reflect real world bias.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;That struck me as the real food for thought here. Maybe this is the mirror that we need to look into as humans to grasp how shitty we are to each other, or how careless and thoughtless our racism and other prejudices really are. If machines learn to be assholes and they&amp;#39;re learning from us, my hope is that we in turn might learn that we&amp;#39;re really the assholes and ought to change our thinking. &lt;/p&gt;\n\n&lt;p&gt;Very little remains of what optimism I once had. I&amp;#39;ll try to hang on to this dim little spark for the rest of the day, anyway. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdpj3w", "score_hidden": false, "stickied": false, "created": 1492475273.0, "created_utc": 1492446473.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 7}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdo68h", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "Bartweiss", "parent_id": "t1_dgdn11e", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "This article is pretty good about it, I've just seen ~5 of these come through /truereddit and /foodforthought recently. I was actually noting that this one is better than most, although only one Tech Review article caught the point that \"biased algorithms\" are sometimes what you want to reflect real world bias.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This article is pretty good about it, I&amp;#39;ve just seen ~5 of these come through /truereddit and /foodforthought recently. I was actually noting that this one is better than most, although only one Tech Review article caught the point that &amp;quot;biased algorithms&amp;quot; are sometimes what you want to reflect real world bias.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdo68h", "score_hidden": false, "stickied": false, "created": 1492473743.0, "created_utc": 1492444943.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 4}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdn11e", "gilded": 0, "archived": false, "score": 15, "report_reasons": null, "author": "Jackmack65", "parent_id": "t1_dgdlwqb", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Interesting. I found the article pretty clear in describing that the algorithm is picking up bias in human language. I didn't impute at all that the article described the bias arising in the algorithm. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting. I found the article pretty clear in describing that the algorithm is picking up bias in human language. I didn&amp;#39;t impute at all that the article described the bias arising in the algorithm. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdn11e", "score_hidden": false, "stickied": false, "created": 1492472432.0, "created_utc": 1492443632.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 15}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdslgc", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Bartweiss", "parent_id": "t1_dgdq43e", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Yeah! Let's see...\n\n[This article](https://www.technologyreview.com/s/602950/how-to-fix-silicon-valleys-sexist-algorithms/) was the one with interesting quotes about bias-prediction as a 'feature'. It also links [this](https://research.googleblog.com/2016/10/equality-of-opportunity-in-machine.html) Google post about [this](https://arxiv.org/abs/1610.02413) paper.\n\nIt looks like the strategy there is to create a \"gender neutrality\" feature in the utility function to discourage learning bias. I'm a bit concerned that this will only catch specific biases you anticipate, but I'm not sure there's a good way around that when your data is biased.\n\nI don't have a link handy for the data-changing thing - I'm not sure I've seen a bias-specific example, but I've seen it as a technique elsewhere.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah! Let&amp;#39;s see...&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.technologyreview.com/s/602950/how-to-fix-silicon-valleys-sexist-algorithms/\"&gt;This article&lt;/a&gt; was the one with interesting quotes about bias-prediction as a &amp;#39;feature&amp;#39;. It also links &lt;a href=\"https://research.googleblog.com/2016/10/equality-of-opportunity-in-machine.html\"&gt;this&lt;/a&gt; Google post about &lt;a href=\"https://arxiv.org/abs/1610.02413\"&gt;this&lt;/a&gt; paper.&lt;/p&gt;\n\n&lt;p&gt;It looks like the strategy there is to create a &amp;quot;gender neutrality&amp;quot; feature in the utility function to discourage learning bias. I&amp;#39;m a bit concerned that this will only catch specific biases you anticipate, but I&amp;#39;m not sure there&amp;#39;s a good way around that when your data is biased.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have a link handy for the data-changing thing - I&amp;#39;m not sure I&amp;#39;ve seen a bias-specific example, but I&amp;#39;ve seen it as a technique elsewhere.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdslgc", "score_hidden": false, "stickied": false, "created": 1492478678.0, "created_utc": 1492449878.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdq43e", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "okiyama", "parent_id": "t1_dgdob90", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "That sounds really interesting! Do you have anything in particular I could look into?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That sounds really interesting! Do you have anything in particular I could look into?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdq43e", "score_hidden": false, "stickied": false, "created": 1492475920.0, "created_utc": 1492447120.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdob90", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "Bartweiss", "parent_id": "t1_dgdlwqb", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; Well the question is then, how do you get a dataset of human language which does not contain biases?\n\nFrankly you don't. Not on large natural-language datasets. You either restrict yourself to things like recipes, or you find another way to deal.\n\nA lot of the articles, though not really this one, seem to think bias is something the designers can just \"turn off\". Obviously you can't, but there are some interesting whitepapers on how to fight bias in learning results - tricks like gender-swapping pronouns and names in contexts where they don't matter to create reverse-biased data and help even out.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Well the question is then, how do you get a dataset of human language which does not contain biases?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Frankly you don&amp;#39;t. Not on large natural-language datasets. You either restrict yourself to things like recipes, or you find another way to deal.&lt;/p&gt;\n\n&lt;p&gt;A lot of the articles, though not really this one, seem to think bias is something the designers can just &amp;quot;turn off&amp;quot;. Obviously you can&amp;#39;t, but there are some interesting whitepapers on how to fight bias in learning results - tricks like gender-swapping pronouns and names in contexts where they don&amp;#39;t matter to create reverse-biased data and help even out.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdob90", "score_hidden": false, "stickied": false, "created": 1492473899.0, "created_utc": 1492445099.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 7}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdp7nd", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "UncleMeat11", "parent_id": "t1_dgdlwqb", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Getting a debiased training set is impossible. But there is active research on debiasing models that is promising.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Getting a debiased training set is impossible. But there is active research on debiasing models that is promising.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdp7nd", "score_hidden": false, "stickied": false, "created": 1492474917.0, "created_utc": 1492446117.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dge5msy", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "bitparity", "parent_id": "t1_dgdlwqb", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "It's impossible, because bias is a relational definition.  It's bias if it doesn't help you, it makes sense if it does.\n\nA good analogy would be the line between game AI that's terrible, AI that's good vs. AI that's cheating.  It can be the exact same AI algorithm, but with three different evaluations by three different levels/experiences of players.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s impossible, because bias is a relational definition.  It&amp;#39;s bias if it doesn&amp;#39;t help you, it makes sense if it does.&lt;/p&gt;\n\n&lt;p&gt;A good analogy would be the line between game AI that&amp;#39;s terrible, AI that&amp;#39;s good vs. AI that&amp;#39;s cheating.  It can be the exact same AI algorithm, but with three different evaluations by three different levels/experiences of players.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge5msy", "score_hidden": false, "stickied": false, "created": 1492493400.0, "created_utc": 1492464600.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdlwqb", "gilded": 0, "archived": false, "score": 12, "report_reasons": null, "author": "okiyama", "parent_id": "t1_dgdinuq", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Well the question is then, how do you get a dataset of human language which does not contain biases? I'm not sure one exists that would be sufficiently large to train a machine learning algorithm. \n\nThe findings are that en masse, the way people use language is biased. This article cocked it up by implying it's the algorithm with the bias rather than the test set but it's still quite an interesting discovery. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well the question is then, how do you get a dataset of human language which does not contain biases? I&amp;#39;m not sure one exists that would be sufficiently large to train a machine learning algorithm. &lt;/p&gt;\n\n&lt;p&gt;The findings are that en masse, the way people use language is biased. This article cocked it up by implying it&amp;#39;s the algorithm with the bias rather than the test set but it&amp;#39;s still quite an interesting discovery. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdlwqb", "score_hidden": false, "stickied": false, "created": 1492471127.0, "created_utc": 1492442327.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 12}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgegwq4", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "bollvirtuoso", "parent_id": "t1_dgdinuq", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "I don't think that's what the story said. In fact, I think it said the exact opposite:\n\n&gt; However, as machines are getting closer to acquiring human-like language abilities, they are also absorbing the deeply ingrained biases **concealed within the patterns of language use**, the latest research reveals.\n\n&gt;Joanna Bryson, a computer scientist at the University of Bath and a co-author, said: \u201cA lot of people are saying this is showing that AI is prejudiced. No. **This is showing we\u2019re prejudiced and that AI is learning it.**\u201d (emphasis supplied).", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think that&amp;#39;s what the story said. In fact, I think it said the exact opposite:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;However, as machines are getting closer to acquiring human-like language abilities, they are also absorbing the deeply ingrained biases &lt;strong&gt;concealed within the patterns of language use&lt;/strong&gt;, the latest research reveals.&lt;/p&gt;\n\n&lt;p&gt;Joanna Bryson, a computer scientist at the University of Bath and a co-author, said: \u201cA lot of people are saying this is showing that AI is prejudiced. No. &lt;strong&gt;This is showing we\u2019re prejudiced and that AI is learning it.&lt;/strong&gt;\u201d (emphasis supplied).&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgegwq4", "score_hidden": false, "stickied": false, "created": 1492507723.0, "created_utc": 1492478923.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgemg7h", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Coolfuckingname", "parent_id": "t1_dgdinuq", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; stripping out the bias isn't always a goal, \n\nWhat do you mean by this?\n\nGenuinely curious.\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;stripping out the bias isn&amp;#39;t always a goal, &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;What do you mean by this?&lt;/p&gt;\n\n&lt;p&gt;Genuinely curious.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgemg7h", "score_hidden": true, "stickied": false, "created": 1492514509.0, "created_utc": 1492485709.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdinuq", "gilded": 0, "archived": false, "score": 97, "report_reasons": null, "author": "Bartweiss", "parent_id": "t1_dgdia1w", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "It's frustrating how often these stories imply that the bias somehow arises at the machine learning layer, instead of being retained from the data set.\n\nI've only seen one article make the good point that stripping out the bias isn't always a *goal*, even. If you want to do sentiment analysis on Twitter posts, for instance, your algorithm had better recognize biases to properly work with that content - understanding the bias is part of your intent.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s frustrating how often these stories imply that the bias somehow arises at the machine learning layer, instead of being retained from the data set.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only seen one article make the good point that stripping out the bias isn&amp;#39;t always a &lt;em&gt;goal&lt;/em&gt;, even. If you want to do sentiment analysis on Twitter posts, for instance, your algorithm had better recognize biases to properly work with that content - understanding the bias is part of your intent.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdinuq", "score_hidden": false, "stickied": false, "created": 1492466914.0, "created_utc": 1492438114.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 97}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdsiwn", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "MrOaiki", "parent_id": "t1_dgdia1w", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "How do you differ between bias in the data and actual relevant data? I mean, if 95% of women are less efficient at moving pianos than men are, is it because women aren't allowed to move pianos (bias) or because women aren't able to move pianos?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How do you differ between bias in the data and actual relevant data? I mean, if 95% of women are less efficient at moving pianos than men are, is it because women aren&amp;#39;t allowed to move pianos (bias) or because women aren&amp;#39;t able to move pianos?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdsiwn", "score_hidden": false, "stickied": false, "created": 1492478599.0, "created_utc": 1492449799.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 7}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dge7xsw", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Singulaire", "parent_id": "t1_dgdpoxj", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "An attitude reflected in the training data may or may not be based in truth. It all depends on how well the distribution of your training data matches the distribution of the real world data that you want to use the algorithm to predict on.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;An attitude reflected in the training data may or may not be based in truth. It all depends on how well the distribution of your training data matches the distribution of the real world data that you want to use the algorithm to predict on.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge7xsw", "score_hidden": false, "stickied": false, "created": 1492496267.0, "created_utc": 1492467467.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdpoxj", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "dougbdl", "parent_id": "t1_dgdia1w", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 1, "body": "Or maybe not. You assume that 'data reflects a certain attitude', where it just may be the data, right? Could it be that the data reflects a reality that some don't want to accept? Please don't get all racist name calling with me, just throwing it out there. I know race is the 3rd rail and just asking questions gets people name called, but I will give it a shot anyway.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Or maybe not. You assume that &amp;#39;data reflects a certain attitude&amp;#39;, where it just may be the data, right? Could it be that the data reflects a reality that some don&amp;#39;t want to accept? Please don&amp;#39;t get all racist name calling with me, just throwing it out there. I know race is the 3rd rail and just asking questions gets people name called, but I will give it a shot anyway.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdpoxj", "score_hidden": false, "stickied": false, "created": 1492475453.0, "created_utc": 1492446653.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdia1w", "gilded": 0, "archived": false, "score": 135, "report_reasons": null, "author": "Singulaire", "parent_id": "t3_65v0kw", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "In other words, the machine learning algorithms learn the statistical relationships present in the data. If the training data reflects a certain attitude, the algorithm will learn to mimic this attitude.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In other words, the machine learning algorithms learn the statistical relationships present in the data. If the training data reflects a certain attitude, the algorithm will learn to mimic this attitude.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdia1w", "score_hidden": false, "stickied": false, "created": 1492466362.0, "created_utc": 1492437562.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 135}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dge3u3o", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "ineedmoresleep", "parent_id": "t1_dgdseur", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; Suppose I learn a linear regressor to predict job performance. My prediction is then just a weighted combination of features. If a name is a positive predictor for performance, then its associated weight will be positive. \n\nname (which could vary widely, so you want to classify names using clusters anyway) is just one of the (multitude) of variables along with past zip codes, schools, subjects and school grades, previous places of work, driving record, and so on. and you don't run a linear regressor, you do some sort of pca-based clustering algorithm, and only then a maybe logistic-based yes/no hiring recommendation. in that case, changing a name is not going to affect the outcomes much, because it will be just one of a multitude of variables. \n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Suppose I learn a linear regressor to predict job performance. My prediction is then just a weighted combination of features. If a name is a positive predictor for performance, then its associated weight will be positive. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;name (which could vary widely, so you want to classify names using clusters anyway) is just one of the (multitude) of variables along with past zip codes, schools, subjects and school grades, previous places of work, driving record, and so on. and you don&amp;#39;t run a linear regressor, you do some sort of pca-based clustering algorithm, and only then a maybe logistic-based yes/no hiring recommendation. in that case, changing a name is not going to affect the outcomes much, because it will be just one of a multitude of variables. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge3u3o", "score_hidden": false, "stickied": false, "created": 1492491272.0, "created_utc": 1492462472.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdseur", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "hbweb500", "parent_id": "t1_dgdq8td", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; So, changing a name will not accomplish anything where ML is concerned\n\nHow do you see this? Suppose I learn a linear regressor to predict job performance. My prediction is then just a weighted combination of features. If a name is a positive predictor for performance, then its associated weight will be positive. If I give the regressor two applicants which differ only in name, then the one with the predictive name will have a strictly higher score. So changing names absolutely does change the prediction.\n\n&gt; make it more adaptive, adjust time frames, look for trends in data\n\nBut looking \"for trends in data\" is an *unsupervised* task in the sense that we don't have labeled instances of cases which constitute of bias or prejudice -- even if we did, they are labeled by humans who have their own biases when it comes to (funnily enough) identifying cases of prejudice. If we don't have labels, unsupervised learning relies heavily on either improving the performance of some other task (like classification) or presenting the user with a pattern which they find interesting. In both cases, we are back to square one: human bias is a part of the learning process.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;So, changing a name will not accomplish anything where ML is concerned&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;How do you see this? Suppose I learn a linear regressor to predict job performance. My prediction is then just a weighted combination of features. If a name is a positive predictor for performance, then its associated weight will be positive. If I give the regressor two applicants which differ only in name, then the one with the predictive name will have a strictly higher score. So changing names absolutely does change the prediction.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;make it more adaptive, adjust time frames, look for trends in data&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;But looking &amp;quot;for trends in data&amp;quot; is an &lt;em&gt;unsupervised&lt;/em&gt; task in the sense that we don&amp;#39;t have labeled instances of cases which constitute of bias or prejudice -- even if we did, they are labeled by humans who have their own biases when it comes to (funnily enough) identifying cases of prejudice. If we don&amp;#39;t have labels, unsupervised learning relies heavily on either improving the performance of some other task (like classification) or presenting the user with a pattern which they find interesting. In both cases, we are back to square one: human bias is a part of the learning process.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdseur", "score_hidden": false, "stickied": false, "created": 1492478473.0, "created_utc": 1492449673.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdq8td", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "ineedmoresleep", "parent_id": "t1_dgdppq9", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; names are (historically) predictive of better performance\n\nI was using names as a discussion point - I am sure that there other other proxy variables that end up doing the same thing. So, changing a name will not accomplish anything where ML is concerned. \n\nas for names (or whatever proxy) being historically predictive - well, make it more adaptive, adjust time frames, look for trends in data.  ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;names are (historically) predictive of better performance&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I was using names as a discussion point - I am sure that there other other proxy variables that end up doing the same thing. So, changing a name will not accomplish anything where ML is concerned. &lt;/p&gt;\n\n&lt;p&gt;as for names (or whatever proxy) being historically predictive - well, make it more adaptive, adjust time frames, look for trends in data.  &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdq8td", "score_hidden": false, "stickied": false, "created": 1492476067.0, "created_utc": 1492447267.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgels02", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "gmano", "parent_id": "t1_dgdppq9", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt;I think you'd agree that changing your name does not make you a better performer.\n\nI'd contest that. Ask any Asian immigrant if changing their name to something familiar and pronounceable makes their job easier. Chances are they will say it has. \n\nPeople are naturally inclined to like you better if they have an easier time pronouncing and remembering your name, and easier recall and pronunciation makes for better communication, which makes for better efficiency in general. These are just facts.\n\nThe problem, IMO, isn't that non-white names are preferred, it's that complex or hard to remember names are disadvantaged.\n\nI would love to see a study that compares an African names like Zane, Asha, or Elea to ones like Andaiye, Haoniyao, or Keambiroiro and difficult white names like Niamh, Trygve, or Anastagio.", "edited": 1492485339.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I think you&amp;#39;d agree that changing your name does not make you a better performer.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I&amp;#39;d contest that. Ask any Asian immigrant if changing their name to something familiar and pronounceable makes their job easier. Chances are they will say it has. &lt;/p&gt;\n\n&lt;p&gt;People are naturally inclined to like you better if they have an easier time pronouncing and remembering your name, and easier recall and pronunciation makes for better communication, which makes for better efficiency in general. These are just facts.&lt;/p&gt;\n\n&lt;p&gt;The problem, IMO, isn&amp;#39;t that non-white names are preferred, it&amp;#39;s that complex or hard to remember names are disadvantaged.&lt;/p&gt;\n\n&lt;p&gt;I would love to see a study that compares an African names like Zane, Asha, or Elea to ones like Andaiye, Haoniyao, or Keambiroiro and difficult white names like Niamh, Trygve, or Anastagio.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgels02", "score_hidden": true, "stickied": false, "created": 1492513638.0, "created_utc": 1492484838.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdppq9", "gilded": 0, "archived": false, "score": 13, "report_reasons": null, "author": "hbweb500", "parent_id": "t1_dgdoy77", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; Do we have studies that show that European-sounding names predict better (or worse) performance on the job?\n\nIt may even be that European-sounding names *are* (historically) predictive of better performance! But I think you'd agree that changing your name does not make you a better performer. So as a society we have (mostly) come to agree that your name should not be used to predict the performance of *you*, the *individual* even though it may have some statistically-significant predictive power when applied to large groups.\n\nAnd then you have to ask why it is the case that a European-sounding name is predictive. Is there a self-fulling cycle: such names *sound* like predictors of success, so more opportunities are given to people having these names, leading to more instances of success and thus reinforcement of the bias? So even if a name is predictive, it is only predictive with respect to the historical data we have -- which is almost certainly biased.\n\n&gt; the solution is more data, and not the kind that is influenced by human prejudices.\n\nGood luck with that. We can try our best to obtain data that does not reflect some human bias, but I think this is a very difficult problem.", "edited": 1492446873.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Do we have studies that show that European-sounding names predict better (or worse) performance on the job?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It may even be that European-sounding names &lt;em&gt;are&lt;/em&gt; (historically) predictive of better performance! But I think you&amp;#39;d agree that changing your name does not make you a better performer. So as a society we have (mostly) come to agree that your name should not be used to predict the performance of &lt;em&gt;you&lt;/em&gt;, the &lt;em&gt;individual&lt;/em&gt; even though it may have some statistically-significant predictive power when applied to large groups.&lt;/p&gt;\n\n&lt;p&gt;And then you have to ask why it is the case that a European-sounding name is predictive. Is there a self-fulling cycle: such names &lt;em&gt;sound&lt;/em&gt; like predictors of success, so more opportunities are given to people having these names, leading to more instances of success and thus reinforcement of the bias? So even if a name is predictive, it is only predictive with respect to the historical data we have -- which is almost certainly biased.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;the solution is more data, and not the kind that is influenced by human prejudices.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Good luck with that. We can try our best to obtain data that does not reflect some human bias, but I think this is a very difficult problem.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdppq9", "score_hidden": false, "stickied": false, "created": 1492475477.0, "created_utc": 1492446677.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 13}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dge06yj", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "bluskale", "parent_id": "t1_dgdpzgf", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Well consider this then: you are developing the Justicator\u2122 sentencing algorithm that will decide how severely convicted criminals will be sentenced for drug-related offenses. Say you base it on the body of historical sentences &amp; feed it all the relevant data. Do you get a fair sentencing system?\n\nNo, you do not. You've simply perpetuated the status quo, which consistently punishes, for the same crimes, convicts of color more than whites.\n\nSo to get back to your point: yes, past experiences must be incorporated. However, this data also needs to be carefully examined and curated to avoid confounding/unwanted biases within it.\n\nThus, if you want a totally fair hiring system, lots of racial, economic, &amp; cultural biases would need removal. If you want s hiring system that reflects the status quo, then you basically don't correct for anything.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well consider this then: you are developing the Justicator\u2122 sentencing algorithm that will decide how severely convicted criminals will be sentenced for drug-related offenses. Say you base it on the body of historical sentences &amp;amp; feed it all the relevant data. Do you get a fair sentencing system?&lt;/p&gt;\n\n&lt;p&gt;No, you do not. You&amp;#39;ve simply perpetuated the status quo, which consistently punishes, for the same crimes, convicts of color more than whites.&lt;/p&gt;\n\n&lt;p&gt;So to get back to your point: yes, past experiences must be incorporated. However, this data also needs to be carefully examined and curated to avoid confounding/unwanted biases within it.&lt;/p&gt;\n\n&lt;p&gt;Thus, if you want a totally fair hiring system, lots of racial, economic, &amp;amp; cultural biases would need removal. If you want s hiring system that reflects the status quo, then you basically don&amp;#39;t correct for anything.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge06yj", "score_hidden": false, "stickied": false, "created": 1492487199.0, "created_utc": 1492458399.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 6}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdpzgf", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "ineedmoresleep", "parent_id": "t1_dgdpdjs", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 1, "body": "&gt;Past poor experience with black people shouldn't reflect negatively on qualified black applicants who just happen to share a skin color. \n\nThis can be legislated, on the same level as affirmative action is being implemented, but not on the ML level - or else you are fighting bias with bias. It will be a mess. \n\nPast experience should absolutely be included in training data sets. That's essentially the point of ML. \n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Past poor experience with black people shouldn&amp;#39;t reflect negatively on qualified black applicants who just happen to share a skin color. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This can be legislated, on the same level as affirmative action is being implemented, but not on the ML level - or else you are fighting bias with bias. It will be a mess. &lt;/p&gt;\n\n&lt;p&gt;Past experience should absolutely be included in training data sets. That&amp;#39;s essentially the point of ML. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdpzgf", "score_hidden": false, "stickied": false, "created": 1492475778.0, "created_utc": 1492446978.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdpdjs", "gilded": 0, "archived": false, "score": 18, "report_reasons": null, "author": "UncleMeat11", "parent_id": "t1_dgdoy77", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "The resumes are identical. Past poor experience with black people shouldn't reflect negatively on qualified black applicants who just happen to share a skin color. \n\nI find your digs at social scientists here baffling. ML bias is an active area of research among CS academics, not just sociologists. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The resumes are identical. Past poor experience with black people shouldn&amp;#39;t reflect negatively on qualified black applicants who just happen to share a skin color. &lt;/p&gt;\n\n&lt;p&gt;I find your digs at social scientists here baffling. ML bias is an active area of research among CS academics, not just sociologists. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdpdjs", "score_hidden": false, "stickied": false, "created": 1492475102.0, "created_utc": 1492446302.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 18}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdoy77", "gilded": 0, "archived": false, "score": -11, "report_reasons": null, "author": "ineedmoresleep", "parent_id": "t1_dgdo6re", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt;Studies have shown that humans have a bias towards European-sounding names as opposed to African-American names, even when the r\u00e9sum\u00e9s are identical. \n\nThese (human) decisions are also made on past experience of performance on the job of people with certain names. Do we have studies that show that European-sounding names predict better (or worse) performance on the job? I would guess that no social science researcher would ever go there since this question is politically inconvenient, but the data is out there. So go ahead and include that data in the training set. \n \n&gt; If we collect a bunch of data from humans \n\nthe solution is more data, and not the kind that is influenced by human prejudices. \n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Studies have shown that humans have a bias towards European-sounding names as opposed to African-American names, even when the r\u00e9sum\u00e9s are identical. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;These (human) decisions are also made on past experience of performance on the job of people with certain names. Do we have studies that show that European-sounding names predict better (or worse) performance on the job? I would guess that no social science researcher would ever go there since this question is politically inconvenient, but the data is out there. So go ahead and include that data in the training set. &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;If we collect a bunch of data from humans &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;the solution is more data, and not the kind that is influenced by human prejudices. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdoy77", "score_hidden": false, "stickied": false, "created": 1492474622.0, "created_utc": 1492445822.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": -11}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdo6re", "gilded": 0, "archived": false, "score": 19, "report_reasons": null, "author": "hbweb500", "parent_id": "t1_dgdjo53", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "I feel that your comment slightly misses the point of the article, which is that the training data itself contains biases that the machine learning algorithm faithfully detects and incorporates in its predictions. Whether this is OK or not depends on the context and is up to us humans to decide.\n\nThe article gives a good example of this in training a classifier to predict whether a given r\u00e9sum\u00e9 should be accepted or denied. Studies have shown that humans have a bias towards European-sounding names as opposed to African-American names, even when the r\u00e9sum\u00e9s are identical. If we collect a bunch of data from humans and train a predictive model, the machine is only going to learn the prejudices that we have taught it.\n\nThe solution isn't to give the machine more data -- it's just going to be biased, too. And it is often unclear how to make the data more \"accurate\". One solution is to inject bias into the learning process: For example, we explicitly ban the machine from using features -- like name or race -- to make predictions. But there may be correlations between these \"forbidden\" features and other features which are hard to detect or separate from the data. All in all, it is a difficult problem, both philosophically and scientifically.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I feel that your comment slightly misses the point of the article, which is that the training data itself contains biases that the machine learning algorithm faithfully detects and incorporates in its predictions. Whether this is OK or not depends on the context and is up to us humans to decide.&lt;/p&gt;\n\n&lt;p&gt;The article gives a good example of this in training a classifier to predict whether a given r\u00e9sum\u00e9 should be accepted or denied. Studies have shown that humans have a bias towards European-sounding names as opposed to African-American names, even when the r\u00e9sum\u00e9s are identical. If we collect a bunch of data from humans and train a predictive model, the machine is only going to learn the prejudices that we have taught it.&lt;/p&gt;\n\n&lt;p&gt;The solution isn&amp;#39;t to give the machine more data -- it&amp;#39;s just going to be biased, too. And it is often unclear how to make the data more &amp;quot;accurate&amp;quot;. One solution is to inject bias into the learning process: For example, we explicitly ban the machine from using features -- like name or race -- to make predictions. But there may be correlations between these &amp;quot;forbidden&amp;quot; features and other features which are hard to detect or separate from the data. All in all, it is a difficult problem, both philosophically and scientifically.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdo6re", "score_hidden": false, "stickied": false, "created": 1492473760.0, "created_utc": 1492444960.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 19}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdq2rq", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "okiyama", "parent_id": "t1_dgdpb76", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt;But those \"societal biases\" are objectively present in reality, right? \n\nThis is the leap I think people would disagree with. Language is not a perfect mirror of reality. For example, the same dataset would probably more strongly associate God with existing than not existing, since most people on earth and therefore most language usage has that inherent bias.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;But those &amp;quot;societal biases&amp;quot; are objectively present in reality, right? &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is the leap I think people would disagree with. Language is not a perfect mirror of reality. For example, the same dataset would probably more strongly associate God with existing than not existing, since most people on earth and therefore most language usage has that inherent bias.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdq2rq", "score_hidden": false, "stickied": false, "created": 1492475877.0, "created_utc": 1492447077.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdpb76", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "ineedmoresleep", "parent_id": "t1_dgdlzof", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt;What is \"accurate\" data when it comes to language usage? \n\nas complete as possible, I would imagine. encompassing all social groups and all forms of usage.  \n\n&gt;societal biases are present in the usage of language itself.\n\nBut those \"societal biases\" are objectively present in reality, right? And AI accurately picks up on them? So are we blaming the mirror for presenting an accurate picture?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;What is &amp;quot;accurate&amp;quot; data when it comes to language usage? &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;as complete as possible, I would imagine. encompassing all social groups and all forms of usage.  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;societal biases are present in the usage of language itself.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;But those &amp;quot;societal biases&amp;quot; are objectively present in reality, right? And AI accurately picks up on them? So are we blaming the mirror for presenting an accurate picture?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdpb76", "score_hidden": false, "stickied": false, "created": 1492475030.0, "created_utc": 1492446230.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdlzof", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "okiyama", "parent_id": "t1_dgdjo53", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "What is \"accurate\" data when it comes to language usage? I think the interesting thing is that when you take a very large sample of English language usage, societal biases are present in the usage of language itself. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What is &amp;quot;accurate&amp;quot; data when it comes to language usage? I think the interesting thing is that when you take a very large sample of English language usage, societal biases are present in the usage of language itself. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdlzof", "score_hidden": false, "stickied": false, "created": 1492471225.0, "created_utc": 1492442425.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdoutk", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "Uncle_Erik", "parent_id": "t1_dgdjo53", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "I agree.  What if accurate data returns an unpleasant result?  What if that result is something socially unacceptable?\n\nAre we going to program machines to lie and be politically correct?\n\nShould AI be used to push a social agenda?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I agree.  What if accurate data returns an unpleasant result?  What if that result is something socially unacceptable?&lt;/p&gt;\n\n&lt;p&gt;Are we going to program machines to lie and be politically correct?&lt;/p&gt;\n\n&lt;p&gt;Should AI be used to push a social agenda?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdoutk", "score_hidden": false, "stickied": false, "created": 1492474520.0, "created_utc": 1492445720.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dge70s5", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "RobToastie", "parent_id": "t1_dge62lz", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Generally speaking yes, as long as you don't try to fit all of the data as closely as possible (which can do all sorts of terrible things). Some algorithms are more resilient to this than others. PCA, as you mentioned, happens to do this by effectively reducing the dimensions of the data based on high level, generic analysis of all the data (and effectively throws out most of the data by doing this as a preprocess phase for other ML algorithms). Essentially it (in this context) sorts out what data is interesting or useful for classification.\n\nThere is a *ton* of human error that is possible when utilizing these algorithms, to the point that it doesn't even matter how well they are designed if someone uses them improperly. If you don't properly vet your data sources you can be in trouble. If you forget to use a validation set your classifier can be awful. If you overtrain a neural-net it can fail to generalize. If you use the wrong algorithm for the wrong problem you get bad results. If you optimize for precision when you really needed high recall you can fail.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Generally speaking yes, as long as you don&amp;#39;t try to fit all of the data as closely as possible (which can do all sorts of terrible things). Some algorithms are more resilient to this than others. PCA, as you mentioned, happens to do this by effectively reducing the dimensions of the data based on high level, generic analysis of all the data (and effectively throws out most of the data by doing this as a preprocess phase for other ML algorithms). Essentially it (in this context) sorts out what data is interesting or useful for classification.&lt;/p&gt;\n\n&lt;p&gt;There is a &lt;em&gt;ton&lt;/em&gt; of human error that is possible when utilizing these algorithms, to the point that it doesn&amp;#39;t even matter how well they are designed if someone uses them improperly. If you don&amp;#39;t properly vet your data sources you can be in trouble. If you forget to use a validation set your classifier can be awful. If you overtrain a neural-net it can fail to generalize. If you use the wrong algorithm for the wrong problem you get bad results. If you optimize for precision when you really needed high recall you can fail.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge70s5", "score_hidden": false, "stickied": false, "created": 1492495120.0, "created_utc": 1492466320.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dge62lz", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "ineedmoresleep", "parent_id": "t1_dge5dvu", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt;if not used properly \n\nno, we are talking about well-designed algorithms here. all that pca-based clustering, for example. the consensus is (as far as I know) the more data, the better. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;if not used properly &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;no, we are talking about well-designed algorithms here. all that pca-based clustering, for example. the consensus is (as far as I know) the more data, the better. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge62lz", "score_hidden": false, "stickied": false, "created": 1492493936.0, "created_utc": 1492465136.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dge5dvu", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "RobToastie", "parent_id": "t1_dge42qf", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Pretty much any ML algorithm can overfit if not used properly. Basically you end up with a classifier that can predict the training data perfectly, but fails in many real world cases. This is why data should always be split into a training set and a validation set, so you can attempt to assess if you overfit.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pretty much any ML algorithm can overfit if not used properly. Basically you end up with a classifier that can predict the training data perfectly, but fails in many real world cases. This is why data should always be split into a training set and a validation set, so you can attempt to assess if you overfit.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge5dvu", "score_hidden": false, "stickied": false, "created": 1492493100.0, "created_utc": 1492464300.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dge42qf", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "ineedmoresleep", "parent_id": "t1_dge2hcm", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; can actually lead to overfitting\n\nis this actually still a problem? outside of linear regression, that is hardly ever getting used anymore? ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;can actually lead to overfitting&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;is this actually still a problem? outside of linear regression, that is hardly ever getting used anymore? &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge42qf", "score_hidden": false, "stickied": false, "created": 1492491541.0, "created_utc": 1492462741.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dge2hcm", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "RobToastie", "parent_id": "t1_dgdjo53", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; Machines don't have \"biases\", they reflect the patterns that are in the data.\n\nThis is actually not true. Algorithms absolutely have biases, and it is something you need to be aware of when selecting and algorithm, training it, and interpreting the result. It may be fair to say that most algorithms don't have an inherit *social* bias though. Generally when you are talking about biases in AI you are asking questions like \"does this overgeneralize?\" or \"does this prefer accuracy or precision?\" If, for example, you are working on an AI judge, it would be reasonable to have your algorithm biased against false positives in order to protect the innocent. This doesn't change the data or stop it from identifying patterns, but it does change how the machine classifies that data.\n\n&gt; The only way to fight this \"bias\" is to provide more data, and make it as accurate as possible.\n\nAlso not technically true. Providing too much data, if you don't do it properly, can actually lead to overfitting, and lead to poor generalization. It may actually be better to change the algorithm such that is is more accurate and / or precise on a smaller data set. Granted there are problems that require colossal amounts of data to converge on any decent classifier (language is one of those), but you have to be careful about how you use that data, which is what the article is really talking about, otherwise you can start picking up \"patterns\" which may or may not actually exist.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Machines don&amp;#39;t have &amp;quot;biases&amp;quot;, they reflect the patterns that are in the data.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is actually not true. Algorithms absolutely have biases, and it is something you need to be aware of when selecting and algorithm, training it, and interpreting the result. It may be fair to say that most algorithms don&amp;#39;t have an inherit &lt;em&gt;social&lt;/em&gt; bias though. Generally when you are talking about biases in AI you are asking questions like &amp;quot;does this overgeneralize?&amp;quot; or &amp;quot;does this prefer accuracy or precision?&amp;quot; If, for example, you are working on an AI judge, it would be reasonable to have your algorithm biased against false positives in order to protect the innocent. This doesn&amp;#39;t change the data or stop it from identifying patterns, but it does change how the machine classifies that data.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The only way to fight this &amp;quot;bias&amp;quot; is to provide more data, and make it as accurate as possible.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Also not technically true. Providing too much data, if you don&amp;#39;t do it properly, can actually lead to overfitting, and lead to poor generalization. It may actually be better to change the algorithm such that is is more accurate and / or precise on a smaller data set. Granted there are problems that require colossal amounts of data to converge on any decent classifier (language is one of those), but you have to be careful about how you use that data, which is what the article is really talking about, otherwise you can start picking up &amp;quot;patterns&amp;quot; which may or may not actually exist.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge2hcm", "score_hidden": false, "stickied": false, "created": 1492489756.0, "created_utc": 1492460956.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdwfub", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "pro_omnibus", "parent_id": "t1_dgdl7z5", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "In primary education it's about %64 women worldwide, and in the US it's closer to %88. So, pretty accurate to say %90, if you specify that it's for *primary school teachers in the US.*\n\nSource: [The World Bank](http://data.worldbank.org/indicator/SE.PRM.TCHR.FE.ZS?end=2014&amp;start=2014&amp;view=map)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In primary education it&amp;#39;s about %64 women worldwide, and in the US it&amp;#39;s closer to %88. So, pretty accurate to say %90, if you specify that it&amp;#39;s for &lt;em&gt;primary school teachers in the US.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Source: &lt;a href=\"http://data.worldbank.org/indicator/SE.PRM.TCHR.FE.ZS?end=2014&amp;amp;start=2014&amp;amp;view=map\"&gt;The World Bank&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdwfub", "score_hidden": false, "stickied": false, "created": 1492482941.0, "created_utc": 1492454141.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 7}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdl7z5", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sharpiefairy666", "parent_id": "t1_dgdjo53", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "90%? Source please", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;90%? Source please&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdl7z5", "score_hidden": false, "stickied": false, "created": 1492470283.0, "created_utc": 1492441483.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdzryg", "gilded": 0, "archived": false, "score": -1, "report_reasons": null, "author": "vilest", "parent_id": "t1_dgdukmv", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 1, "body": "Well, let me be more specific: I'm 100% certain postmodern sociology theories are baseless and unfounded in reality. If you want absurd and contrived, examine any postmodernist theory regarding anything.", "edited": 1492458190.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, let me be more specific: I&amp;#39;m 100% certain postmodern sociology theories are baseless and unfounded in reality. If you want absurd and contrived, examine any postmodernist theory regarding anything.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdzryg", "score_hidden": false, "stickied": false, "created": 1492486733.0, "created_utc": 1492457933.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": -1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdukmv", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "LaoTzusGymShoes", "parent_id": "t1_dgdleql", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; I even have my reservations about sociology even being a science these days.\n\nThis is absurd.  Are you sure you simply don't like what it's telling you?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I even have my reservations about sociology even being a science these days.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is absurd.  Are you sure you simply don&amp;#39;t like what it&amp;#39;s telling you?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdukmv", "score_hidden": false, "stickied": false, "created": 1492480851.0, "created_utc": 1492452051.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdleql", "gilded": 0, "archived": false, "score": -5, "report_reasons": null, "author": "vilest", "parent_id": "t1_dgdjo53", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "I also take exception with the term 'bias' being used. I even have my reservations about sociology even being a science these days.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I also take exception with the term &amp;#39;bias&amp;#39; being used. I even have my reservations about sociology even being a science these days.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdleql", "score_hidden": false, "stickied": false, "created": 1492470519.0, "created_utc": 1492441719.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": -5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdqbbs", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "ineedmoresleep", "parent_id": "t1_dgdpayi", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "by definition. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;by definition. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdqbbs", "score_hidden": false, "stickied": false, "created": 1492476144.0, "created_utc": 1492447344.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdpayi", "gilded": 0, "archived": false, "score": -2, "report_reasons": null, "author": "BadTimeSkeleton", "parent_id": "t1_dgdjo53", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; Machines don't have \"biases\"\n\n[CITATION NEEDED]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Machines don&amp;#39;t have &amp;quot;biases&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;[CITATION NEEDED]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdpayi", "score_hidden": false, "stickied": false, "created": 1492475023.0, "created_utc": 1492446223.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": -2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdjo53", "gilded": 0, "archived": false, "score": 32, "report_reasons": null, "author": "ineedmoresleep", "parent_id": "t3_65v0kw", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Machines don't have \"biases\", they reflect the patterns that are in the data. \n\nThe only way to fight this \"bias\" is to provide more data, and make it as accurate as possible. \n\nAnd honestly, look at the reality of it. The majority of teachers are women. Is it \"biased\" to assume (with a given degree of certainty) that a randomly selected teacher is female?  \n\nI also question the usage of the term \"bias\" here. I don't think that social \"scientists\" use it properly (perhaps they skipped or failed their basic statistics course?). Bias is a difference between your estimate and a true value of a parameter. So, when in reality, 90% of teachers are female, and you demand to pretend that the gender ratio is 50/50, the bias here is all yours, buddy.  \n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Machines don&amp;#39;t have &amp;quot;biases&amp;quot;, they reflect the patterns that are in the data. &lt;/p&gt;\n\n&lt;p&gt;The only way to fight this &amp;quot;bias&amp;quot; is to provide more data, and make it as accurate as possible. &lt;/p&gt;\n\n&lt;p&gt;And honestly, look at the reality of it. The majority of teachers are women. Is it &amp;quot;biased&amp;quot; to assume (with a given degree of certainty) that a randomly selected teacher is female?  &lt;/p&gt;\n\n&lt;p&gt;I also question the usage of the term &amp;quot;bias&amp;quot; here. I don&amp;#39;t think that social &amp;quot;scientists&amp;quot; use it properly (perhaps they skipped or failed their basic statistics course?). Bias is a difference between your estimate and a true value of a parameter. So, when in reality, 90% of teachers are female, and you demand to pretend that the gender ratio is 50/50, the bias here is all yours, buddy.  &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdjo53", "score_hidden": false, "stickied": false, "created": 1492468301.0, "created_utc": 1492439501.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 32}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdq6g7", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "UncleMeat11", "parent_id": "t1_dgdp3eu", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "The goal of ai is to perform a task. Intelligence is a red herring that comes from sci-fi and media. Regardless of a system's ability to behave like a sci-fi ai, we want them to do things like aid in the hiring process or diagnose diseases and learned bias from training data can be a problem.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The goal of ai is to perform a task. Intelligence is a red herring that comes from sci-fi and media. Regardless of a system&amp;#39;s ability to behave like a sci-fi ai, we want them to do things like aid in the hiring process or diagnose diseases and learned bias from training data can be a problem.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdq6g7", "score_hidden": false, "stickied": false, "created": 1492475994.0, "created_utc": 1492447194.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 7}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dge08yg", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "MonsieurGrimm", "parent_id": "t1_dgdp3eu", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt;Isn't the goal of AI to be intelligent?\n\nThat's the end goal. Neural networks aren't exactly intelligent in their own right, though. They're a method for classification, with known problems regarding how explainable their results are. It makes perfect sense that a neural network would pick up racist/sexist classification behaviours - it just means that more work needs to be done on designing these systems in an intelligent, explainable way.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Isn&amp;#39;t the goal of AI to be intelligent?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;That&amp;#39;s the end goal. Neural networks aren&amp;#39;t exactly intelligent in their own right, though. They&amp;#39;re a method for classification, with known problems regarding how explainable their results are. It makes perfect sense that a neural network would pick up racist/sexist classification behaviours - it just means that more work needs to be done on designing these systems in an intelligent, explainable way.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge08yg", "score_hidden": false, "stickied": false, "created": 1492487261.0, "created_utc": 1492458461.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdp3eu", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "Uncle_Erik", "parent_id": "t1_dgdmirm", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Isn't the goal of AI to be intelligent?  I would not consider a machine intelligent unless it can question itself and its own beliefs.  Sure, you can program something to give racist results.  But shouldn't an AI question its own assumptions?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Isn&amp;#39;t the goal of AI to be intelligent?  I would not consider a machine intelligent unless it can question itself and its own beliefs.  Sure, you can program something to give racist results.  But shouldn&amp;#39;t an AI question its own assumptions?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdp3eu", "score_hidden": false, "stickied": false, "created": 1492474789.0, "created_utc": 1492445989.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 0}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdq9ql", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "UncleMeat11", "parent_id": "t1_dgdnj47", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "The stereotypes don't need to have validity to make it into the model, they just need to be present in the training data. If, for example, our writing is less likely to include smart women then the model might associate maleness with intelligence not due to any real connection but instead because the connection is present in the training data.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The stereotypes don&amp;#39;t need to have validity to make it into the model, they just need to be present in the training data. If, for example, our writing is less likely to include smart women then the model might associate maleness with intelligence not due to any real connection but instead because the connection is present in the training data.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdq9ql", "score_hidden": false, "stickied": false, "created": 1492476095.0, "created_utc": 1492447295.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdo97y", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "okiyama", "parent_id": "t1_dgdnj47", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "It's not necessarily that the stereotypes have validity, just that we use language in a way that validates stereotypes. If it were observing people directly it would be a different story. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not necessarily that the stereotypes have validity, just that we use language in a way that validates stereotypes. If it were observing people directly it would be a different story. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdo97y", "score_hidden": false, "stickied": false, "created": 1492473836.0, "created_utc": 1492445036.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdr1dq", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "hn17", "parent_id": "t1_dgdnj47", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "I haven't said with one word that \"the computer is racist\". That's your fantasy, ok? \n\nI'm saying that it's hard to train neural networks, because the data that is analyzed will contain non-obvious correlations. If we want to use AI to help us guide our decisions on a large scale, we should be aware of those problems and we should make sure that there aren't unwanted valuations in the datasets as they are in this example. But right now we don't have good tools to analyze neural networks and to retrace *why* a NN comes to certain results and the more complex the systems get, the harder it will be to judge the validity of the results. \n\n*That* is my point. \n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I haven&amp;#39;t said with one word that &amp;quot;the computer is racist&amp;quot;. That&amp;#39;s your fantasy, ok? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m saying that it&amp;#39;s hard to train neural networks, because the data that is analyzed will contain non-obvious correlations. If we want to use AI to help us guide our decisions on a large scale, we should be aware of those problems and we should make sure that there aren&amp;#39;t unwanted valuations in the datasets as they are in this example. But right now we don&amp;#39;t have good tools to analyze neural networks and to retrace &lt;em&gt;why&lt;/em&gt; a NN comes to certain results and the more complex the systems get, the harder it will be to judge the validity of the results. &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;That&lt;/em&gt; is my point. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdr1dq", "score_hidden": false, "stickied": false, "created": 1492476940.0, "created_utc": 1492448140.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdnj47", "gilded": 0, "archived": false, "score": -1, "report_reasons": null, "author": "Telluride12", "parent_id": "t1_dgdmirm", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "if, if, if. Yes, you *could* design a neural net to be a bigot, but that's your fantasy, not reality.  There would be no commercial value to an A.I. that doesn't work properly (i.e. nobody would pay them for it to use for example in self driving software) What's happening is some stereotypes are being proven to have some validity.  For example, self driving cars may notice cyclists have certain tendencies to behave a certain way. In some cases, it may be a negative tendency. The AI would account for this behavior and hopefully save lives. This is ok, right? But when the AI starts accounting for patterns noticed in racial/gender subsets it's suddenly a racist computer. That's not how statistical data is interpreted in reality.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;if, if, if. Yes, you &lt;em&gt;could&lt;/em&gt; design a neural net to be a bigot, but that&amp;#39;s your fantasy, not reality.  There would be no commercial value to an A.I. that doesn&amp;#39;t work properly (i.e. nobody would pay them for it to use for example in self driving software) What&amp;#39;s happening is some stereotypes are being proven to have some validity.  For example, self driving cars may notice cyclists have certain tendencies to behave a certain way. In some cases, it may be a negative tendency. The AI would account for this behavior and hopefully save lives. This is ok, right? But when the AI starts accounting for patterns noticed in racial/gender subsets it&amp;#39;s suddenly a racist computer. That&amp;#39;s not how statistical data is interpreted in reality.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdnj47", "score_hidden": false, "stickied": false, "created": 1492473009.0, "created_utc": 1492444209.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": -1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdmirm", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "hn17", "parent_id": "t1_dgdkjpi", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "When you train a neural net like this and it uncovers a \"closeness\" in word pairs like \"dark, bad, negro\" and \"white, good, european\" because the texts that were used as input for the system use these words frequently together, then that is technically pattern recognition, yes. \n\nIf you would then build an AI with this neural net, that is able to build semantic connections and \"is a\" or \"has a\" relationships, then this AI would probably express the same biases that it learned by analyzing the data before. Without further training it would suggest that \"Europeans are good\" and \"Negros are bad\", because that's what the data says. The machine can't be biased, but it would still express the same racist bullshit that some people do.\n\nMy question now is: what if you don't get a credit, because the AI that checks your application recognized the pattern that people with your surname are usually poor and not credit worthy? That is a simple made-up and obvious example, but in complex data sets might be hidden correlations that are not easily recognizable for humans. \n\nIf machines would make decisions based on these data sets, you will end up with biased decisions.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;When you train a neural net like this and it uncovers a &amp;quot;closeness&amp;quot; in word pairs like &amp;quot;dark, bad, negro&amp;quot; and &amp;quot;white, good, european&amp;quot; because the texts that were used as input for the system use these words frequently together, then that is technically pattern recognition, yes. &lt;/p&gt;\n\n&lt;p&gt;If you would then build an AI with this neural net, that is able to build semantic connections and &amp;quot;is a&amp;quot; or &amp;quot;has a&amp;quot; relationships, then this AI would probably express the same biases that it learned by analyzing the data before. Without further training it would suggest that &amp;quot;Europeans are good&amp;quot; and &amp;quot;Negros are bad&amp;quot;, because that&amp;#39;s what the data says. The machine can&amp;#39;t be biased, but it would still express the same racist bullshit that some people do.&lt;/p&gt;\n\n&lt;p&gt;My question now is: what if you don&amp;#39;t get a credit, because the AI that checks your application recognized the pattern that people with your surname are usually poor and not credit worthy? That is a simple made-up and obvious example, but in complex data sets might be hidden correlations that are not easily recognizable for humans. &lt;/p&gt;\n\n&lt;p&gt;If machines would make decisions based on these data sets, you will end up with biased decisions.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdmirm", "score_hidden": false, "stickied": false, "created": 1492471843.0, "created_utc": 1492443043.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 7}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdkjpi", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "Telluride12", "parent_id": "t1_dgdg8qc", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Pattern recognition isn't the same as bias. As was posted lower in this thread, \"Bias is a difference between your estimate and a true value of a parameter.\"", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pattern recognition isn&amp;#39;t the same as bias. As was posted lower in this thread, &amp;quot;Bias is a difference between your estimate and a true value of a parameter.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdkjpi", "score_hidden": false, "stickied": false, "created": 1492469439.0, "created_utc": 1492440639.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 9}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dge2tlo", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "RobToastie", "parent_id": "t1_dgdg8qc", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "&gt; The question is: what if scientists don't see the connections anymore that might lead to other biases of self-learning neural nets?\n\nThis is a very tricky problem actually, and one that everyone in the field should be acutely aware of. Generally speaking, whenever you are designing an AI, one of the things you should be asking about any algorithm you are considering is \"what are its biases?\" ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The question is: what if scientists don&amp;#39;t see the connections anymore that might lead to other biases of self-learning neural nets?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is a very tricky problem actually, and one that everyone in the field should be acutely aware of. Generally speaking, whenever you are designing an AI, one of the things you should be asking about any algorithm you are considering is &amp;quot;what are its biases?&amp;quot; &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge2tlo", "score_hidden": false, "stickied": false, "created": 1492490140.0, "created_utc": 1492461340.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdg8qc", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "hn17", "parent_id": "t3_65v0kw", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Very interesting. It makes sense that an AI that 'learns' from Humans will also learn the same deeply ingrained prejudices of our societies. \n\nThe question is: what if scientists don't see the connections anymore that might lead to other biases of self-learning neural nets? \n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Very interesting. It makes sense that an AI that &amp;#39;learns&amp;#39; from Humans will also learn the same deeply ingrained prejudices of our societies. &lt;/p&gt;\n\n&lt;p&gt;The question is: what if scientists don&amp;#39;t see the connections anymore that might lead to other biases of self-learning neural nets? &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdg8qc", "score_hidden": false, "stickied": false, "created": 1492463119.0, "created_utc": 1492434319.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 7}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dge41pq", "gilded": 0, "archived": false, "score": -3, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dge3yj2", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "[removed]", "edited": 1492476270.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge41pq", "score_hidden": false, "stickied": false, "created": 1492491509.0, "created_utc": 1492462709.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": -3}}], "after": null, "before": null}}, "user_reports": [], "id": "dge3yj2", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "berlinbrown", "parent_id": "t1_dge3k0j", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Calm down dude.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Calm down dude.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge3yj2", "score_hidden": false, "stickied": false, "created": 1492491410.0, "created_utc": 1492462610.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dge3k0j", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "hereforthemoney", "parent_id": "t1_dgdy8p9", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Jesus Christ, it's not actual AI. It's just an automated bot set to pick-up datasets scoured from human interactions online and analyze them. The machine isn't actually sentient.\n\nI knew soon as I saw this article that there'd be people online taking it at face-value to try prejudice is an inherent part of nature and here you are, bang to schedule! \n\nSeriously, how in the fuck is a machine expected to be biased against certain races and people anyway? It doesn't have any emotions. Even if machines did become self-aware and biased overnight it'd be against humanity as a whole, because guess what? It's a fucking machine! They're not going to be bothered by race or gender, as that's just stupid.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Jesus Christ, it&amp;#39;s not actual AI. It&amp;#39;s just an automated bot set to pick-up datasets scoured from human interactions online and analyze them. The machine isn&amp;#39;t actually sentient.&lt;/p&gt;\n\n&lt;p&gt;I knew soon as I saw this article that there&amp;#39;d be people online taking it at face-value to try prejudice is an inherent part of nature and here you are, bang to schedule! &lt;/p&gt;\n\n&lt;p&gt;Seriously, how in the fuck is a machine expected to be biased against certain races and people anyway? It doesn&amp;#39;t have any emotions. Even if machines did become self-aware and biased overnight it&amp;#39;d be against humanity as a whole, because guess what? It&amp;#39;s a fucking machine! They&amp;#39;re not going to be bothered by race or gender, as that&amp;#39;s just stupid.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dge3k0j", "score_hidden": false, "stickied": false, "created": 1492490961.0, "created_utc": 1492462161.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdy8p9", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "berlinbrown", "parent_id": "t3_65v0kw", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 1, "body": "Maybe they aren't actually racist but we humans think they are racist.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe they aren&amp;#39;t actually racist but we humans think they are racist.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdy8p9", "score_hidden": false, "stickied": false, "created": 1492484986.0, "created_utc": 1492456186.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdtd1z", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "ButtsexEurope", "parent_id": "t1_dgdpdnk", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "What the hell does that have to do with anything?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What the hell does that have to do with anything?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdtd1z", "score_hidden": false, "stickied": false, "created": 1492479526.0, "created_utc": 1492450726.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2rete", "removal_reason": null, "link_id": "t3_65v0kw", "likes": null, "replies": "", "user_reports": [], "id": "dgdpsvf", "gilded": 0, "archived": false, "score": -1, "report_reasons": null, "author": "someshwaguy", "parent_id": "t1_dgdpdnk", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Well, then leave them to it, I say. North Korea is still capitalist-free, right?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, then leave them to it, I say. North Korea is still capitalist-free, right?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdpsvf", "score_hidden": false, "stickied": false, "created": 1492475576.0, "created_utc": 1492446776.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": -1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdpdnk", "gilded": 0, "archived": false, "score": -5, "report_reasons": null, "author": "BadTimeSkeleton", "parent_id": "t1_dgdgpj4", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 0, "body": "Capitalism=white supremacist kyriarchy", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Capitalism=white supremacist kyriarchy&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdpdnk", "score_hidden": false, "stickied": false, "created": 1492475105.0, "created_utc": 1492446305.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": -5}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdgpj4", "gilded": 0, "archived": false, "score": -1, "report_reasons": null, "author": "someshwaguy", "parent_id": "t3_65v0kw", "subreddit_name_prefixed": "r/Foodforthought", "controversiality": 1, "body": "One must please the creators.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;One must please the creators.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "Foodforthought", "name": "t1_dgdgpj4", "score_hidden": false, "stickied": false, "created": 1492463918.0, "created_utc": 1492435118.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": -1}}], "after": null, "before": null}}]