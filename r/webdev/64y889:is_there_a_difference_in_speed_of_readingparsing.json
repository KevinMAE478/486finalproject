[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "webdev", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say I had a CSV file with 2 columns of 100k rows, would it be read any faster if I instead transposed it so that it was now 2 rows with 100k columns? Or is there no difference?&lt;/p&gt;\n\n&lt;p&gt;Just thinking because of the way the browser reads the file, it&amp;#39;s reading thousands of new lines versus thousands of commas on the same line.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "Let's say I had a CSV file with 2 columns of 100k rows, would it be read any faster if I instead transposed it so that it was now 2 rows with 100k columns? Or is there no difference?\n\nJust thinking because of the way the browser reads the file, it's reading thousands of new lines versus thousands of commas on the same line.", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": null, "id": "64y889", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 10, "report_reasons": null, "author": "Oreoloveboss", "saved": false, "mod_reports": [], "name": "t3_64y889", "subreddit_name_prefixed": "r/webdev", "approved_by": null, "over_18": false, "domain": "self.webdev", "hidden": false, "thumbnail": "self", "subreddit_id": "t5_2qs0q", "edited": false, "link_flair_css_class": null, "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/webdev/comments/64y889/is_there_a_difference_in_speed_of_readingparsing/", "num_reports": null, "locked": false, "stickied": false, "created": 1492033557.0, "url": "https://www.reddit.com/r/webdev/comments/64y889/is_there_a_difference_in_speed_of_readingparsing/", "author_flair_text": null, "quarantine": false, "title": "Is there a difference in speed of reading/parsing CSV file if it had a long row versus long column?", "created_utc": 1492004757.0, "distinguished": null, "media": null, "upvote_ratio": 0.82, "num_comments": 8, "visited": false, "subreddit_type": "public", "ups": 10}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qs0q", "removal_reason": null, "link_id": "t3_64y889", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qs0q", "removal_reason": null, "link_id": "t3_64y889", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qs0q", "removal_reason": null, "link_id": "t3_64y889", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qs0q", "removal_reason": null, "link_id": "t3_64y889", "likes": null, "replies": "", "user_reports": [], "id": "dg67w6d", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "lostburner", "parent_id": "t1_dg5zptg", "subreddit_name_prefixed": "r/webdev", "controversiality": 0, "body": "Agreed with all of this. The newlines are pretty trivial because it's easy to run the code through an autoformatter, but you still have serious obfuscation in a minified file due to the variable names all being replaced.", "edited": 1492018972.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Agreed with all of this. The newlines are pretty trivial because it&amp;#39;s easy to run the code through an autoformatter, but you still have serious obfuscation in a minified file due to the variable names all being replaced.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "webdev", "name": "t1_dg67w6d", "score_hidden": false, "stickied": false, "created": 1492047207.0, "created_utc": 1492018407.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5zptg", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "Alucard256", "parent_id": "t1_dg5yr3d", "subreddit_name_prefixed": "r/webdev", "controversiality": 0, "body": "That's a good example, however the removing of \"new-lines\" (and other characters) is because they're totally not needed by the machine. It's only us humans that need to \"see\" text formatted nicely into wrapped text with tabs, new-lines, etc.\n\nIt's not that the task is \"easier\" for the machine when a file is \"mini-fied\", it just removes unnecessary characters to shorten the over-all file size by every byte possible. This means that loading these files and transmitting them (if needed) is just that much quicker.\n\nIt's also a (very minor and trivial) layer of protection because your code isn't all laid out nicely and ready to be copied (like the old days of the net when you could actually see 100% of the code that made up a webpage with just \"View Source\"). This doesn't fall under the heading of actual \"security\", but in my mind, it falls under the category of \"don't make it easy for them\".", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s a good example, however the removing of &amp;quot;new-lines&amp;quot; (and other characters) is because they&amp;#39;re totally not needed by the machine. It&amp;#39;s only us humans that need to &amp;quot;see&amp;quot; text formatted nicely into wrapped text with tabs, new-lines, etc.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not that the task is &amp;quot;easier&amp;quot; for the machine when a file is &amp;quot;mini-fied&amp;quot;, it just removes unnecessary characters to shorten the over-all file size by every byte possible. This means that loading these files and transmitting them (if needed) is just that much quicker.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also a (very minor and trivial) layer of protection because your code isn&amp;#39;t all laid out nicely and ready to be copied (like the old days of the net when you could actually see 100% of the code that made up a webpage with just &amp;quot;View Source&amp;quot;). This doesn&amp;#39;t fall under the heading of actual &amp;quot;security&amp;quot;, but in my mind, it falls under the category of &amp;quot;don&amp;#39;t make it easy for them&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "webdev", "name": "t1_dg5zptg", "score_hidden": false, "stickied": false, "created": 1492038576.0, "created_utc": 1492009776.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5yr3d", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Oreoloveboss", "parent_id": "t1_dg5yn2v", "subreddit_name_prefixed": "r/webdev", "controversiality": 0, "body": "Thanks for the advice, I'll look into it. It was just out of curiosity because I thought it could be for the same reason that CSS is minified to be on a single line rather than multiple lines. I thought that meant a line thousands of characters long is 'easier' to read than thousands of lines with only a few characters.\n\nWhat I've created is basically a html drop down with a list of all the unique values in the first column of a CSV, when you choose a value, it then outputs a &lt;ul&gt; of all matching values in column 2. It's a live CSV so it's eaiser for users to edit it with long columns versus rows, but I could always add in an extra step that they simply transpose at the end.", "edited": 1492009030.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the advice, I&amp;#39;ll look into it. It was just out of curiosity because I thought it could be for the same reason that CSS is minified to be on a single line rather than multiple lines. I thought that meant a line thousands of characters long is &amp;#39;easier&amp;#39; to read than thousands of lines with only a few characters.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;ve created is basically a html drop down with a list of all the unique values in the first column of a CSV, when you choose a value, it then outputs a &amp;lt;ul&amp;gt; of all matching values in column 2. It&amp;#39;s a live CSV so it&amp;#39;s eaiser for users to edit it with long columns versus rows, but I could always add in an extra step that they simply transpose at the end.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "webdev", "name": "t1_dg5yr3d", "score_hidden": false, "stickied": false, "created": 1492037485.0, "created_utc": 1492008685.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5yn2v", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "Alucard256", "parent_id": "t3_64y889", "subreddit_name_prefixed": "r/webdev", "controversiality": 0, "body": "If 2 sets of code where written to read/parse the 2 different sets of this data, I'm guessing the read/parse time would be the same.\n\nHowever, further guessing, I bet the amount of memory needed during run-time would be vastly different.\n\nThis sounds like a really good case for learning and a personal project in general. I would write up both and see what the differences are. Then, in the future, when this question comes up again (and it will), you will know the answer *for sure*.\n\nThis is also a good example of my #1 mantra/advice: There is NO \"one right way\" to do anything. If you find a way that solves the problem, does so fairly efficiently, and doesn't crash, then you have found *a way*; there is no *the way*.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If 2 sets of code where written to read/parse the 2 different sets of this data, I&amp;#39;m guessing the read/parse time would be the same.&lt;/p&gt;\n\n&lt;p&gt;However, further guessing, I bet the amount of memory needed during run-time would be vastly different.&lt;/p&gt;\n\n&lt;p&gt;This sounds like a really good case for learning and a personal project in general. I would write up both and see what the differences are. Then, in the future, when this question comes up again (and it will), you will know the answer &lt;em&gt;for sure&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;This is also a good example of my #1 mantra/advice: There is NO &amp;quot;one right way&amp;quot; to do anything. If you find a way that solves the problem, does so fairly efficiently, and doesn&amp;#39;t crash, then you have found &lt;em&gt;a way&lt;/em&gt;; there is no &lt;em&gt;the way&lt;/em&gt;.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "webdev", "name": "t1_dg5yn2v", "score_hidden": false, "stickied": false, "created": 1492037359.0, "created_utc": 1492008559.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qs0q", "removal_reason": null, "link_id": "t3_64y889", "likes": null, "replies": "", "user_reports": [], "id": "dg683wp", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "KSubedi", "parent_id": "t3_64y889", "subreddit_name_prefixed": "r/webdev", "controversiality": 0, "body": "I had to build my own CSV parser for work because the dataset I was working with was huge and all the existing libraries were too slow for the use case, and I was pulling one line at a time to buffer and parsing everything one at  a time. Wider rows means you would have higher memory usage since there will be more content in the buffer, however I think the parsing speed shouldnt be much different as its the same amount of data.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I had to build my own CSV parser for work because the dataset I was working with was huge and all the existing libraries were too slow for the use case, and I was pulling one line at a time to buffer and parsing everything one at  a time. Wider rows means you would have higher memory usage since there will be more content in the buffer, however I think the parsing speed shouldnt be much different as its the same amount of data.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "webdev", "name": "t1_dg683wp", "score_hidden": false, "stickied": false, "created": 1492047425.0, "created_utc": 1492018625.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qs0q", "removal_reason": null, "link_id": "t3_64y889", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2qs0q", "removal_reason": null, "link_id": "t3_64y889", "likes": null, "replies": "", "user_reports": [], "id": "dg69fme", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Oreoloveboss", "parent_id": "t1_dg66dd0", "subreddit_name_prefixed": "r/webdev", "controversiality": 0, "body": "It's through jquery and ajax and very basic, first time ever doing something like this. If the html select menu is changed at all, it runs the function and loads the file. Then it reads the file and splits it by line, then it splits again on the comma and does an if statement to append it as a `&lt;li&gt;` if it matches the html select value.\n\n    jQuery(function($){\n      $(\"#client-list\").change(function() {\n      $(\"#client-data\").html(\"\");  \n      var clientid = $('#client-list').val(); \n      $.ajax({\n        url: \"~pathtofile~\",\n        type: 'GET',\n        headers: { \"cache-control\": \"no-cache\" },  \n        dataType: 'text',\n        success: function (data) {\n           $.each(data.split(/[\\n\\r]+/), function(index, line) {\n            temp = line.split(\",\");\n            if (temp[0] == clientid )\n             $('&lt;li&gt;').text(temp[1]).appendTo('#client-data');\n           });\n        },\n         async: true,\n        });\n      });\n    });\n\nI would have like to do it as a json file, but the request was for a 'spreadsheet' that users could simply edit and add rows to via ftp.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s through jquery and ajax and very basic, first time ever doing something like this. If the html select menu is changed at all, it runs the function and loads the file. Then it reads the file and splits it by line, then it splits again on the comma and does an if statement to append it as a &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt; if it matches the html select value.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;jQuery(function($){\n  $(&amp;quot;#client-list&amp;quot;).change(function() {\n  $(&amp;quot;#client-data&amp;quot;).html(&amp;quot;&amp;quot;);  \n  var clientid = $(&amp;#39;#client-list&amp;#39;).val(); \n  $.ajax({\n    url: &amp;quot;~pathtofile~&amp;quot;,\n    type: &amp;#39;GET&amp;#39;,\n    headers: { &amp;quot;cache-control&amp;quot;: &amp;quot;no-cache&amp;quot; },  \n    dataType: &amp;#39;text&amp;#39;,\n    success: function (data) {\n       $.each(data.split(/[\\n\\r]+/), function(index, line) {\n        temp = line.split(&amp;quot;,&amp;quot;);\n        if (temp[0] == clientid )\n         $(&amp;#39;&amp;lt;li&amp;gt;&amp;#39;).text(temp[1]).appendTo(&amp;#39;#client-data&amp;#39;);\n       });\n    },\n     async: true,\n    });\n  });\n});\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I would have like to do it as a json file, but the request was for a &amp;#39;spreadsheet&amp;#39; that users could simply edit and add rows to via ftp.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "webdev", "name": "t1_dg69fme", "score_hidden": false, "stickied": false, "created": 1492048757.0, "created_utc": 1492019957.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg66dd0", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "RotationSurgeon", "parent_id": "t3_64y889", "subreddit_name_prefixed": "r/webdev", "controversiality": 0, "body": "My evidence is anecdotal insofar as I didn't actually measure the speed quantitatively, but qualitatively, on a project I put together which included a retail location finder tool using a large JSON file to store thousands of locations, minifying the file provided qualitatively better performance (it _seemed_ to load much more quickly) than leaving the JSON fully expanded.  Of course, this likely had as much or more to do with the size of the file being transferred over HTTP than it did the actual time spent processing it.\n\nWhat language / CSV handling library are you using?  Do you know if it reads the CSV one line at a time, or as an entire stream at once? For instance, the Python CSV module calls the `next()` method of the underlying iterator for each line in a CSV file.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My evidence is anecdotal insofar as I didn&amp;#39;t actually measure the speed quantitatively, but qualitatively, on a project I put together which included a retail location finder tool using a large JSON file to store thousands of locations, minifying the file provided qualitatively better performance (it &lt;em&gt;seemed&lt;/em&gt; to load much more quickly) than leaving the JSON fully expanded.  Of course, this likely had as much or more to do with the size of the file being transferred over HTTP than it did the actual time spent processing it.&lt;/p&gt;\n\n&lt;p&gt;What language / CSV handling library are you using?  Do you know if it reads the CSV one line at a time, or as an entire stream at once? For instance, the Python CSV module calls the &lt;code&gt;next()&lt;/code&gt; method of the underlying iterator for each line in a CSV file.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "webdev", "name": "t1_dg66dd0", "score_hidden": false, "stickied": false, "created": 1492045679.0, "created_utc": 1492016879.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qs0q", "removal_reason": null, "link_id": "t3_64y889", "likes": null, "replies": "", "user_reports": [], "id": "dg67qdc", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "blahyawnblah", "parent_id": "t3_64y889", "subreddit_name_prefixed": "r/webdev", "controversiality": 0, "body": "The wide one will probably require much more memory during the parsing because you have to create an object with lots of properties. In the end, the memory usage will be similar unless you don't keep the whole thing in memory and use some kind of streamer or iterable.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The wide one will probably require much more memory during the parsing because you have to create an object with lots of properties. In the end, the memory usage will be similar unless you don&amp;#39;t keep the whole thing in memory and use some kind of streamer or iterable.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "webdev", "name": "t1_dg67qdc", "score_hidden": false, "stickied": false, "created": 1492047047.0, "created_utc": 1492018247.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]