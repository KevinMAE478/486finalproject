[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Research", "id": "65sfj3", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 16, "report_reasons": null, "author": "MetricSpade007", "saved": false, "mod_reports": [], "name": "t3_65sfj3", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "arxiv.org", "hidden": false, "thumbnail": "default", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "three", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65sfj3/r_on_generalized_bellman_equations_and/", "num_reports": null, "locked": false, "stickied": false, "created": 1492417800.0, "url": "https://arxiv.org/pdf/1704.04463.pdf", "author_flair_text": null, "quarantine": false, "title": "[R] On Generalized Bellman Equations and Temporal-Difference Learning", "created_utc": 1492389000.0, "distinguished": null, "media": null, "upvote_ratio": 1.0, "num_comments": 1, "visited": false, "subreddit_type": "public", "ups": 16}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65sfj3", "likes": null, "replies": "", "user_reports": [], "id": "dgctu2u", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "arXiv_abstract_bot", "parent_id": "t3_65sfj3", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Title: On Generalized Bellman Equations and Temporal-Difference Learning  \n\nAuthors: [Huizhen Yu](http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1), [A. Rupam Mahmood](http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1), [Richard S. Sutton](http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1)  \n\n&gt; Abstract: We consider off-policy temporal-difference (TD) learning in discounted Markov decision processes, where the goal is to evaluate a policy in a model-free way by using observations of a state process generated without executing the policy. To curb the high variance issue in off-policy TD learning, we propose a new scheme of setting the $\\lambda$-parameters of TD, based on generalized Bellman equations. Our scheme is to set $\\lambda$ according to the eligibility trace iterates calculated in TD, thereby easily keeping these traces in a desired bounded range. Compared to prior works, this scheme is more direct and flexible, and allows much larger $\\lambda$ values for off-policy TD learning with bounded traces. Using Markov chain theory, we prove the ergodicity of the joint state-trace process under nonrestrictive conditions, and we show that associated with our scheme is a generalized Bellman equation (for the policy to be evaluated) that depends on both the evolution of $\\lambda$ and the unique invariant probability measure of the state-trace process. These results not only lead immediately to a characterization of the convergence behavior of least-squares based implementation of our scheme, but also prepare the ground for further analysis of gradient-based implementations.  \n\n[PDF link](https://arxiv.org/pdf/1704.04463)  [Landing page](https://arxiv.org/abs/1704.04463)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Title: On Generalized Bellman Equations and Temporal-Difference Learning  &lt;/p&gt;\n\n&lt;p&gt;Authors: &lt;a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\"&gt;Huizhen Yu&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1\"&gt;A. Rupam Mahmood&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\"&gt;Richard S. Sutton&lt;/a&gt;  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Abstract: We consider off-policy temporal-difference (TD) learning in discounted Markov decision processes, where the goal is to evaluate a policy in a model-free way by using observations of a state process generated without executing the policy. To curb the high variance issue in off-policy TD learning, we propose a new scheme of setting the $\\lambda$-parameters of TD, based on generalized Bellman equations. Our scheme is to set $\\lambda$ according to the eligibility trace iterates calculated in TD, thereby easily keeping these traces in a desired bounded range. Compared to prior works, this scheme is more direct and flexible, and allows much larger $\\lambda$ values for off-policy TD learning with bounded traces. Using Markov chain theory, we prove the ergodicity of the joint state-trace process under nonrestrictive conditions, and we show that associated with our scheme is a generalized Bellman equation (for the policy to be evaluated) that depends on both the evolution of $\\lambda$ and the unique invariant probability measure of the state-trace process. These results not only lead immediately to a characterization of the convergence behavior of least-squares based implementation of our scheme, but also prepare the ground for further analysis of gradient-based implementations.  &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/1704.04463\"&gt;PDF link&lt;/a&gt;  &lt;a href=\"https://arxiv.org/abs/1704.04463\"&gt;Landing page&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgctu2u", "score_hidden": false, "stickied": false, "created": 1492417821.0, "created_utc": 1492389021.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}]