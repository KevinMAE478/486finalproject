[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m very new to Neural Networks but I&amp;#39;ve managed to create a character level RNN which uses 1-of-K encoding using the Sigmoid function and softmax at the final layer to get the probability of the next character based on &lt;a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\"&gt;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m trying to understand is that instead of using 1-of-K encoding method for each possible character, what if I used binary encoding to reduce the input layer size, but then at the output layer I&amp;#39;d need a method of allowing a binary encoded output.&lt;/p&gt;\n\n&lt;p&gt;E.g. \n - Convert the ascii code for &amp;quot;A&amp;quot; (65) to binary (01000001) and use that as input data. \n - Then I want to train it to output B in binary for the ascii code which would be (01000010). &lt;/p&gt;\n\n&lt;p&gt;However by using softmax only 1 output node would be set to 1, removing the possibility of (01000010).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve considered using ReLu but I&amp;#39;m not sure if that would work? Also I&amp;#39;m using no python libraries for machine learning, and would like to maintain that.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be appreciated! I&amp;#39;m very new to this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "I'm very new to Neural Networks but I've managed to create a character level RNN which uses 1-of-K encoding using the Sigmoid function and softmax at the final layer to get the probability of the next character based on http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n\nWhat I'm trying to understand is that instead of using 1-of-K encoding method for each possible character, what if I used binary encoding to reduce the input layer size, but then at the output layer I'd need a method of allowing a binary encoded output.\n\nE.g. \n - Convert the ascii code for \"A\" (65) to binary (01000001) and use that as input data. \n - Then I want to train it to output B in binary for the ascii code which would be (01000010). \n\nHowever by using softmax only 1 output node would be set to 1, removing the possibility of (01000010).\n\nI've considered using ReLu but I'm not sure if that would work? Also I'm using no python libraries for machine learning, and would like to maintain that.\n\nAny advice would be appreciated! I'm very new to this.", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "65fq5h", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 1, "report_reasons": null, "author": "DiproIV", "saved": false, "mod_reports": [], "name": "t3_65fq5h", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/bQadO_ovZDNTBdcbqmX3WZQFUj4Vc-rPVVGGu5Zd_sE.jpg?s=3885dc0fd461f7ee6768ea39c36d9b8f", "width": 1639, "height": 825}, "resolutions": [{"url": "https://i.redditmedia.com/bQadO_ovZDNTBdcbqmX3WZQFUj4Vc-rPVVGGu5Zd_sE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=9c08da67ef30380b8b582ab366d18cd3", "width": 108, "height": 54}, {"url": "https://i.redditmedia.com/bQadO_ovZDNTBdcbqmX3WZQFUj4Vc-rPVVGGu5Zd_sE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=ae0bdf6891d71bd404614b2c28d9a7c9", "width": 216, "height": 108}, {"url": "https://i.redditmedia.com/bQadO_ovZDNTBdcbqmX3WZQFUj4Vc-rPVVGGu5Zd_sE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=e31d1a855a398eb8245e6b538e60f91b", "width": 320, "height": 161}, {"url": "https://i.redditmedia.com/bQadO_ovZDNTBdcbqmX3WZQFUj4Vc-rPVVGGu5Zd_sE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=c562630fb581236f9378b11b64680e5f", "width": 640, "height": 322}, {"url": "https://i.redditmedia.com/bQadO_ovZDNTBdcbqmX3WZQFUj4Vc-rPVVGGu5Zd_sE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=960&amp;s=ce9ca2aae351210aaa0075d6ed99bc18", "width": 960, "height": 483}, {"url": "https://i.redditmedia.com/bQadO_ovZDNTBdcbqmX3WZQFUj4Vc-rPVVGGu5Zd_sE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=1080&amp;s=2f1ac1470d6323a4e78474d1be7d193c", "width": 1080, "height": 543}], "variants": {}, "id": "JiGPgaHX71ivSm17nn1b-3z3BBQtBEUm4A3c1ID8dNw"}], "enabled": false}, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "self", "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65fq5h/d_binary_encoding_and_activation_functions/", "num_reports": null, "locked": false, "stickied": false, "created": 1492238281.0, "url": "https://www.reddit.com/r/MachineLearning/comments/65fq5h/d_binary_encoding_and_activation_functions/", "author_flair_text": null, "quarantine": false, "title": "[D] Binary encoding and activation functions", "created_utc": 1492209481.0, "distinguished": null, "media": null, "upvote_ratio": 0.6, "num_comments": 6, "visited": false, "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65fq5h", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65fq5h", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65fq5h", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65fq5h", "likes": null, "replies": "", "user_reports": [], "id": "dgak9t2", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "DiproIV", "parent_id": "t1_dga91li", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Oooo could you elaborate on this a bit? Is this just implied through learning?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oooo could you elaborate on this a bit? Is this just implied through learning?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgak9t2", "score_hidden": false, "stickied": false, "created": 1492285468.0, "created_utc": 1492256668.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65fq5h", "likes": null, "replies": "", "user_reports": [], "id": "dgdo9tf", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "jlobrist", "parent_id": "t1_dga91li", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I think this could lead to large erroneous results if one threshold was off. I would suggest a gray code  encoding to reduce errors like this. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think this could lead to large erroneous results if one threshold was off. I would suggest a gray code  encoding to reduce errors like this. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdo9tf", "score_hidden": false, "stickied": false, "created": 1492473854.0, "created_utc": 1492445054.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dga91li", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "iforgot120", "parent_id": "t1_dg9xwk2", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yup, you can have threshold values that can be learned.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yup, you can have threshold values that can be learned.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dga91li", "score_hidden": false, "stickied": false, "created": 1492257442.0, "created_utc": 1492228642.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9xwk2", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "DiproIV", "parent_id": "t1_dg9whlk", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I tried a similar approach, but how would I clip the outputs to either 0 or 1 (to match binary).. Would it just be a case rounding up or down?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I tried a similar approach, but how would I clip the outputs to either 0 or 1 (to match binary).. Would it just be a case rounding up or down?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9xwk2", "score_hidden": false, "stickied": false, "created": 1492241666.0, "created_utc": 1492212866.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9whlk", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "kjearns", "parent_id": "t3_65fq5h", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You can a sigmoid for each bit, which is equivalent to having a classifier for each bit being on or off in the output.  The appropriate loss function for this is typically called \"binary cross entropy\".\n\nI expect doing this will make learning much harder.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can a sigmoid for each bit, which is equivalent to having a classifier for each bit being on or off in the output.  The appropriate loss function for this is typically called &amp;quot;binary cross entropy&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I expect doing this will make learning much harder.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9whlk", "score_hidden": false, "stickied": false, "created": 1492239790.0, "created_utc": 1492210990.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65fq5h", "likes": null, "replies": "", "user_reports": [], "id": "dgam54d", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "olBaa", "parent_id": "t3_65fq5h", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "This idea sounds pretty similar to hierarchical softmax. Char-rnn does not sudder from the complexity of the last output layer, though.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This idea sounds pretty similar to hierarchical softmax. Char-rnn does not sudder from the complexity of the last output layer, though.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgam54d", "score_hidden": false, "stickied": false, "created": 1492289870.0, "created_utc": 1492261070.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]