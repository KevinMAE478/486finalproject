[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Research", "id": "660bwb", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 1, "report_reasons": null, "author": "evc123", "saved": false, "mod_reports": [], "name": "t3_660bwb", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "arxiv.org", "hidden": false, "thumbnail": "default", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "three", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/660bwb/r_170404651_the_reactor_a_sampleefficient/", "num_reports": null, "locked": false, "stickied": false, "created": 1492513485.0, "url": "https://arxiv.org/abs/1704.04651", "author_flair_text": null, "quarantine": false, "title": "[R] [1704.04651] The Reactor: A Sample-Efficient Actor-Critic Architecture", "created_utc": 1492484685.0, "distinguished": null, "media": null, "upvote_ratio": 1.0, "num_comments": 1, "visited": false, "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_660bwb", "likes": null, "replies": "", "user_reports": [], "id": "dgelold", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "arXiv_abstract_bot", "parent_id": "t3_660bwb", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Title: The Reactor: A Sample-Efficient Actor-Critic Architecture  \n\nAuthors: [Audrunas Gruslys](http://arxiv.org/find/cs/1/au:+Gruslys_A/0/1/0/all/0/1), [Mohammad Gheshlaghi Azar](http://arxiv.org/find/cs/1/au:+Azar_M/0/1/0/all/0/1), [Marc G. Bellemare](http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1), [Remi Munos](http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1)  \n\n&gt; Abstract: In this work we present a new reinforcement learning agent, called Reactor (for Retrace-actor), based on an off-policy multi-step return actor- critic architecture. The agent uses a deep recurrent neural network for function approximation. The network outputs a target policy {\\pi} (the actor), an action-value Q-function (the critic) evaluating the current policy {\\pi}, and an estimated behavioral policy {\\hat \\mu} which we use for off-policy correction. The agent maintains a memory buffer filled with past experiences. The critic is trained by the multi-step off-policy Retrace algorithm and the actor is trained by a novel {\\beta}-leave-one-out policy gradient estimate (which uses both the off-policy corrected return and the estimated Q-function). The Reactor is sample-efficient thanks to the use of memory replay, and numerical efficient since it uses multi-step returns. Also both acting and learning can be parallelized. We evaluated our algorithm on 57 Atari 2600 games and demonstrate that it achieves state-of-the-art performance.  \n\n[PDF link](https://arxiv.org/pdf/1704.04651)  [Landing page](https://arxiv.org/abs/1704.04651)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Title: The Reactor: A Sample-Efficient Actor-Critic Architecture  &lt;/p&gt;\n\n&lt;p&gt;Authors: &lt;a href=\"http://arxiv.org/find/cs/1/au:+Gruslys_A/0/1/0/all/0/1\"&gt;Audrunas Gruslys&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Azar_M/0/1/0/all/0/1\"&gt;Mohammad Gheshlaghi Azar&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1\"&gt;Marc G. Bellemare&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1\"&gt;Remi Munos&lt;/a&gt;  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Abstract: In this work we present a new reinforcement learning agent, called Reactor (for Retrace-actor), based on an off-policy multi-step return actor- critic architecture. The agent uses a deep recurrent neural network for function approximation. The network outputs a target policy {\\pi} (the actor), an action-value Q-function (the critic) evaluating the current policy {\\pi}, and an estimated behavioral policy {\\hat \\mu} which we use for off-policy correction. The agent maintains a memory buffer filled with past experiences. The critic is trained by the multi-step off-policy Retrace algorithm and the actor is trained by a novel {\\beta}-leave-one-out policy gradient estimate (which uses both the off-policy corrected return and the estimated Q-function). The Reactor is sample-efficient thanks to the use of memory replay, and numerical efficient since it uses multi-step returns. Also both acting and learning can be parallelized. We evaluated our algorithm on 57 Atari 2600 games and demonstrate that it achieves state-of-the-art performance.  &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/1704.04651\"&gt;PDF link&lt;/a&gt;  &lt;a href=\"https://arxiv.org/abs/1704.04651\"&gt;Landing page&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgelold", "score_hidden": false, "stickied": false, "created": 1492513517.0, "created_utc": 1492484717.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]