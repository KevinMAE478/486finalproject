[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, as part of my final project for a ML course I&amp;#39;m trying to implement Variational LSTM Autoencoders as described in this &lt;a href=\"https://arxiv.org/abs/1511.06349\"&gt;paper&lt;/a&gt;. At first I wanted to use tensorflow and tried to follow their tutorials on RNN and Seq2Seq but the library the tutorial uses (tensorflow/tensorflow/python/ops/seq2seq.py) seems to be deprecated and the recently open-sourced tf-seq2seq is too black box for my purposes. &lt;strong&gt;My question is should I use the deprecated seq2seq library or is it possible to implement VLAE in tf-seq2seq specifically with control over the variational approximation or should I go in a different direction (look below)?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I found a &lt;a href=\"https://github.com/cheng6076/Variational-LSTM-Autoencoder\"&gt;torch implementation&lt;/a&gt; but it&amp;#39;s in torch and lua both of which I&amp;#39;m new to. I&amp;#39;m comfortable in python and would&amp;#39;ve liked to work with pytorch or tensorflow. I don&amp;#39;t want to learn torch because it seems pytorch is taking over torch now but with my situation and tensorflow&amp;#39;s decisions of changing seq2seq puts me in a frustrating situation.&lt;/p&gt;\n\n&lt;p&gt;Thank you for reading!&lt;/p&gt;\n\n&lt;p&gt;P.S. Was it only me or tensorflow&amp;#39;s tutorial/documentation on RNN very infuriating to follow? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "Hi, as part of my final project for a ML course I'm trying to implement Variational LSTM Autoencoders as described in this [paper](https://arxiv.org/abs/1511.06349). At first I wanted to use tensorflow and tried to follow their tutorials on RNN and Seq2Seq but the library the tutorial uses (tensorflow/tensorflow/python/ops/seq2seq.py) seems to be deprecated and the recently open-sourced tf-seq2seq is too black box for my purposes. **My question is should I use the deprecated seq2seq library or is it possible to implement VLAE in tf-seq2seq specifically with control over the variational approximation or should I go in a different direction (look below)?**\n\nI found a [torch implementation](https://github.com/cheng6076/Variational-LSTM-Autoencoder) but it's in torch and lua both of which I'm new to. I'm comfortable in python and would've liked to work with pytorch or tensorflow. I don't want to learn torch because it seems pytorch is taking over torch now but with my situation and tensorflow's decisions of changing seq2seq puts me in a frustrating situation.\n\nThank you for reading!\n\nP.S. Was it only me or tensorflow's tutorial/documentation on RNN very infuriating to follow? ", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Project", "id": "65jwms", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 13, "report_reasons": null, "author": "curious_neuron", "saved": false, "mod_reports": [], "name": "t3_65jwms", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": 1492278483.0, "link_flair_css_class": "four", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65jwms/p_help_with_starting_variationallstmautoencoders/", "num_reports": null, "locked": false, "stickied": false, "created": 1492302062.0, "url": "https://www.reddit.com/r/MachineLearning/comments/65jwms/p_help_with_starting_variationallstmautoencoders/", "author_flair_text": null, "quarantine": false, "title": "[P] Help with starting Variational-LSTM-Autoencoders", "created_utc": 1492273262.0, "distinguished": null, "media": null, "upvote_ratio": 0.89, "num_comments": 11, "visited": false, "subreddit_type": "public", "ups": 13}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": "", "user_reports": [], "id": "dgay7j3", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "Ciulerson2", "parent_id": "t1_dgaxus3", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "This isn't exactly seq2seq but I think it shouldnt be hard to tweak it appropriately\n\nhttp://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This isn&amp;#39;t exactly seq2seq but I think it shouldnt be hard to tweak it appropriately&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\"&gt;http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgay7j3", "score_hidden": false, "stickied": false, "created": 1492308138.0, "created_utc": 1492279338.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dgaxus3", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "curious_neuron", "parent_id": "t1_dgaxhaz", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I managed to get through the tensorflow tutorial by reading the source code. Do you have any references for seq2seq in tensorflow? All the tutorials I'm finding use the deprecated seq2seq library.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I managed to get through the tensorflow tutorial by reading the source code. Do you have any references for seq2seq in tensorflow? All the tutorials I&amp;#39;m finding use the deprecated seq2seq library.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgaxus3", "score_hidden": false, "stickied": false, "created": 1492307662.0, "created_utc": 1492278862.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgaxhaz", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "Ciulerson2", "parent_id": "t3_65jwms", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Oh god yes, the official tutorial for tensorflow RNNs is horrible! What really got me going were (among others) r2rt's and wildml's RNN tutorials, you should definately check them out.\n\nAs for the autoencoders, I don't know much about them so I can't really help you, but I think you should be fine using the lower level RNN tensorflow stuff.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh god yes, the official tutorial for tensorflow RNNs is horrible! What really got me going were (among others) r2rt&amp;#39;s and wildml&amp;#39;s RNN tutorials, you should definately check them out.&lt;/p&gt;\n\n&lt;p&gt;As for the autoencoders, I don&amp;#39;t know much about them so I can&amp;#39;t really help you, but I think you should be fine using the lower level RNN tensorflow stuff.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgaxhaz", "score_hidden": false, "stickied": false, "created": 1492307148.0, "created_utc": 1492278348.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": "", "user_reports": [], "id": "dgb0apc", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "RaionTategami", "parent_id": "t1_dgazne6", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "So it's not deprecated, they've just moved the modules for whatever reason.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So it&amp;#39;s not deprecated, they&amp;#39;ve just moved the modules for whatever reason.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb0apc", "score_hidden": false, "stickied": false, "created": 1492311007.0, "created_utc": 1492282207.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgazne6", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "curious_neuron", "parent_id": "t1_dgayzqm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I just found a github issue that confirms my finding: https://github.com/tensorflow/tensorflow/issues/6483 \n\nI would very much like help though. I PMed you and thanks in advance!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just found a github issue that confirms my finding: &lt;a href=\"https://github.com/tensorflow/tensorflow/issues/6483\"&gt;https://github.com/tensorflow/tensorflow/issues/6483&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I would very much like help though. I PMed you and thanks in advance!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgazne6", "score_hidden": false, "stickied": false, "created": 1492310117.0, "created_utc": 1492281317.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": "", "user_reports": [], "id": "dgc09lp", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "curious_neuron", "parent_id": "t1_dgbw5m8", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Unfortunately I can't because I'm doing this for a class project :(. I'll make it public after my course is over and if all goes well.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unfortunately I can&amp;#39;t because I&amp;#39;m doing this for a class project :(. I&amp;#39;ll make it public after my course is over and if all goes well.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgc09lp", "score_hidden": false, "stickied": false, "created": 1492376922.0, "created_utc": 1492348122.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbw5m8", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "shagunsodhani", "parent_id": "t1_dgayzqm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "@curious_neuron: Please share the repo where you are implementing the paper.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;@curious_neuron: Please share the repo where you are implementing the paper.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbw5m8", "score_hidden": false, "stickied": false, "created": 1492365203.0, "created_utc": 1492336403.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgayzqm", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "RaionTategami", "parent_id": "t3_65jwms", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Why do you think it's deprecated? It should be quite easy to implement a VAE in TF. I'll happily help you out. I'm familiar with TFs RNN API. It took a while but actually gotten to a reasonable place recently. If you want my assistance PM me and I'll send you my email.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why do you think it&amp;#39;s deprecated? It should be quite easy to implement a VAE in TF. I&amp;#39;ll happily help you out. I&amp;#39;m familiar with TFs RNN API. It took a while but actually gotten to a reasonable place recently. If you want my assistance PM me and I&amp;#39;ll send you my email.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgayzqm", "score_hidden": false, "stickied": false, "created": 1492309196.0, "created_utc": 1492280396.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": "", "user_reports": [], "id": "dgc0a3t", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "curious_neuron", "parent_id": "t1_dgbu2t0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thanks! I'll check it out.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks! I&amp;#39;ll check it out.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgc0a3t", "score_hidden": false, "stickied": false, "created": 1492376952.0, "created_utc": 1492348152.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbu2t0", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Zhong-I", "parent_id": "t3_65jwms", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I was also working on the implementation of \"Generating Sentences from a Continuous Space\" in TensorFlow. Here's my [repo](https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow). But the code was quite buggy and was written in r0.12. I'm considering transitioning to r1.0, and feed the input in TFRecord format instead of feed_dict. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was also working on the implementation of &amp;quot;Generating Sentences from a Continuous Space&amp;quot; in TensorFlow. Here&amp;#39;s my &lt;a href=\"https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow\"&gt;repo&lt;/a&gt;. But the code was quite buggy and was written in r0.12. I&amp;#39;m considering transitioning to r1.0, and feed the input in TFRecord format instead of feed_dict. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbu2t0", "score_hidden": false, "stickied": false, "created": 1492358284.0, "created_utc": 1492329484.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65jwms", "likes": null, "replies": "", "user_reports": [], "id": "dgda9s8", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "TalkingJellyFish", "parent_id": "t3_65jwms", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I think that since this is an autoencoder you don't need the power of seq2seq. \nseq2seq solves the problem of generating an output with a possible different length than the input. In your case input and output are the same length and belong to the same \"space/language\" \nIn seq2seq, during the decoder phase, they cast each decoder output to the most likely token so that they can check for a EOS token and stop the decoding. You don't need that so you can do something easier\nI think that means you can use a standard rnn as an encoder, and for the decoder - where you'd feed the decoder its own outputs as inputs. \n\nI'm working on something similar [here](https://github.com/talolard/DenseContinuousSentances/blob/master/models/seq2seqvae.py#L50) is a possible example\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think that since this is an autoencoder you don&amp;#39;t need the power of seq2seq. \nseq2seq solves the problem of generating an output with a possible different length than the input. In your case input and output are the same length and belong to the same &amp;quot;space/language&amp;quot; \nIn seq2seq, during the decoder phase, they cast each decoder output to the most likely token so that they can check for a EOS token and stop the decoding. You don&amp;#39;t need that so you can do something easier\nI think that means you can use a standard rnn as an encoder, and for the decoder - where you&amp;#39;d feed the decoder its own outputs as inputs. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on something similar &lt;a href=\"https://github.com/talolard/DenseContinuousSentances/blob/master/models/seq2seqvae.py#L50\"&gt;here&lt;/a&gt; is a possible example&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgda9s8", "score_hidden": false, "stickied": false, "created": 1492447028.0, "created_utc": 1492418228.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]