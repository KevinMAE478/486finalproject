[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to wrap my mind around why TensorFlow uses variable scope. For example, the &lt;a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\"&gt;TensorFlow RNN models&lt;/a&gt; all use scope. I think this means that if I create a LSTM object and then another LSTM object, the two models will share the same weights by default. The only way I can change this behavior is to define separate scopes for each object.&lt;/p&gt;\n\n&lt;p&gt;Does anyone else have experience with this? Why does the scope even exist in the first place? I am trying to implement &lt;a href=\"https://github.com/jostmey/cas/blob/master/RWACell.py\"&gt;my own RNN&lt;/a&gt; model using TensorFlow&amp;#39;s framework, and the scope issue is driving me mad. I am so confused with TensorFlow&amp;#39;s scope I am thinking of switching over to Pytorch, which seems to do a much better job at deploying RNN models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "I am trying to wrap my mind around why TensorFlow uses variable scope. For example, the [TensorFlow RNN models](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py) all use scope. I think this means that if I create a LSTM object and then another LSTM object, the two models will share the same weights by default. The only way I can change this behavior is to define separate scopes for each object.\n\nDoes anyone else have experience with this? Why does the scope even exist in the first place? I am trying to implement [my own RNN](https://github.com/jostmey/cas/blob/master/RWACell.py) model using TensorFlow's framework, and the scope issue is driving me mad. I am so confused with TensorFlow's scope I am thinking of switching over to Pytorch, which seems to do a much better job at deploying RNN models.", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "65kivu", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 27, "report_reasons": null, "author": "jostmey", "saved": false, "mod_reports": [], "name": "t3_65kivu", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/KBY1RwDRTRB9x3myIJcownmQRtiULpfyB2RdtlmHJJY.jpg?s=fcfb7a890774312c053ff940b468b5a7", "width": 322, "height": 322}, "resolutions": [{"url": "https://i.redditmedia.com/KBY1RwDRTRB9x3myIJcownmQRtiULpfyB2RdtlmHJJY.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=0c07169e93c9e3fa2272b7012d9f835c", "width": 108, "height": 108}, {"url": "https://i.redditmedia.com/KBY1RwDRTRB9x3myIJcownmQRtiULpfyB2RdtlmHJJY.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=7c2d846560e4788823d22cd363ba253f", "width": 216, "height": 216}, {"url": "https://i.redditmedia.com/KBY1RwDRTRB9x3myIJcownmQRtiULpfyB2RdtlmHJJY.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=583fbea61c33adeba896c19512f524fc", "width": 320, "height": 320}], "variants": {}, "id": "u-zn8YwxK1Ozn90F1p8DllluDkRCuT_RYH_t9VpmMB4"}], "enabled": false}, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": 1492398308.0, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "self", "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65kivu/d_does_anyone_else_find_tensorflows_variable/", "num_reports": null, "locked": false, "stickied": false, "created": 1492308812.0, "url": "https://www.reddit.com/r/MachineLearning/comments/65kivu/d_does_anyone_else_find_tensorflows_variable/", "author_flair_text": null, "quarantine": false, "title": "[D] Does anyone else find TensorFlow's variable scope as it applies to RNN models confusing?", "created_utc": 1492280012.0, "distinguished": null, "media": null, "upvote_ratio": 0.79, "num_comments": 32, "visited": false, "subreddit_type": "public", "ups": 27}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgbfhz5", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "ebix", "parent_id": "t1_dgazejh", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I actually like variable_scope quite a bit. Important to note that it's not just useful for variable use, but also parameter sharing, setting regularizers and initializers and a context manager makes a lot of sense for this. \n\nDisclaimer: I work at Google on Tensorflow, though not this area. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I actually like variable_scope quite a bit. Important to note that it&amp;#39;s not just useful for variable use, but also parameter sharing, setting regularizers and initializers and a context manager makes a lot of sense for this. &lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I work at Google on Tensorflow, though not this area. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbfhz5", "score_hidden": false, "stickied": false, "created": 1492332193.0, "created_utc": 1492303393.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 7}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgc9g4x", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "jostmey", "parent_id": "t1_dgc5mqj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yeah, until Sonnet supports python3 I won't be using it", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, until Sonnet supports python3 I won&amp;#39;t be using it&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgc9g4x", "score_hidden": false, "stickied": false, "created": 1492390719.0, "created_utc": 1492361919.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dgc5mqj", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "wdroz", "parent_id": "t1_dgb9za5", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yes, but doesn't support Python3...", "edited": 1492358282.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, but doesn&amp;#39;t support Python3...&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgc5mqj", "score_hidden": false, "stickied": false, "created": 1492385635.0, "created_utc": 1492356835.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb9za5", "gilded": 0, "archived": false, "score": 10, "report_reasons": null, "author": "OriolVinyals", "parent_id": "t1_dgazejh", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Sonnet is pretty useful for this particular use case.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sonnet is pretty useful for this particular use case.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb9za5", "score_hidden": false, "stickied": false, "created": 1492324263.0, "created_utc": 1492295463.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 10}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgbtah7", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "mujjingun", "parent_id": "t1_dgazejh", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thanks so much for this info! It makes everything less painful.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks so much for this info! It makes everything less painful.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbtah7", "score_hidden": false, "stickied": false, "created": 1492356057.0, "created_utc": 1492327257.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgazejh", "gilded": 0, "archived": false, "score": 25, "report_reasons": null, "author": "RaionTategami", "parent_id": "t3_65kivu", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Oh my God yes. I *hate* the reuse=True nonsense. So ugly. It's also not clear at all what is shared between different cells.\n\nThankfully there is an little known alternative called tf.make_template. If you wrap a function in this then the first time you call it then it creates the variables but all the other times you call it, it reuses the variables automatically. See a very simple example of this here under \"meta learning\". https://hackernoon.com/learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2\nSee how there's no need to use a variable scope or the reuse parameter in the rnn loop?\n\nThis makes it much easier to follow since you are now passing about functions that represent parts of your model and you reuse variables when you are reusing these templates.\n\nAlternatively there are functions in the API that will unroll the RNN for you and keep track of the sharing for you.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh my God yes. I &lt;em&gt;hate&lt;/em&gt; the reuse=True nonsense. So ugly. It&amp;#39;s also not clear at all what is shared between different cells.&lt;/p&gt;\n\n&lt;p&gt;Thankfully there is an little known alternative called tf.make_template. If you wrap a function in this then the first time you call it then it creates the variables but all the other times you call it, it reuses the variables automatically. See a very simple example of this here under &amp;quot;meta learning&amp;quot;. &lt;a href=\"https://hackernoon.com/learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2\"&gt;https://hackernoon.com/learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2&lt;/a&gt;\nSee how there&amp;#39;s no need to use a variable scope or the reuse parameter in the rnn loop?&lt;/p&gt;\n\n&lt;p&gt;This makes it much easier to follow since you are now passing about functions that represent parts of your model and you reuse variables when you are reusing these templates.&lt;/p&gt;\n\n&lt;p&gt;Alternatively there are functions in the API that will unroll the RNN for you and keep track of the sharing for you.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgazejh", "score_hidden": false, "stickied": false, "created": 1492309776.0, "created_utc": 1492280976.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 25}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "more"}], "after": null, "before": null}}, "user_reports": [], "id": "dgcyrta", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "jostmey", "parent_id": "t1_dgcsz8r", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "That little bit of information has been a big help. I did not realize that the cell is only called once in dynamic_rnn, and then unrolled. It was the source of a lot of my headache.\n\nYes I could create a function in RNNCell that would generate the initial state for dynamic_rnn, call it `generate_initial_state`. I would then pass the scope to `generate_initial_state`. It would only be sensible to check that the scope of `generate_initial_state` matched the scope of `__call__`. Otherwise, a user not familiar with my model would make the mistake of calling `generate_initial_state` outside the same scope :-(", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That little bit of information has been a big help. I did not realize that the cell is only called once in dynamic_rnn, and then unrolled. It was the source of a lot of my headache.&lt;/p&gt;\n\n&lt;p&gt;Yes I could create a function in RNNCell that would generate the initial state for dynamic_rnn, call it &lt;code&gt;generate_initial_state&lt;/code&gt;. I would then pass the scope to &lt;code&gt;generate_initial_state&lt;/code&gt;. It would only be sensible to check that the scope of &lt;code&gt;generate_initial_state&lt;/code&gt; matched the scope of &lt;code&gt;__call__&lt;/code&gt;. Otherwise, a user not familiar with my model would make the mistake of calling &lt;code&gt;generate_initial_state&lt;/code&gt; outside the same scope :-(&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcyrta", "score_hidden": false, "stickied": false, "created": 1492424447.0, "created_utc": 1492395647.0, "depth": 9, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcsz8r", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "RaionTategami", "parent_id": "t1_dgcndcs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yeah dynamic_rnn uses tf.while to unroll the RNN loop dynamically. static_rnn would call your cell each time for each time step. Not sure what your question is but I would suggest sending in your own initial state into dynamic_rnn instead rather than using zero_state.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah dynamic_rnn uses tf.while to unroll the RNN loop dynamically. static_rnn would call your cell each time for each time step. Not sure what your question is but I would suggest sending in your own initial state into dynamic_rnn instead rather than using zero_state.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcsz8r", "score_hidden": false, "stickied": false, "created": 1492416676.0, "created_utc": 1492387876.0, "depth": 8, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgco99q", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "jostmey", "parent_id": "t1_dgcndcs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I fixed up my code. Please let me know if it is correct. I still don't know how to introduce scope in the `zero_state` :-(", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I fixed up my code. Please let me know if it is correct. I still don&amp;#39;t know how to introduce scope in the &lt;code&gt;zero_state&lt;/code&gt; :-(&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgco99q", "score_hidden": false, "stickied": false, "created": 1492410198.0, "created_utc": 1492381398.0, "depth": 8, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcndcs", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "jostmey", "parent_id": "t1_dgck1g0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I did not realize it was only called once! I though it was called at every step in the timeseries. Now that I realize how it really works, I am facing a new issue.\n\nI want the initial state of my RNN to be a parameter that is fitted to the data. The place to do this is `zero_state`. But no scope is passed to `zero_state`", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I did not realize it was only called once! I though it was called at every step in the timeseries. Now that I realize how it really works, I am facing a new issue.&lt;/p&gt;\n\n&lt;p&gt;I want the initial state of my RNN to be a parameter that is fitted to the data. The place to do this is &lt;code&gt;zero_state&lt;/code&gt;. But no scope is passed to &lt;code&gt;zero_state&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcndcs", "score_hidden": false, "stickied": false, "created": 1492409024.0, "created_utc": 1492380224.0, "depth": 7, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgck1g0", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "RaionTategami", "parent_id": "t1_dgchh1n", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You shouldn't see do to anything with reuse in your cell, tf.nn.dynamic_rnn should be taking care of that with you, esp since dynamic_rnn should only call your cell once. Just remove the reuse=True param and the whole try catch block.\n\nWhat error do you see?\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You shouldn&amp;#39;t see do to anything with reuse in your cell, tf.nn.dynamic_rnn should be taking care of that with you, esp since dynamic_rnn should only call your cell once. Just remove the reuse=True param and the whole try catch block.&lt;/p&gt;\n\n&lt;p&gt;What error do you see?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgck1g0", "score_hidden": false, "stickied": false, "created": 1492404646.0, "created_utc": 1492375846.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgchh1n", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "jostmey", "parent_id": "t1_dgcd7ri", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Here is my code:\n\nhttps://github.com/jostmey/cas/blob/master/RWACell.py\n\nI think I am going to have to duplicate the functions '_checked_scope' and '_linear' from TensorFlow's RNN code here:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\n\nIt shouldn't be a lot of work. It just took me a long time to realize I had to do this. Feel free to guide me in a better direction if you can", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here is my code:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/jostmey/cas/blob/master/RWACell.py\"&gt;https://github.com/jostmey/cas/blob/master/RWACell.py&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I think I am going to have to duplicate the functions &amp;#39;_checked_scope&amp;#39; and &amp;#39;_linear&amp;#39; from TensorFlow&amp;#39;s RNN code here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\"&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It shouldn&amp;#39;t be a lot of work. It just took me a long time to realize I had to do this. Feel free to guide me in a better direction if you can&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgchh1n", "score_hidden": false, "stickied": false, "created": 1492401339.0, "created_utc": 1492372539.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcd7ri", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "RaionTategami", "parent_id": "t1_dgccr4e", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Can you tell me what the issues are? Or share some code?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can you tell me what the issues are? Or share some code?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcd7ri", "score_hidden": false, "stickied": false, "created": 1492395694.0, "created_utc": 1492366894.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgccr4e", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "jostmey", "parent_id": "t1_dgcbzhm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I'm sorry, now that I look at it again I see you are right. I'm feeling frustrated with the scope issue. I have an extension of the RNNCell that works in some situtations, but I want to create a RNNCell that works interchangeable with the built in LSTM models and it is proving much harder than I thought", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sorry, now that I look at it again I see you are right. I&amp;#39;m feeling frustrated with the scope issue. I have an extension of the RNNCell that works in some situtations, but I want to create a RNNCell that works interchangeable with the built in LSTM models and it is proving much harder than I thought&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgccr4e", "score_hidden": false, "stickied": false, "created": 1492395074.0, "created_utc": 1492366274.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcbzhm", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "RaionTategami", "parent_id": "t1_dgcb0u8", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Are you talking about the array_ops, clip_ops modules that they are importing? Those are in the public API. The reason they have to do that is because you can't import TF *inside* TF. Everything they do there should be available to you in the public tensorflow module.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you talking about the array_ops, clip_ops modules that they are importing? Those are in the public API. The reason they have to do that is because you can&amp;#39;t import TF &lt;em&gt;inside&lt;/em&gt; TF. Everything they do there should be available to you in the public tensorflow module.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcbzhm", "score_hidden": false, "stickied": false, "created": 1492394056.0, "created_utc": 1492365256.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcb0u8", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "jostmey", "parent_id": "t1_dgazsht", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "When I look at the implementations of the the built in [TensorFlow RNN models](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py), there are several helper functions being used. The helper functions *then* call functions that are not found in the API documentation. The only way I can build my own RNN model using the convention you explained is to call all these functions.\n\nI feel like it is a disaster\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;When I look at the implementations of the the built in &lt;a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\"&gt;TensorFlow RNN models&lt;/a&gt;, there are several helper functions being used. The helper functions &lt;em&gt;then&lt;/em&gt; call functions that are not found in the API documentation. The only way I can build my own RNN model using the convention you explained is to call all these functions.&lt;/p&gt;\n\n&lt;p&gt;I feel like it is a disaster&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcb0u8", "score_hidden": false, "stickied": false, "created": 1492392795.0, "created_utc": 1492363995.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgazsht", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "RaionTategami", "parent_id": "t3_65kivu", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "To actually answer your questions. The variable scope is mostly there for variable sharing but also for giving a heirarchy to variable names. You share variables by giving them exactly the same name, this has to include all the variable scopes above it. AND you have to set reuse=True otherwise you get an error. The default is reuse=None means inherit reuse from the outer scope and reuse=False means throw an error if I try to create a variable with the same name. (Yey for tri-states!) The RNN cells are odd and their behavior have actually changed recently making this even more confusing. You pretty much need to look at the variable list from tf.all_variables to make sure that the ones you expected were created.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;To actually answer your questions. The variable scope is mostly there for variable sharing but also for giving a heirarchy to variable names. You share variables by giving them exactly the same name, this has to include all the variable scopes above it. AND you have to set reuse=True otherwise you get an error. The default is reuse=None means inherit reuse from the outer scope and reuse=False means throw an error if I try to create a variable with the same name. (Yey for tri-states!) The RNN cells are odd and their behavior have actually changed recently making this even more confusing. You pretty much need to look at the variable list from tf.all_variables to make sure that the ones you expected were created.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgazsht", "score_hidden": false, "stickied": false, "created": 1492310312.0, "created_utc": 1492281512.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgcboei", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "RaionTategami", "parent_id": "t1_dgc0as5", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "That does work but has slightly different semantics which can be a problem when used in a larger model. e.g. if you are running the same code again to build your inference graph then it will blow up when you set reuse =True higher up since False will overwrite it and say don't reuse. none would inherit it.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That does work but has slightly different semantics which can be a problem when used in a larger model. e.g. if you are running the same code again to build your inference graph then it will blow up when you set reuse =True higher up since False will overwrite it and say don&amp;#39;t reuse. none would inherit it.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcboei", "score_hidden": false, "stickied": false, "created": 1492393655.0, "created_utc": 1492364855.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgc0as5", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Response777", "parent_id": "t1_dgb03tq", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; And to address your code: I have seen this pattern but it's not the best. The first time you call your cell reuse should be None, and then True. But how do you know it's the first time right? Well you can set reuse in the variable scope above it and this VS will inherit the reuse value. So when you are unrolling your RNN and calling your custom cell you do;\n&gt; \n\nwell, I think\n\n    for step in range(num_rnn_steps):\n        with tf.variable_scope(\"my_lovely_rnn\", reuse = step &gt; 0:\n            out, state = cell(inp, state)\n\nis more concise. While it seems that dynamic_rnn is a better choice nowadays.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;And to address your code: I have seen this pattern but it&amp;#39;s not the best. The first time you call your cell reuse should be None, and then True. But how do you know it&amp;#39;s the first time right? Well you can set reuse in the variable scope above it and this VS will inherit the reuse value. So when you are unrolling your RNN and calling your custom cell you do;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;well, I think&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;for step in range(num_rnn_steps):\n    with tf.variable_scope(&amp;quot;my_lovely_rnn&amp;quot;, reuse = step &amp;gt; 0:\n        out, state = cell(inp, state)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;is more concise. While it seems that dynamic_rnn is a better choice nowadays.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgc0as5", "score_hidden": false, "stickied": false, "created": 1492376989.0, "created_utc": 1492348189.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb03tq", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "RaionTategami", "parent_id": "t3_65kivu", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "And to address your code: I have seen this pattern but it's not the best. The first time you call your cell reuse should be None, and then True. But how do you know it's the first time right? Well you can set reuse in the variable scope above it and this VS will inherit the reuse value. So when you are unrolling your RNN and calling your custom cell you do;\n\n    for i in range(num_rnn_steps):\n        with tf.variable_scope(\"my_lovely_rnn\", reuse=True if i&gt;0 else None):\n            out, state = cell(inp, state)\n\nThat's the usual pattern but I don't like it.\n   ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And to address your code: I have seen this pattern but it&amp;#39;s not the best. The first time you call your cell reuse should be None, and then True. But how do you know it&amp;#39;s the first time right? Well you can set reuse in the variable scope above it and this VS will inherit the reuse value. So when you are unrolling your RNN and calling your custom cell you do;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;for i in range(num_rnn_steps):\n    with tf.variable_scope(&amp;quot;my_lovely_rnn&amp;quot;, reuse=True if i&amp;gt;0 else None):\n        out, state = cell(inp, state)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;That&amp;#39;s the usual pattern but I don&amp;#39;t like it.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb03tq", "score_hidden": false, "stickied": false, "created": 1492310748.0, "created_utc": 1492281948.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgbrgsi", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "IdentifiableParam", "parent_id": "t1_dgb3zbd", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It is possible to use TF without caring about variable scope once you accept a slight graph def increase and write 25 lines of python.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is possible to use TF without caring about variable scope once you accept a slight graph def increase and write 25 lines of python.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbrgsi", "score_hidden": false, "stickied": false, "created": 1492351453.0, "created_utc": 1492322653.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgccl61", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "sbt_", "parent_id": "t1_dgb5oh6", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "While PyTorch's API definitely looks more elegant, some people also care about deployment", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;While PyTorch&amp;#39;s API definitely looks more elegant, some people also care about deployment&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgccl61", "score_hidden": false, "stickied": false, "created": 1492394853.0, "created_utc": 1492366053.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgbliqa", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "sour_losers", "parent_id": "t1_dgbewed", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 1, "body": "&gt; TF ... super fast\n\nNot often seen in the same sentence.\n\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;TF ... super fast&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Not often seen in the same sentence.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbliqa", "score_hidden": false, "stickied": false, "created": 1492340944.0, "created_utc": 1492312144.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbewed", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "SuperFX", "parent_id": "t1_dgb64q7", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You must not be very familiar with TF, as it [uses cuDNN bindings too](https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn) (if you want it to), so I don't think there'll be any difference in performance there. But that's not what I was referring to. What I meant by RNNs are things that actually require a framework like TF or PyTorch, i.e. a situation where you're doing a non-standard thing that requires a framework and not an off-the-shelf solution as in cuDNN. So new unit types, regularization beyond input dropout, etc. That's where the frameworks become interesting and that's where I'm wondering how PyTorch would fair, as TF is super fast in this regard (relatively-speaking; obviously it won't beat cuDNN, but that's a pre-cooked solution that can't be modified). Sorry if I weren't clear.", "edited": 1492302704.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You must not be very familiar with TF, as it &lt;a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn\"&gt;uses cuDNN bindings too&lt;/a&gt; (if you want it to), so I don&amp;#39;t think there&amp;#39;ll be any difference in performance there. But that&amp;#39;s not what I was referring to. What I meant by RNNs are things that actually require a framework like TF or PyTorch, i.e. a situation where you&amp;#39;re doing a non-standard thing that requires a framework and not an off-the-shelf solution as in cuDNN. So new unit types, regularization beyond input dropout, etc. That&amp;#39;s where the frameworks become interesting and that&amp;#39;s where I&amp;#39;m wondering how PyTorch would fair, as TF is super fast in this regard (relatively-speaking; obviously it won&amp;#39;t beat cuDNN, but that&amp;#39;s a pre-cooked solution that can&amp;#39;t be modified). Sorry if I weren&amp;#39;t clear.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbewed", "score_hidden": false, "stickied": false, "created": 1492331321.0, "created_utc": 1492302521.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb64q7", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "sour_losers", "parent_id": "t1_dgb5oh6", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Pytorch RNN performance is actually way better, and there's no reason to believe that it's any worse. Pytorch, like torch, and unlike tensorflow, uses CUDNN bindings for RNNs", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pytorch RNN performance is actually way better, and there&amp;#39;s no reason to believe that it&amp;#39;s any worse. Pytorch, like torch, and unlike tensorflow, uses CUDNN bindings for RNNs&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb64q7", "score_hidden": false, "stickied": false, "created": 1492318886.0, "created_utc": 1492290086.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 7}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb5oh6", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "SuperFX", "parent_id": "t1_dgb3zbd", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "What's pytorch's performance like when it comes to RNNs? It's great to have a clean API but performance is equally important.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s pytorch&amp;#39;s performance like when it comes to RNNs? It&amp;#39;s great to have a clean API but performance is equally important.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb5oh6", "score_hidden": false, "stickied": false, "created": 1492318260.0, "created_utc": 1492289460.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgbb5j5", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "MaxTalanov", "parent_id": "t1_dgb3zbd", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 1, "body": "Or move to an API where variable sharing does not rely on variable_scope, like Keras.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Or move to an API where variable sharing does not rely on variable_scope, like Keras.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbb5j5", "score_hidden": false, "stickied": false, "created": 1492326000.0, "created_utc": 1492297200.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb3zbd", "gilded": 0, "archived": false, "score": 11, "report_reasons": null, "author": "MachinegunX", "parent_id": "t3_65kivu", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "variable_scope is confusing in general, not just for RNNs. It's an API disaster.\n\nUnless you have a large TF codebase you can't get rid of, just move to PyTorch.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;variable_scope is confusing in general, not just for RNNs. It&amp;#39;s an API disaster.&lt;/p&gt;\n\n&lt;p&gt;Unless you have a large TF codebase you can&amp;#39;t get rid of, just move to PyTorch.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb3zbd", "score_hidden": false, "stickied": false, "created": 1492315958.0, "created_utc": 1492287158.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 11}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgbq4bj", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "IdentifiableParam", "parent_id": "t3_65kivu", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I find it very annoying. But there are ways to avoid most of it while still remaining compatible with code that uses variable scopes. Basically make a superclass for all your models where you subclass it and implement some sort of graph building method. The superclass constructor will wrap that user defined method in tf.make_template (and bind the template as a replacement graph building method) and then create a garbage name scope, call the template the first time in the constructor, and then you can ignore things.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I find it very annoying. But there are ways to avoid most of it while still remaining compatible with code that uses variable scopes. Basically make a superclass for all your models where you subclass it and implement some sort of graph building method. The superclass constructor will wrap that user defined method in tf.make_template (and bind the template as a replacement graph building method) and then create a garbage name scope, call the template the first time in the constructor, and then you can ignore things.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbq4bj", "score_hidden": false, "stickied": false, "created": 1492348648.0, "created_utc": 1492319848.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65kivu", "likes": null, "replies": "", "user_reports": [], "id": "dgbhpr1", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "RockyMcNuts", "parent_id": "t3_65kivu", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Scopes clean up TensorBoard graphs ... a scope is a big box and you can zoom in to the individual variables.\n\nI don't know another reason\u2026You don't need to use scopes if you don't like them.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Scopes clean up TensorBoard graphs ... a scope is a big box and you can zoom in to the individual variables.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know another reason\u2026You don&amp;#39;t need to use scopes if you don&amp;#39;t like them.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbhpr1", "score_hidden": false, "stickied": false, "created": 1492335349.0, "created_utc": 1492306549.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}]