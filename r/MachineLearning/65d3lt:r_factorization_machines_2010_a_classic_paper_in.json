[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Research", "id": "65d3lt", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 36, "report_reasons": null, "author": "pmigdal", "saved": false, "mod_reports": [], "name": "t3_65d3lt", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "ismll.uni-hildesheim.de", "hidden": false, "thumbnail": "default", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "three", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65d3lt/r_factorization_machines_2010_a_classic_paper_in/", "num_reports": null, "locked": false, "stickied": false, "created": 1492211790.0, "url": "https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf", "author_flair_text": null, "quarantine": false, "title": "[R] Factorization Machines (2010) - a classic paper in recommender systems", "created_utc": 1492182990.0, "distinguished": null, "media": null, "upvote_ratio": 0.86, "num_comments": 7, "visited": false, "subreddit_type": "public", "ups": 36}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65d3lt", "likes": null, "replies": "", "user_reports": [], "id": "dg99lj8", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "pmigdal", "parent_id": "t3_65d3lt", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "For some context:\n\n* Edwin Chen, [Winning the Netflix Prize: A Summary](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/) (2011)\n* Y Koren, R Bell, C Volinsky, [Matrix factorization techniques for recommender systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf) (2009)\n\nAnd for implementations:\n\n* [libFM](http://www.libfm.org/) by Steffen Rendle (author of the paper)\n* https://github.com/geffy/tffm - in TensorFlow", "edited": 1492194863.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For some context:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Edwin Chen, &lt;a href=\"http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/\"&gt;Winning the Netflix Prize: A Summary&lt;/a&gt; (2011)&lt;/li&gt;\n&lt;li&gt;Y Koren, R Bell, C Volinsky, &lt;a href=\"https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf\"&gt;Matrix factorization techniques for recommender systems&lt;/a&gt; (2009)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;And for implementations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"http://www.libfm.org/\"&gt;libFM&lt;/a&gt; by Steffen Rendle (author of the paper)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/geffy/tffm\"&gt;https://github.com/geffy/tffm&lt;/a&gt; - in TensorFlow&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg99lj8", "score_hidden": false, "stickied": false, "created": 1492212167.0, "created_utc": 1492183367.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65d3lt", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65d3lt", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65d3lt", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65d3lt", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65d3lt", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65d3lt", "likes": null, "replies": "", "user_reports": [], "id": "dgbdo27", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "theophrastzunz", "parent_id": "t1_dgb4kqa", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "There are interpretations of svms as  scale mixture models.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There are interpretations of svms as  scale mixture models.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbdo27", "score_hidden": false, "stickied": false, "created": 1492329558.0, "created_utc": 1492300758.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb4kqa", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "pmigdal", "parent_id": "t1_dgb44o2", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I think we are from different camps of thinking about ML. Mine goes from Bayesian statistics, where log-loss is essentially a max-likelihood model.  \n\nModels which cannot be considered *some* Bayesian models, or generalized to such, feel for me hacky (even if they work well). \nYes, it also goes with decision trees and their ensembles. I do like, and use them, still - they feel hacky.\n\nIn any case - word \"ad hoc\" was not the best one here; and thank you for the provided link!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think we are from different camps of thinking about ML. Mine goes from Bayesian statistics, where log-loss is essentially a max-likelihood model.  &lt;/p&gt;\n\n&lt;p&gt;Models which cannot be considered &lt;em&gt;some&lt;/em&gt; Bayesian models, or generalized to such, feel for me hacky (even if they work well). \nYes, it also goes with decision trees and their ensembles. I do like, and use them, still - they feel hacky.&lt;/p&gt;\n\n&lt;p&gt;In any case - word &amp;quot;ad hoc&amp;quot; was not the best one here; and thank you for the provided link!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb4kqa", "score_hidden": false, "stickied": false, "created": 1492316769.0, "created_utc": 1492287969.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb44o2", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "afireohno", "parent_id": "t1_dgaghyg", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; Hinge loss (which I don't like, because it is ad hoc)\n\nHuh? Just because you don't get well calibrated probabilities out of a model optimizing hinge loss doesn't mean it is ad-hoc. In fact, if what one is interested in is estimating decision boundaries, i.e., classification, there are cases when using a loss that tries to match conditional probabilities exactly (like log loss) can produce incorrect decision boundaries. See [here](http://yaroslavvb.blogspot.com/2007/06/log-loss-or-hinge-loss.html) for an example plus discussion.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Hinge loss (which I don&amp;#39;t like, because it is ad hoc)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Huh? Just because you don&amp;#39;t get well calibrated probabilities out of a model optimizing hinge loss doesn&amp;#39;t mean it is ad-hoc. In fact, if what one is interested in is estimating decision boundaries, i.e., classification, there are cases when using a loss that tries to match conditional probabilities exactly (like log loss) can produce incorrect decision boundaries. See &lt;a href=\"http://yaroslavvb.blogspot.com/2007/06/log-loss-or-hinge-loss.html\"&gt;here&lt;/a&gt; for an example plus discussion.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb44o2", "score_hidden": false, "stickied": false, "created": 1492316158.0, "created_utc": 1492287358.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgaghyg", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "pmigdal", "parent_id": "t1_dga11wu", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "In the face of Factorization Machines the name is inspired not by [Hinge loss](https://en.wikipedia.org/wiki/Hinge_loss) (which I don't like, [because it is ad hoc](https://www.reddit.com/r/MachineLearning/comments/63szps/p_probability_calibration/)), but by the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method).", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In the face of Factorization Machines the name is inspired not by &lt;a href=\"https://en.wikipedia.org/wiki/Hinge_loss\"&gt;Hinge loss&lt;/a&gt; (which I don&amp;#39;t like, &lt;a href=\"https://www.reddit.com/r/MachineLearning/comments/63szps/p_probability_calibration/\"&gt;because it is ad hoc&lt;/a&gt;), but by the &lt;a href=\"https://en.wikipedia.org/wiki/Kernel_method\"&gt;kernel trick&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgaghyg", "score_hidden": false, "stickied": false, "created": 1492273753.0, "created_utc": 1492244953.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dga11wu", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "gabrielgoh", "parent_id": "t1_dg9yz8x", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "it started with support vector machines - and spilled over to every model which uses the hinge loss, a hinge of any kind, and finally any model without a log in it.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;it started with support vector machines - and spilled over to every model which uses the hinge loss, a hinge of any kind, and finally any model without a log in it.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dga11wu", "score_hidden": false, "stickied": false, "created": 1492245844.0, "created_utc": 1492217044.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9yz8x", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "apliens", "parent_id": "t3_65d3lt", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "A slight aside: can someone explain the use of 'machine' in model naming convention? As I was typing this, I realized that this is a relatively absurd question for a field literally called *machine* learning. However, calling your model a machine sounds antiquated.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A slight aside: can someone explain the use of &amp;#39;machine&amp;#39; in model naming convention? As I was typing this, I realized that this is a relatively absurd question for a field literally called &lt;em&gt;machine&lt;/em&gt; learning. However, calling your model a machine sounds antiquated.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9yz8x", "score_hidden": false, "stickied": false, "created": 1492243091.0, "created_utc": 1492214291.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}]