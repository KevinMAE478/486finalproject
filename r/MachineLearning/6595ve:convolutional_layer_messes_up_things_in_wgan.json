[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m kind of pulling my hair over this problem and I&amp;#39;ve been trying to pin it down for quite some time.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;; Introduction of a convolutional layer in the critic of WGAN architecture seems to mess things up. Fully connected WGAN works, \nconvolutional layers in generator work, autoencoder with an almost identical convolutional architecture works (except it&amp;#39;s reversed). Something strange is happening and I need your superpowers.\nI&amp;#39;m using Tensorflow and code is available &lt;a href=\"https://github.com/bgavran/AMDS_FER/tree/master/src\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Not TLDR;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to play around with WGAN for quite some time to generate faces. I first started implementing the deep convolutional version right away, but since nothing worked like it should I decided to incrementally build up the model.\nI&amp;#39;ve actually implemented the Improved WGAN paper but its just because I thought it&amp;#39;d help me fix the problems I developed (it didn&amp;#39;t). So both methods of enforcing the Lipschitz constraint have the same problem which means that&amp;#39;s &lt;em&gt;probably&lt;/em&gt; not it.&lt;/p&gt;\n\n&lt;p&gt;I ended up creating a functional version of basic WGAN which has one fully connected layer in the generator and two fc layers in the critic. After just a short time, it generates wonderful images like you can see &lt;a href=\"http://imgur.com/a/wwftw\"&gt;here&lt;/a&gt; (the image on the left side, the right one is the problematic one).\nHowever, after replacing the:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;        image = tf.reshape(image, [-1, self.img_size * self.img_size * 3])\n        image = tf.layers.dense(image, 512, tf.nn.relu)\n        image = tf.layers.dense(image, 1)\n        return image\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;with: &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;        image = tf.layers.conv2d(image, filters=128, **kwargs)\n        image = tf.reshape(image, [-1, 16 * 16 * 128])\n        image = tf.layers.dense(image, 1)\n        return image\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;all hell broke loose.\nHere the **kwargs are&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;        kwargs = {&amp;quot;kernel_size&amp;quot;: (4, 4), &amp;quot;strides&amp;quot;: (4, 4), &amp;quot;padding&amp;quot;: &amp;quot;valid&amp;quot;, &amp;quot;activation&amp;quot;: tf.nn.relu}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The image created is the right one in the previous link and &lt;a href=\"http://imgur.com/a/Px74B\"&gt;here&amp;#39;s&lt;/a&gt; one more picture for reference.\nWhat you can notice straight away is that the general structure of the image is all right (it resembles the face), but the low level textures (4x4 patches) are completely messed up. To me this is a clear indicator that the problem has strictly to do with convolution. Adding more convolutional layers just disguises the problem even more. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve tried:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A big number of architectures, number of filters, numbers of layers, padding, activation functions&lt;/li&gt;\n&lt;li&gt;Letting it train for a longer time with many of the above architectures. The results always take longer and look worse than FC&lt;/li&gt;\n&lt;li&gt;Creating an convolutional autoencoder which I train normally (MSE error) and which has the &amp;quot;encoder as the discriminator and decoder as the critic&amp;quot;. What I mean by that is that the architectures of those are very similar and the autoencoder works flawlessly.&lt;/li&gt;\n&lt;li&gt;Some potentially useful hint: I get a NaN in tensorboard visualization and a completely red image when I have a low number of filters in IWGAN (never had the problem with regular WGAN)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I hope I&amp;#39;m not missing something obvious.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "So I'm kind of pulling my hair over this problem and I've been trying to pin it down for quite some time.\n\n**TLDR**; Introduction of a convolutional layer in the critic of WGAN architecture seems to mess things up. Fully connected WGAN works, \nconvolutional layers in generator work, autoencoder with an almost identical convolutional architecture works (except it's reversed). Something strange is happening and I need your superpowers.\nI'm using Tensorflow and code is available [here](https://github.com/bgavran/AMDS_FER/tree/master/src).\n\n**Not TLDR;**\n\nI'm trying to play around with WGAN for quite some time to generate faces. I first started implementing the deep convolutional version right away, but since nothing worked like it should I decided to incrementally build up the model.\nI've actually implemented the Improved WGAN paper but its just because I thought it'd help me fix the problems I developed (it didn't). So both methods of enforcing the Lipschitz constraint have the same problem which means that's *probably* not it.\n\nI ended up creating a functional version of basic WGAN which has one fully connected layer in the generator and two fc layers in the critic. After just a short time, it generates wonderful images like you can see [here](http://imgur.com/a/wwftw) (the image on the left side, the right one is the problematic one).\nHowever, after replacing the:\n\n            image = tf.reshape(image, [-1, self.img_size * self.img_size * 3])\n            image = tf.layers.dense(image, 512, tf.nn.relu)\n            image = tf.layers.dense(image, 1)\n            return image\n\n\nwith: \n\n            image = tf.layers.conv2d(image, filters=128, **kwargs)\n            image = tf.reshape(image, [-1, 16 * 16 * 128])\n            image = tf.layers.dense(image, 1)\n            return image\n\nall hell broke loose.\nHere the **kwargs are\n\n            kwargs = {\"kernel_size\": (4, 4), \"strides\": (4, 4), \"padding\": \"valid\", \"activation\": tf.nn.relu}\n\nThe image created is the right one in the previous link and [here's](http://imgur.com/a/Px74B) one more picture for reference.\nWhat you can notice straight away is that the general structure of the image is all right (it resembles the face), but the low level textures (4x4 patches) are completely messed up. To me this is a clear indicator that the problem has strictly to do with convolution. Adding more convolutional layers just disguises the problem even more. \n\n**What I've tried:**\n\n* A big number of architectures, number of filters, numbers of layers, padding, activation functions\n* Letting it train for a longer time with many of the above architectures. The results always take longer and look worse than FC\n* Creating an convolutional autoencoder which I train normally (MSE error) and which has the \"encoder as the discriminator and decoder as the critic\". What I mean by that is that the architectures of those are very similar and the autoencoder works flawlessly.\n* Some potentially useful hint: I get a NaN in tensorboard visualization and a completely red image when I have a low number of filters in IWGAN (never had the problem with regular WGAN)\n\nI hope I'm not missing something obvious.\n", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": null, "id": "6595ve", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 9, "report_reasons": null, "author": "warmsnail", "saved": false, "mod_reports": [], "name": "t3_6595ve", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/XibWnYazV80V9Fj6lgOfxIzruvgh8ymJ7WKN05Rfw4A.jpg?s=0e66d72e28ffb25aa1a2d0e2b35e08da", "width": 805, "height": 465}, "resolutions": [{"url": "https://i.redditmedia.com/XibWnYazV80V9Fj6lgOfxIzruvgh8ymJ7WKN05Rfw4A.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=6b8164e816ac4241dd683042490c5b3d", "width": 108, "height": 62}, {"url": "https://i.redditmedia.com/XibWnYazV80V9Fj6lgOfxIzruvgh8ymJ7WKN05Rfw4A.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=c6448526badbcfcae269d65dc761690d", "width": 216, "height": 124}, {"url": "https://i.redditmedia.com/XibWnYazV80V9Fj6lgOfxIzruvgh8ymJ7WKN05Rfw4A.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=26ee2a4a396d2aae93e60c9ff49482c9", "width": 320, "height": 184}, {"url": "https://i.redditmedia.com/XibWnYazV80V9Fj6lgOfxIzruvgh8ymJ7WKN05Rfw4A.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=ab9ce4920bc7d1b0d55655c7212892cc", "width": 640, "height": 369}], "variants": {}, "id": "0YRMfKkhSZDbH-dTWv-EoexbXZGAdZjlzm6F66XWvtE"}], "enabled": false}, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": null, "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "self", "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/6595ve/convolutional_layer_messes_up_things_in_wgan/", "num_reports": null, "locked": false, "stickied": false, "created": 1492155467.0, "url": "https://www.reddit.com/r/MachineLearning/comments/6595ve/convolutional_layer_messes_up_things_in_wgan/", "author_flair_text": null, "quarantine": false, "title": "Convolutional layer messes up things in WGAN", "created_utc": 1492126667.0, "distinguished": null, "media": null, "upvote_ratio": 1.0, "num_comments": 19, "visited": false, "subreddit_type": "public", "ups": 9}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": "", "user_reports": [], "id": "dgayupe", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "MarioYC", "parent_id": "t1_dgaxdg0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Sure, it's here: https://github.com/marioyc/gans-eval/blob/master/wgan.py", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sure, it&amp;#39;s here: &lt;a href=\"https://github.com/marioyc/gans-eval/blob/master/wgan.py\"&gt;https://github.com/marioyc/gans-eval/blob/master/wgan.py&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgayupe", "score_hidden": false, "stickied": false, "created": 1492309003.0, "created_utc": 1492280203.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": "", "user_reports": [], "id": "dgbyiht", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "MarioYC", "parent_id": "t1_dgaxdg0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Added the same initalization you used and it seems to work much better already http://imgur.com/r0rZ0O9\nAlso seems like it really needed that sqrt(3) factor, didn't change much without it", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Added the same initalization you used and it seems to work much better already &lt;a href=\"http://imgur.com/r0rZ0O9\"&gt;http://imgur.com/r0rZ0O9&lt;/a&gt;\nAlso seems like it really needed that sqrt(3) factor, didn&amp;#39;t change much without it&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbyiht", "score_hidden": false, "stickied": false, "created": 1492372745.0, "created_utc": 1492343945.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgaxdg0", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "__ishaan", "parent_id": "t1_dgawhem", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Just tried running the code at https://github.com/igul222/improved_wgan_training/blob/master/gan_toy.py and it seems to converge nicely out of the box after about 5 minutes of training. Here it is at 15,000 generator iterations: http://imgur.com/a/SKMAe\n\nIf you can put your code on GitHub I'd be curious to take a look and see if there's any differences I can find that might be breaking things.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just tried running the code at &lt;a href=\"https://github.com/igul222/improved_wgan_training/blob/master/gan_toy.py\"&gt;https://github.com/igul222/improved_wgan_training/blob/master/gan_toy.py&lt;/a&gt; and it seems to converge nicely out of the box after about 5 minutes of training. Here it is at 15,000 generator iterations: &lt;a href=\"http://imgur.com/a/SKMAe\"&gt;http://imgur.com/a/SKMAe&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you can put your code on GitHub I&amp;#39;d be curious to take a look and see if there&amp;#39;s any differences I can find that might be breaking things.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgaxdg0", "score_hidden": false, "stickied": false, "created": 1492307008.0, "created_utc": 1492278208.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgawhem", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "MarioYC", "parent_id": "t1_dgaunj1", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Good to know, in that case it should be a bug or something I'm missing on my implementation.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Good to know, in that case it should be a bug or something I&amp;#39;m missing on my implementation.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgawhem", "score_hidden": false, "stickied": false, "created": 1492305800.0, "created_utc": 1492277000.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgaunj1", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "__ishaan", "parent_id": "t1_dgao6oa", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Interesting! We did try and it worked fine for us, but I guess something must have gotten broken when we cleaned up the code for release. I'll look into this today. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting! We did try and it worked fine for us, but I guess something must have gotten broken when we cleaned up the code for release. I&amp;#39;ll look into this today. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgaunj1", "score_hidden": false, "stickied": false, "created": 1492303284.0, "created_utc": 1492274484.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgao6oa", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "MarioYC", "parent_id": "t1_dg9m00j", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I wonder if you've tested the 8 gaussians in a circle that the authors of the unrolled GAN tested on. I gave it a try with the same architecture/hyperparms as those is wgan_toy and I get this http://imgur.com/JmkiK3X which is not bad, but still far from the true distribution.\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I wonder if you&amp;#39;ve tested the 8 gaussians in a circle that the authors of the unrolled GAN tested on. I gave it a try with the same architecture/hyperparms as those is wgan_toy and I get this &lt;a href=\"http://imgur.com/JmkiK3X\"&gt;http://imgur.com/JmkiK3X&lt;/a&gt; which is not bad, but still far from the true distribution.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgao6oa", "score_hidden": false, "stickied": false, "created": 1492293744.0, "created_utc": 1492264944.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": "", "user_reports": [], "id": "dga08dn", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "__ishaan", "parent_id": "t1_dg9trfq", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; It's very interesting what you say on the second point because I've been wondering the same thing myself. To my understanding, each sample from the batch gets associated one scalar value of regularization, which is lambda multiplied by the squared difference of L2 norm and 1.\n\nThis is accurate. The reason we only sum along axis 1 in our implementation is because our generator flattens the output to shape [batch size, 3\\*64\\*64] before returning it (and likewise for the true data). Since this doesn't seem to be the case with your implementation, you should sum along axes 1,2,3.\n\n&gt; And about eta, shouldn't it be practically the same? Since a new eta gets generated every step, the sampling should cover the whole range of values (albeit, perhaps resulting in a bit worse gradient estimation).\n\nYou're right in theory, but the extra variance in the gradient is actually quite large and in practice this performs substantially worse.", "edited": 1492216222.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;It&amp;#39;s very interesting what you say on the second point because I&amp;#39;ve been wondering the same thing myself. To my understanding, each sample from the batch gets associated one scalar value of regularization, which is lambda multiplied by the squared difference of L2 norm and 1.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is accurate. The reason we only sum along axis 1 in our implementation is because our generator flattens the output to shape [batch size, 3*64*64] before returning it (and likewise for the true data). Since this doesn&amp;#39;t seem to be the case with your implementation, you should sum along axes 1,2,3.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;And about eta, shouldn&amp;#39;t it be practically the same? Since a new eta gets generated every step, the sampling should cover the whole range of values (albeit, perhaps resulting in a bit worse gradient estimation).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You&amp;#39;re right in theory, but the extra variance in the gradient is actually quite large and in practice this performs substantially worse.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dga08dn", "score_hidden": false, "stickied": false, "created": 1492244755.0, "created_utc": 1492215955.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9trfq", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "warmsnail", "parent_id": "t1_dg9m00j", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thanks a lot for such detailed inspection!\n\nIt's very interesting what you say on the second point because I've been wondering the same thing myself. To my understanding, each sample from the batch gets associated one scalar value of regularization, which is lambda multiplied by the squared difference of L2 norm and 1.\n\nHowever, I noticed that in [your implementation](https://github.com/igul222/improved_wgan_training/blob/master/gan_64x64.py) you summed only along the axis 1. Taking into account that I had a million unknowns in my model, I decided to postpone studying your implementation and just went along with it :).\n\nAnd about eta, shouldn't it be practically the same? Since a new eta gets generated every step, the sampling should cover the whole range of values (albeit, perhaps resulting in a bit worse gradient estimation).\n\nI'll keep tuning the architecture and see what I can get!\n\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks a lot for such detailed inspection!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s very interesting what you say on the second point because I&amp;#39;ve been wondering the same thing myself. To my understanding, each sample from the batch gets associated one scalar value of regularization, which is lambda multiplied by the squared difference of L2 norm and 1.&lt;/p&gt;\n\n&lt;p&gt;However, I noticed that in &lt;a href=\"https://github.com/igul222/improved_wgan_training/blob/master/gan_64x64.py\"&gt;your implementation&lt;/a&gt; you summed only along the axis 1. Taking into account that I had a million unknowns in my model, I decided to postpone studying your implementation and just went along with it :).&lt;/p&gt;\n\n&lt;p&gt;And about eta, shouldn&amp;#39;t it be practically the same? Since a new eta gets generated every step, the sampling should cover the whole range of values (albeit, perhaps resulting in a bit worse gradient estimation).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll keep tuning the architecture and see what I can get!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9trfq", "score_hidden": false, "stickied": false, "created": 1492236140.0, "created_utc": 1492207340.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9m00j", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "__ishaan", "parent_id": "t3_6595ve", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "'Improved WGAN' author here. I found a few potential problems in your implementation:\n\n- wgan.py:30, self.eta should have shape (batch size, 1, 1, 1)\n- wgan.py:34, you should be summing along axes 1, 2, and 3\n- wgan.py:72, we don't usually overtrain the critic every 500 steps (I realize the vanilla WGAN implementation does this, so it's a little confusing)\n\nOther than that, I agree with the other comments that a more standard critic architecture will probably be helpful, but I'd expect this architecture should at least converge reliably (even if the faces don't look great). It's expensive, but one trick to get basically anything to converge is to increase c_times (wgan.py:25) until it works. 5 works for most things, 10 helps on harder problems, 100 has yet to fail for me on anything at all. This works because the theory requires that the critic is trained to optimality at each step.\n\nAlong the same lines, increasing the capacity of the critic (relative to the generator) helps with stability quite a bit. In normal GANs, you usually want the generator and the critic to have roughly the same capacity. In WGANs, it's generally best to make the critic as big and powerful (and train it for as long) as you can afford.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#39;Improved WGAN&amp;#39; author here. I found a few potential problems in your implementation:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;wgan.py:30, self.eta should have shape (batch size, 1, 1, 1)&lt;/li&gt;\n&lt;li&gt;wgan.py:34, you should be summing along axes 1, 2, and 3&lt;/li&gt;\n&lt;li&gt;wgan.py:72, we don&amp;#39;t usually overtrain the critic every 500 steps (I realize the vanilla WGAN implementation does this, so it&amp;#39;s a little confusing)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Other than that, I agree with the other comments that a more standard critic architecture will probably be helpful, but I&amp;#39;d expect this architecture should at least converge reliably (even if the faces don&amp;#39;t look great). It&amp;#39;s expensive, but one trick to get basically anything to converge is to increase c_times (wgan.py:25) until it works. 5 works for most things, 10 helps on harder problems, 100 has yet to fail for me on anything at all. This works because the theory requires that the critic is trained to optimality at each step.&lt;/p&gt;\n\n&lt;p&gt;Along the same lines, increasing the capacity of the critic (relative to the generator) helps with stability quite a bit. In normal GANs, you usually want the generator and the critic to have roughly the same capacity. In WGANs, it&amp;#39;s generally best to make the critic as big and powerful (and train it for as long) as you can afford.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9m00j", "score_hidden": false, "stickied": false, "created": 1492226579.0, "created_utc": 1492197779.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": "", "user_reports": [], "id": "dgawge9", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "warmsnail", "parent_id": "t1_dg9tmk4", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I still don't understand where the checkerboard artifacts stem from.\n\n&gt; Each 4x4 patch is independent of one another, and within the patch the pixels should look like some weighted combination of the filters in the first conv layer.\n\nYou're saying that the pixels in the conv layer should look like a weighted combination of the filters in the first conv layer (the RGB image). I guess you're saying its not one, but 8192 weighted combinations of those pixels.\n\nFrom that follows that the 4x4 weighted combinations can't fully exploit the information in the image, ie. the 8192 weighted combinations are far more than useful to capture the all possible useful combinations of those 4x4 pixels.\n\nI also understand that I should see 4x4 sets of patterns in the image, but I don't get it why \n&gt;  you see the exact same pattern tiled in each of those patches (with only slight variations) and why they're grouped into 4x4 sets. \n\nWhy should it be the exact same pattern? There's 8192 channels, more than enough to capture all possible edges and Gabor filters and whatnot.\n\nAnd from where does it follow there's checkerboard artifacts? The FC layer supplies it with spatially variant signal, yes, but shouldn't those 8192 channels be able to use that lower resolution signal to transform it into smooth edges?\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I still don&amp;#39;t understand where the checkerboard artifacts stem from.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Each 4x4 patch is independent of one another, and within the patch the pixels should look like some weighted combination of the filters in the first conv layer.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You&amp;#39;re saying that the pixels in the conv layer should look like a weighted combination of the filters in the first conv layer (the RGB image). I guess you&amp;#39;re saying its not one, but 8192 weighted combinations of those pixels.&lt;/p&gt;\n\n&lt;p&gt;From that follows that the 4x4 weighted combinations can&amp;#39;t fully exploit the information in the image, ie. the 8192 weighted combinations are far more than useful to capture the all possible useful combinations of those 4x4 pixels.&lt;/p&gt;\n\n&lt;p&gt;I also understand that I should see 4x4 sets of patterns in the image, but I don&amp;#39;t get it why &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;you see the exact same pattern tiled in each of those patches (with only slight variations) and why they&amp;#39;re grouped into 4x4 sets. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Why should it be the exact same pattern? There&amp;#39;s 8192 channels, more than enough to capture all possible edges and Gabor filters and whatnot.&lt;/p&gt;\n\n&lt;p&gt;And from where does it follow there&amp;#39;s checkerboard artifacts? The FC layer supplies it with spatially variant signal, yes, but shouldn&amp;#39;t those 8192 channels be able to use that lower resolution signal to transform it into smooth edges?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgawge9", "score_hidden": false, "stickied": false, "created": 1492305761.0, "created_utc": 1492276961.0, "depth": 7, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9tmk4", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "ajmooch", "parent_id": "t1_dg9soey", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "That's how it's explained in the distill post, but the reason they show up in your extreme-strided case is because the signal supplied to the individual elements of your output is basically a single number dotted with your kernel at each patch of 4 pixels. Checkerboard artifacts are absolutely dependent on the discriminator architecture, not just the generator.\n\nConsider the first 4x4 patch in the upper left hand corner of your image output X. You have a kernel of stride 4 and size 4, which means that the top left output of the convolutional layer is basically the dot product of your kernel with those 16 pixels. Moving over one step moves you 4 pixels in X, which means that the pixels in that top-left 4x4 patch are only passed through to the output via that single dot product. On the backward pass, then, that 4x4 patch receives a signal that is some scalar sumproduct of the conv weights. \n\nWhat this results in is a learning signal that tells the generator, \"Each 4x4 patch is independent of one another, and within the patch the pixels should look like some weighted combination of the filters in the first conv layer.\" This is why you see the exact same pattern tiled in each of those patches (with only slight variations) and why they're grouped into 4x4 sets. The FC layer supplies it with some spatially-variant signal (hence why the basic facial attributes sort of show up) that determines how the sum varies across patches.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s how it&amp;#39;s explained in the distill post, but the reason they show up in your extreme-strided case is because the signal supplied to the individual elements of your output is basically a single number dotted with your kernel at each patch of 4 pixels. Checkerboard artifacts are absolutely dependent on the discriminator architecture, not just the generator.&lt;/p&gt;\n\n&lt;p&gt;Consider the first 4x4 patch in the upper left hand corner of your image output X. You have a kernel of stride 4 and size 4, which means that the top left output of the convolutional layer is basically the dot product of your kernel with those 16 pixels. Moving over one step moves you 4 pixels in X, which means that the pixels in that top-left 4x4 patch are only passed through to the output via that single dot product. On the backward pass, then, that 4x4 patch receives a signal that is some scalar sumproduct of the conv weights. &lt;/p&gt;\n\n&lt;p&gt;What this results in is a learning signal that tells the generator, &amp;quot;Each 4x4 patch is independent of one another, and within the patch the pixels should look like some weighted combination of the filters in the first conv layer.&amp;quot; This is why you see the exact same pattern tiled in each of those patches (with only slight variations) and why they&amp;#39;re grouped into 4x4 sets. The FC layer supplies it with some spatially-variant signal (hence why the basic facial attributes sort of show up) that determines how the sum varies across patches.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9tmk4", "score_hidden": false, "stickied": false, "created": 1492235966.0, "created_utc": 1492207166.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9soey", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "warmsnail", "parent_id": "t1_dg94z5z", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I'm not sure what you mean by the checkerboard patterns stemming from the discriminator. \n\nTo my understanding, their generation stems from any part of the network that has overlapping kernels in convolution.\n&gt;     Ironically, you can see an extreme form of this in your 4-strided conv results, where you get patches of 4 pixels that have independent structure; checkerboard artifacts also occur if you use dilated convs in the discriminator.\n\nI guess I can see the patterns, but I don't understand your description of the actual reason of their generation.\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure what you mean by the checkerboard patterns stemming from the discriminator. &lt;/p&gt;\n\n&lt;p&gt;To my understanding, their generation stems from any part of the network that has overlapping kernels in convolution.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;pre&gt;&lt;code&gt;Ironically, you can see an extreme form of this in your 4-strided conv results, where you get patches of 4 pixels that have independent structure; checkerboard artifacts also occur if you use dilated convs in the discriminator.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I guess I can see the patterns, but I don&amp;#39;t understand your description of the actual reason of their generation.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9soey", "score_hidden": false, "stickied": false, "created": 1492234747.0, "created_utc": 1492205947.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg94z5z", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "ajmooch", "parent_id": "t1_dg93d8m", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt;It is super slow, though.\n\nWelcome to DL! There's a lot of details involved in designing these kinds of architectures and it takes  a lot of tinkering and experimenting to wrap your head around the details and gain the appropriate intuition.\n\nJust one point: while checkerboard artifacts can stem from using solely transposed convs in the generator, they do also stem from the discriminator (I've been meaning to write up a rebuttal demonstrating this). Ironically, you can see an extreme form of this in your 4-strided conv results, where you get patches of 4 pixels that have independent structure; checkerboard artifacts also occur if you use dilated convs in the discriminator.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;It is super slow, though.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Welcome to DL! There&amp;#39;s a lot of details involved in designing these kinds of architectures and it takes  a lot of tinkering and experimenting to wrap your head around the details and gain the appropriate intuition.&lt;/p&gt;\n\n&lt;p&gt;Just one point: while checkerboard artifacts can stem from using solely transposed convs in the generator, they do also stem from the discriminator (I&amp;#39;ve been meaning to write up a rebuttal demonstrating this). Ironically, you can see an extreme form of this in your 4-strided conv results, where you get patches of 4 pixels that have independent structure; checkerboard artifacts also occur if you use dilated convs in the discriminator.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg94z5z", "score_hidden": false, "stickied": false, "created": 1492206058.0, "created_utc": 1492177258.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dg93d8m", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "warmsnail", "parent_id": "t1_dg90g5x", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thanks so much for the answer, I think I understand it now.\n\nIn short, the problem is that the capacity of the convolutional layer is drastically lower than the FC one. Changing the model to a FC one with less hidden units or to a convolutional layer with crazy number of filters solves it.\n\n---\n\nIf my method of parameter counting is correct, the FC model from my example had around 6.3 million parameters (64^2 \\* 3 \\* 512 + 512) while the convolutional one had only 39k ((4^2 + 1)\\*3 \\* 128 + 16^2 \\* 128). By solving for filter size, it turns out that it should have 40 times as much filters as there are units in the hidden layer of my FC critic in order to have the same number of parameters, which *should be?* equal to the representational power.\n\nAs I said, my goal isn't just to have one convolutional layer, but it is to figure out why my WGAN isn't working (which, it seems, might be just because there is a low number of parameters in the model). I think of the convolutional layer the same way as the FC layer, that is, it's a linear transformation followed by bias and a nonlinearity. The only difference between the conv layer and FC layer seems to be the reason why the example above didn't work:  some of the weights are tied together, which brings down the parameter count.\n\nI never tried the original DCGAN because of the same reason why I don't think trying strides of size one seems like a good idea: [checkerboard artifacts](http://distill.pub/2016/deconv-checkerboard/).\n\nAll in all, I'm currently training the convolutional critic from my original post with 8192 filters, just to see if the above explanation is correct and it seems that it is: [new generated images](http://imgur.com/a/PRezF).\n\nIt is super slow, though.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks so much for the answer, I think I understand it now.&lt;/p&gt;\n\n&lt;p&gt;In short, the problem is that the capacity of the convolutional layer is drastically lower than the FC one. Changing the model to a FC one with less hidden units or to a convolutional layer with crazy number of filters solves it.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;If my method of parameter counting is correct, the FC model from my example had around 6.3 million parameters (64&lt;sup&gt;2&lt;/sup&gt; * 3 * 512 + 512) while the convolutional one had only 39k ((4&lt;sup&gt;2&lt;/sup&gt; + 1)*3 * 128 + 16&lt;sup&gt;2&lt;/sup&gt; * 128). By solving for filter size, it turns out that it should have 40 times as much filters as there are units in the hidden layer of my FC critic in order to have the same number of parameters, which &lt;em&gt;should be?&lt;/em&gt; equal to the representational power.&lt;/p&gt;\n\n&lt;p&gt;As I said, my goal isn&amp;#39;t just to have one convolutional layer, but it is to figure out why my WGAN isn&amp;#39;t working (which, it seems, might be just because there is a low number of parameters in the model). I think of the convolutional layer the same way as the FC layer, that is, it&amp;#39;s a linear transformation followed by bias and a nonlinearity. The only difference between the conv layer and FC layer seems to be the reason why the example above didn&amp;#39;t work:  some of the weights are tied together, which brings down the parameter count.&lt;/p&gt;\n\n&lt;p&gt;I never tried the original DCGAN because of the same reason why I don&amp;#39;t think trying strides of size one seems like a good idea: &lt;a href=\"http://distill.pub/2016/deconv-checkerboard/\"&gt;checkerboard artifacts&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;All in all, I&amp;#39;m currently training the convolutional critic from my original post with 8192 filters, just to see if the above explanation is correct and it seems that it is: &lt;a href=\"http://imgur.com/a/PRezF\"&gt;new generated images&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;It is super slow, though.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg93d8m", "score_hidden": false, "stickied": false, "created": 1492203434.0, "created_utc": 1492174634.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg90g5x", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "ajmooch", "parent_id": "t1_dg907i1", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "FC layers operate very differently from conv layers, and while they're drastically less parametrically efficient, they do have a lot of representational power. Don't expect a single convolutional layer to be able to learn the same things a single FC layer can--again, not that an FC layer is desirable in practice, just that a single conv layer with small filters can only learn local relationships, and having that bottleneck in the backward flow of the signal provided by the critic messes with the whole unsteady shenanigan that is GAN training. \n\nLooking at your code a little more, the main issue that I really see is that you're using a single convolutional layer with kernel size 4 and stride 4, meaning that you're downsampling the hell out of the image. This is what results in the right side image having very clear blocks of 4 pixels--G is getting signals from C that are tiled based on the extreme stride and the small kernel. If you really want to use a shallow architecture for some reason, try using a single conv layer with kernel size 4 or 5 and stride *1* followed your FC layer. I'd expect that to at least not give you demon-spawn, though I doubt you'll see HQ results.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;FC layers operate very differently from conv layers, and while they&amp;#39;re drastically less parametrically efficient, they do have a lot of representational power. Don&amp;#39;t expect a single convolutional layer to be able to learn the same things a single FC layer can--again, not that an FC layer is desirable in practice, just that a single conv layer with small filters can only learn local relationships, and having that bottleneck in the backward flow of the signal provided by the critic messes with the whole unsteady shenanigan that is GAN training. &lt;/p&gt;\n\n&lt;p&gt;Looking at your code a little more, the main issue that I really see is that you&amp;#39;re using a single convolutional layer with kernel size 4 and stride 4, meaning that you&amp;#39;re downsampling the hell out of the image. This is what results in the right side image having very clear blocks of 4 pixels--G is getting signals from C that are tiled based on the extreme stride and the small kernel. If you really want to use a shallow architecture for some reason, try using a single conv layer with kernel size 4 or 5 and stride &lt;em&gt;1&lt;/em&gt; followed your FC layer. I&amp;#39;d expect that to at least not give you demon-spawn, though I doubt you&amp;#39;ll see HQ results.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg90g5x", "score_hidden": false, "stickied": false, "created": 1492196961.0, "created_utc": 1492168161.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 8}}], "after": null, "before": null}}, "user_reports": [], "id": "dg907i1", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "warmsnail", "parent_id": "t1_dg8zmsm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; Your critic architecture in the code you provided is only a single conv layer followed by an FC layer--this is definitely too shallow to get good results out on celebA, even at 64x64. Start with the standard DCGAN architecture and tweak hyperparameters from there.\n\nIf it is too shallow for the critic, why does the similarly constructed shallow FC architecture work? Shallow FC still has a hard time learning complex abstractions that a deep net easily can, but there exist incremental improvements as layers are added (more detail, higher abstractions).\n\nI understand I can try to tweak DCGAN, but why doesn't the same thing hold for convolutional layers? \n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Your critic architecture in the code you provided is only a single conv layer followed by an FC layer--this is definitely too shallow to get good results out on celebA, even at 64x64. Start with the standard DCGAN architecture and tweak hyperparameters from there.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;If it is too shallow for the critic, why does the similarly constructed shallow FC architecture work? Shallow FC still has a hard time learning complex abstractions that a deep net easily can, but there exist incremental improvements as layers are added (more detail, higher abstractions).&lt;/p&gt;\n\n&lt;p&gt;I understand I can try to tweak DCGAN, but why doesn&amp;#39;t the same thing hold for convolutional layers? &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg907i1", "score_hidden": false, "stickied": false, "created": 1492196251.0, "created_utc": 1492167451.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8zmsm", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "ajmooch", "parent_id": "t3_6595ve", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Your critic architecture in the code you provided is only a single conv layer followed by an FC layer--this is definitely too shallow to get good results out on celebA, even at 64x64. Start with the standard [DCGAN architecture](https://arxiv.org/abs/1511.06434) and tweak hyperparameters from there.\n\nVanilla WGAN is less sensitive to architecture choices but you still need to consider the problem at hand when designing your nets. If C only has a single fat convolutional layer it's going to have a really hard time learning the complex abstractions that a deeper net can (it's basically going to learn 1000 edge filters then try to weighted-ly combine them in the FC layers). Typically the \"you can use any architecture\" tagline of the newer GANs means to a practitioner \"oh yay we can deepen these things,\" with the \"you could even use an FC G and C\" coming as a demonstration of versatility, not a practical suggestion.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your critic architecture in the code you provided is only a single conv layer followed by an FC layer--this is definitely too shallow to get good results out on celebA, even at 64x64. Start with the standard &lt;a href=\"https://arxiv.org/abs/1511.06434\"&gt;DCGAN architecture&lt;/a&gt; and tweak hyperparameters from there.&lt;/p&gt;\n\n&lt;p&gt;Vanilla WGAN is less sensitive to architecture choices but you still need to consider the problem at hand when designing your nets. If C only has a single fat convolutional layer it&amp;#39;s going to have a really hard time learning the complex abstractions that a deeper net can (it&amp;#39;s basically going to learn 1000 edge filters then try to weighted-ly combine them in the FC layers). Typically the &amp;quot;you can use any architecture&amp;quot; tagline of the newer GANs means to a practitioner &amp;quot;oh yay we can deepen these things,&amp;quot; with the &amp;quot;you could even use an FC G and C&amp;quot; coming as a demonstration of versatility, not a practical suggestion.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8zmsm", "score_hidden": false, "stickied": false, "created": 1492194494.0, "created_utc": 1492165694.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_6595ve", "likes": null, "replies": "", "user_reports": [], "id": "dg9g923", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "skhehw", "parent_id": "t1_dg9e7wi", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "hmm yes shallow and pedantic", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;hmm yes shallow and pedantic&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9g923", "score_hidden": false, "stickied": false, "created": 1492219737.0, "created_utc": 1492190937.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9e7wi", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "darkconfidantislife", "parent_id": "t3_6595ve", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "The critic is probably too shallow. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The critic is probably too shallow. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9e7wi", "score_hidden": false, "stickied": false, "created": 1492217415.0, "created_utc": 1492188615.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}]