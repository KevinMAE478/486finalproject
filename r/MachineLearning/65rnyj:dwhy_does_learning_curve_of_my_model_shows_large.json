[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "65rnyj", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 43, "report_reasons": null, "author": "breadFTD", "saved": false, "mod_reports": [], "name": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "i.redd.it", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/zvEt2rV1adSjjBGqAqAFq1JcorrOoHK74mresKTeB-k.png?s=8bd0e061c6da981ac45be7c7be627278", "width": 900, "height": 900}, "resolutions": [{"url": "https://i.redditmedia.com/zvEt2rV1adSjjBGqAqAFq1JcorrOoHK74mresKTeB-k.png?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=f04d6ecd1354f388046b01def19f2df5", "width": 108, "height": 108}, {"url": "https://i.redditmedia.com/zvEt2rV1adSjjBGqAqAFq1JcorrOoHK74mresKTeB-k.png?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=df8cfd095816e9e825f4f43f7a4c2bb5", "width": 216, "height": 216}, {"url": "https://i.redditmedia.com/zvEt2rV1adSjjBGqAqAFq1JcorrOoHK74mresKTeB-k.png?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=c8e177af960088cf2f61b9d061eb6db7", "width": 320, "height": 320}, {"url": "https://i.redditmedia.com/zvEt2rV1adSjjBGqAqAFq1JcorrOoHK74mresKTeB-k.png?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=1630f478efe8101bf5eaee9abccc3797", "width": 640, "height": 640}], "variants": {}, "id": "7wZ9sILeKh3sZZzBYui76_XchjyBRxr0BN8BF401mEQ"}], "enabled": true}, "thumbnail": "https://b.thumbs.redditmedia.com/qmRbs96ZsfNOH9Cqc1qCn9hjTauWfAh8wZ6odeq7Rxw.jpg", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "image", "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65rnyj/dwhy_does_learning_curve_of_my_model_shows_large/", "num_reports": null, "locked": false, "stickied": false, "created": 1492408512.0, "url": "https://i.redd.it/ru07zrijlzry.png", "author_flair_text": null, "quarantine": false, "title": "[D]Why does learning curve of my model shows large variance in training error? How to fix it?", "created_utc": 1492379712.0, "distinguished": null, "media": null, "upvote_ratio": 0.82, "num_comments": 35, "visited": false, "subreddit_type": "public", "ups": 43}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgcupjk", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgcp54l", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I am not sure if it is okay, that's why I am asking.\nI will try with bigger batches, I think the LR is pretty good as I already tried various values.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am not sure if it is okay, that&amp;#39;s why I am asking.\nI will try with bigger batches, I think the LR is pretty good as I already tried various values.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcupjk", "score_hidden": false, "stickied": false, "created": 1492418999.0, "created_utc": 1492390199.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcp54l", "gilded": 0, "archived": false, "score": 15, "report_reasons": null, "author": "theLaurens", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Bigger batches, lower learning rate. But I guess it's kind of normal for your training accuracy not to be a straight line, as long as your loss has a nice curve, it should all be fine?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Bigger batches, lower learning rate. But I guess it&amp;#39;s kind of normal for your training accuracy not to be a straight line, as long as your loss has a nice curve, it should all be fine?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcp54l", "score_hidden": false, "stickied": false, "created": 1492411428.0, "created_utc": 1492382628.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 15}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgcnz48", "gilded": 0, "archived": false, "score": 26, "report_reasons": null, "author": "JCPenis", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "bigger batches", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;bigger batches&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcnz48", "score_hidden": false, "stickied": false, "created": 1492409820.0, "created_utc": 1492381020.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 26}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dge2mhs", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgd7leo", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thx, will read!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thx, will read!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dge2mhs", "score_hidden": false, "stickied": false, "created": 1492489916.0, "created_utc": 1492461116.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgd7leo", "gilded": 0, "archived": false, "score": 10, "report_reasons": null, "author": "JustFinishedBSG", "parent_id": "t1_dgcuu6d", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "https://arxiv.org/pdf/1609.04836.pdf", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/1609.04836.pdf\"&gt;https://arxiv.org/pdf/1609.04836.pdf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgd7leo", "score_hidden": false, "stickied": false, "created": 1492439583.0, "created_utc": 1492410783.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 10}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dge2les", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgdtueb", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Prediction user's response time in social networks, similar to CTR prediction in advertisement.\nFeatures are user profile features (e.g. user profile embedded by w2v) and event features, e.g. features of a hashtags (character level embedding).", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Prediction user&amp;#39;s response time in social networks, similar to CTR prediction in advertisement.\nFeatures are user profile features (e.g. user profile embedded by w2v) and event features, e.g. features of a hashtags (character level embedding).&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dge2les", "score_hidden": false, "stickied": false, "created": 1492489884.0, "created_utc": 1492461084.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdtueb", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "xyz4d", "parent_id": "t1_dgcuzs5", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "What exactly is it you're working with?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What exactly is it you&amp;#39;re working with?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdtueb", "score_hidden": false, "stickied": false, "created": 1492480055.0, "created_utc": 1492451255.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcuzs5", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgcuu6d", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It could be a noisy dataset, but I have to work with it. I can extract more features out of it later for a bigger model.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It could be a noisy dataset, but I have to work with it. I can extract more features out of it later for a bigger model.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcuzs5", "score_hidden": false, "stickied": false, "created": 1492419379.0, "created_utc": 1492390579.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgd4jkw", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "darkconfidantislife", "parent_id": "t1_dgcuu6d", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "The \"sharp minima\" paper. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The &amp;quot;sharp minima&amp;quot; paper. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgd4jkw", "score_hidden": false, "stickied": false, "created": 1492433248.0, "created_utc": 1492404448.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcuu6d", "gilded": 0, "archived": false, "score": 12, "report_reasons": null, "author": "xyz4d", "parent_id": "t1_dgcpq4o", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "\"Bigger batches hurt generalization\" source? I had been taught bigger batches worked better. Also, couldn't it just be a noisy dataset?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Bigger batches hurt generalization&amp;quot; source? I had been taught bigger batches worked better. Also, couldn&amp;#39;t it just be a noisy dataset?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcuu6d", "score_hidden": false, "stickied": false, "created": 1492419171.0, "created_utc": 1492390371.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 12}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgcvzyd", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "anangusp", "parent_id": "t1_dgcveig", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Well, I probably more meant expected behaviour. In a fairly complex convolutional net with DropOut and pooling layers, I've seen moderate noise in error and accuracy statistics for the networks in my own experience", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, I probably more meant expected behaviour. In a fairly complex convolutional net with DropOut and pooling layers, I&amp;#39;ve seen moderate noise in error and accuracy statistics for the networks in my own experience&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcvzyd", "score_hidden": false, "stickied": false, "created": 1492420681.0, "created_utc": 1492391881.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcveig", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgcqg81", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Could you provide an example model where this is not a undesirable behavior?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Could you provide an example model where this is not a undesirable behavior?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcveig", "score_hidden": false, "stickied": false, "created": 1492419915.0, "created_utc": 1492391115.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcqg81", "gilded": 0, "archived": false, "score": 10, "report_reasons": null, "author": "anangusp", "parent_id": "t1_dgcpq4o", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Also whether this is a undesirable or bad behaviour depends entirely on what model architecture OP has", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also whether this is a undesirable or bad behaviour depends entirely on what model architecture OP has&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcqg81", "score_hidden": false, "stickied": false, "created": 1492413237.0, "created_utc": 1492384437.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 10}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dge2pty", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgd7mj9", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "LOL, that's a nice justification.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;LOL, that&amp;#39;s a nice justification.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dge2pty", "score_hidden": false, "stickied": false, "created": 1492490021.0, "created_utc": 1492461221.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgd7mj9", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "JustFinishedBSG", "parent_id": "t1_dgcusn0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I personally choose batch size to be just enough to use my GPU entirely, because if it's too small that lazy ass just isn't used 100% while it waits for more data ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I personally choose batch size to be just enough to use my GPU entirely, because if it&amp;#39;s too small that lazy ass just isn&amp;#39;t used 100% while it waits for more data &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgd7mj9", "score_hidden": false, "stickied": false, "created": 1492439649.0, "created_utc": 1492410849.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcusn0", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgcpq4o", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thanks for your advice, I wonder if there is any general rules I should obey for selecting batch size?\n\nCurrently I have 80k samples, batch size = 64.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for your advice, I wonder if there is any general rules I should obey for selecting batch size?&lt;/p&gt;\n\n&lt;p&gt;Currently I have 80k samples, batch size = 64.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcusn0", "score_hidden": false, "stickied": false, "created": 1492419113.0, "created_utc": 1492390313.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgdxjvi", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "besirk", "parent_id": "t1_dgde97p", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "How was BSG?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How was BSG?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdxjvi", "score_hidden": false, "stickied": false, "created": 1492484204.0, "created_utc": 1492455404.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgde97p", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "JustFinishedBSG", "parent_id": "t1_dgdd5xs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Because the small variance in updates makes it possible for the model to converge to a sharp local minimum, which has a higher chance of generalizing badly than a flat minimum A minimulm reached via noisy updates is more robust to noise", "edited": 1492430626.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because the small variance in updates makes it possible for the model to converge to a sharp local minimum, which has a higher chance of generalizing badly than a flat minimum A minimulm reached via noisy updates is more robust to noise&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgde97p", "score_hidden": false, "stickied": false, "created": 1492459217.0, "created_utc": 1492430417.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdd5xs", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "WaxyChocolate", "parent_id": "t1_dgcpq4o", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; bigger batches hurt generalization\n\nWhy?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;bigger batches hurt generalization&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Why?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdd5xs", "score_hidden": false, "stickied": false, "created": 1492456521.0, "created_utc": 1492427721.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcpq4o", "gilded": 0, "archived": false, "score": 28, "report_reasons": null, "author": "JustFinishedBSG", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It's normal. No big deal. Don't try to fix it, bigger batches hurt generalization.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s normal. No big deal. Don&amp;#39;t try to fix it, bigger batches hurt generalization.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcpq4o", "score_hidden": false, "stickied": false, "created": 1492412222.0, "created_utc": 1492383422.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 28}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgcvalm", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgcv4lt", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Not 8 seconds, that's in minutes I think. I don't have many features/samples now, so the model is small and training/validation is fast.\nI would like to try second-order methods, but later I will increase dimension and number of samples for the final experiment.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not 8 seconds, that&amp;#39;s in minutes I think. I don&amp;#39;t have many features/samples now, so the model is small and training/validation is fast.\nI would like to try second-order methods, but later I will increase dimension and number of samples for the final experiment.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcvalm", "score_hidden": false, "stickied": false, "created": 1492419772.0, "created_utc": 1492390972.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcv4lt", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Megatron_McLargeHuge", "parent_id": "t1_dgcuxa8", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "That many epochs in 8 seconds? Sounds like you don't have many training examples, and/or you have a small model. In that case you can probably use full-data second order methods instead of SGD, such as L-BFGS or CG.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That many epochs in 8 seconds? Sounds like you don&amp;#39;t have many training examples, and/or you have a small model. In that case you can probably use full-data second order methods instead of SGD, such as L-BFGS or CG.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcv4lt", "score_hidden": false, "stickied": false, "created": 1492419556.0, "created_utc": 1492390756.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcuxa8", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgcrewm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "That's a great suggestion, I will try with loss function value.\nP.S. Each point is an epoch. \n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s a great suggestion, I will try with loss function value.\nP.S. Each point is an epoch. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcuxa8", "score_hidden": false, "stickied": false, "created": 1492419286.0, "created_utc": 1492390486.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcrewm", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "Megatron_McLargeHuge", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Assuming those are batch numbers for training. You can plot stats over a larger subset of your training data even if you keep your batch size the same for learning. Also it's usually more informative to look at log likelihood or similar loss functions more than accuracy during development.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Assuming those are batch numbers for training. You can plot stats over a larger subset of your training data even if you keep your batch size the same for learning. Also it&amp;#39;s usually more informative to look at log likelihood or similar loss functions more than accuracy during development.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcrewm", "score_hidden": false, "stickied": false, "created": 1492414551.0, "created_utc": 1492385751.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 6}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dge8lkl", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgdbeyf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thanks, I will try the what you recommended and let you know.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, I will try the what you recommended and let you know.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dge8lkl", "score_hidden": false, "stickied": false, "created": 1492497116.0, "created_utc": 1492468316.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdbeyf", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "noman2561", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It could be a number of things so I'll rant a little and you can see what might be applicable. \n\nIt may be a problem with the data. If you don't have enough data, your network simply can't optimize on good features. If your data is extremely diverse, your features will change greatly each iteration because the input is vastly different from what it was just told to learn last time. You might have a batch size that is too small. You could have an imbalance between your training and testing sets: they should both be representative of the same set. \n\nIt could be a problem with the structure of your network. If you have too few nodes in the first feature layer of a regressor (like comically too few) you could have trouble capturing enough information to make a decision. Regressors capture the most information in those first few layers and tend to ignore everything that doesn't help to make the decision. Pretraining with an autoencoder helps to build features that don't ignore information while also helping you to get the layer sizes a little closer. You might not have enough layers. Recalling the universal approximation theorem, the node in the third layer (the second feature layer) can approximate any continuous function. \n\nIt could be a problem with the optimization or activation functions. Typically this isn't it but make sure you chose carefully according to what makes sense for your data. Your learning rate could be too fast. This would cause you to overstep around the min or max (depending on if you're set up to minimize error or maximize likelihood). I'm afraid to say too much about the activation function(s) because in my own work the selection has caused large variances in the features and in the results but I don't read a lot about that in other people's work. I'd say it's worth a shot to try out different activation functions and see what works best. \n\nOverall what I would recommend is to try pretraining with an autoencoder, make sure your network has enough layers (try 3-5 layers), increase the batch size, and play around with different activation functions. It's not a bad idea to dissect the network while it's training to see what kind of features are forming. Good luck and let us know how it goes. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It could be a number of things so I&amp;#39;ll rant a little and you can see what might be applicable. &lt;/p&gt;\n\n&lt;p&gt;It may be a problem with the data. If you don&amp;#39;t have enough data, your network simply can&amp;#39;t optimize on good features. If your data is extremely diverse, your features will change greatly each iteration because the input is vastly different from what it was just told to learn last time. You might have a batch size that is too small. You could have an imbalance between your training and testing sets: they should both be representative of the same set. &lt;/p&gt;\n\n&lt;p&gt;It could be a problem with the structure of your network. If you have too few nodes in the first feature layer of a regressor (like comically too few) you could have trouble capturing enough information to make a decision. Regressors capture the most information in those first few layers and tend to ignore everything that doesn&amp;#39;t help to make the decision. Pretraining with an autoencoder helps to build features that don&amp;#39;t ignore information while also helping you to get the layer sizes a little closer. You might not have enough layers. Recalling the universal approximation theorem, the node in the third layer (the second feature layer) can approximate any continuous function. &lt;/p&gt;\n\n&lt;p&gt;It could be a problem with the optimization or activation functions. Typically this isn&amp;#39;t it but make sure you chose carefully according to what makes sense for your data. Your learning rate could be too fast. This would cause you to overstep around the min or max (depending on if you&amp;#39;re set up to minimize error or maximize likelihood). I&amp;#39;m afraid to say too much about the activation function(s) because in my own work the selection has caused large variances in the features and in the results but I don&amp;#39;t read a lot about that in other people&amp;#39;s work. I&amp;#39;d say it&amp;#39;s worth a shot to try out different activation functions and see what works best. &lt;/p&gt;\n\n&lt;p&gt;Overall what I would recommend is to try pretraining with an autoencoder, make sure your network has enough layers (try 3-5 layers), increase the batch size, and play around with different activation functions. It&amp;#39;s not a bad idea to dissect the network while it&amp;#39;s training to see what kind of features are forming. Good luck and let us know how it goes. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdbeyf", "score_hidden": false, "stickied": false, "created": 1492451019.0, "created_utc": 1492422219.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgco0ej", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "rubikhan", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "As someone new to machine learning, I'd like to hear opinions about this as well.  However, can you provide more info about what you're training, how you're training it, etc?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As someone new to machine learning, I&amp;#39;d like to hear opinions about this as well.  However, can you provide more info about what you&amp;#39;re training, how you&amp;#39;re training it, etc?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgco0ej", "score_hidden": false, "stickied": false, "created": 1492409868.0, "created_utc": 1492381068.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 9}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgcn159", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "breadFTD", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "By the way, I am sure the amount of parameters is significantly smaller than number of samples in this case.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;By the way, I am sure the amount of parameters is significantly smaller than number of samples in this case.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcn159", "score_hidden": false, "stickied": false, "created": 1492408581.0, "created_utc": 1492379781.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgd9zqa", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "lysecret", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Can't click on the link are the neural nets?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can&amp;#39;t click on the link are the neural nets?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgd9zqa", "score_hidden": false, "stickied": false, "created": 1492446092.0, "created_utc": 1492417292.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dge4eu1", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "brockl33", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Like others have said, training noise is normal but if you really want to you can try the following depending on your data and application you could do a couple things:\n\n1. Optimize batch size - not too small, not too big. In my particular domain its 16 or 32. Too large and you miss out on good gradient variation. Too small and you suffer from computing inefficiency and over-fitting.\n\n2. Use batch normalization - this tends to smooth training out a lot for many people. Many different explanations for this, imo its a cheap and easy way of reigning in crazy weights.\n\n3. Use some kind of regularization eg L2 or orthogonal regularization - also tends to smooth training out a little by reigning in crazy weights.\n\n4. Implement an unsupervised objective aka semi-supervised learning - Also tends to tie down the weights to the structure of the data instead of letting them go crazy.\n\n5. Decrease learning rate.\n\n6. Train on minibatches but evaluate training accuracy across the entire training set like you do for validation accuracy.\n\n7. Use a simpler network structure - less degrees of freedom.\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Like others have said, training noise is normal but if you really want to you can try the following depending on your data and application you could do a couple things:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Optimize batch size - not too small, not too big. In my particular domain its 16 or 32. Too large and you miss out on good gradient variation. Too small and you suffer from computing inefficiency and over-fitting.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use batch normalization - this tends to smooth training out a lot for many people. Many different explanations for this, imo its a cheap and easy way of reigning in crazy weights.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use some kind of regularization eg L2 or orthogonal regularization - also tends to smooth training out a little by reigning in crazy weights.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Implement an unsupervised objective aka semi-supervised learning - Also tends to tie down the weights to the structure of the data instead of letting them go crazy.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Decrease learning rate.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Train on minibatches but evaluate training accuracy across the entire training set like you do for validation accuracy.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use a simpler network structure - less degrees of freedom.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dge4eu1", "score_hidden": false, "stickied": false, "created": 1492491933.0, "created_utc": 1492463133.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgcsj1t", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "UsernameOmitted", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "More information would be helpful. What are you training, what kind of machine learning, did you make it yourself, using a library? What about your test and training data, are you testing on the same set as training, or keeping a separate set to train and test, etc...", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;More information would be helpful. What are you training, what kind of machine learning, did you make it yourself, using a library? What about your test and training data, are you testing on the same set as training, or keeping a separate set to train and test, etc...&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcsj1t", "score_hidden": false, "stickied": false, "created": 1492416047.0, "created_utc": 1492387247.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgcuqas", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "breadFTD", "parent_id": "t1_dgcpxvt", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "SGD with momentum = 0.9, learning rate = 0.01", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;SGD with momentum = 0.9, learning rate = 0.01&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcuqas", "score_hidden": false, "stickied": false, "created": 1492419028.0, "created_utc": 1492390228.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 4}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcpxvt", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "negativeLearningRate", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "What optimizer do you use?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What optimizer do you use?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcpxvt", "score_hidden": false, "stickied": false, "created": 1492412522.0, "created_utc": 1492383722.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgdc3t9", "gilded": 0, "archived": false, "score": -1, "report_reasons": null, "author": "scionaura", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Removing the neural network should help.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Removing the neural network should help.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdc3t9", "score_hidden": false, "stickied": false, "created": 1492453321.0, "created_utc": 1492424521.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": -1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65rnyj", "likes": null, "replies": "", "user_reports": [], "id": "dgcub0j", "gilded": 0, "archived": false, "score": -5, "report_reasons": null, "author": "FastAndTheHilarious", "parent_id": "t3_65rnyj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Depends on what kind of model you're using. If you're using too much data for training, you might be overfitting your model to the training data", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Depends on what kind of model you&amp;#39;re using. If you&amp;#39;re using too much data for training, you might be overfitting your model to the training data&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcub0j", "score_hidden": false, "stickied": false, "created": 1492418464.0, "created_utc": 1492389664.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": -5}}], "after": null, "before": null}}]