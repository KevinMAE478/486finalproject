[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "64tv5x", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 1, "report_reasons": null, "author": "cvikasreddy", "saved": false, "mod_reports": [], "name": "t3_64tv5x", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/64tv5x/discussion_seq2seq_with_char_embeddings_is_not/", "num_reports": null, "locked": false, "stickied": false, "created": 1491974943.0, "url": "https://www.reddit.com/r/MachineLearning/comments/64tv5x/discussion_seq2seq_with_char_embeddings_is_not/", "author_flair_text": null, "quarantine": false, "title": "[Discussion] seq2seq with char embeddings is not overfitting but the same model with word embeddings is overfitting a lot even after reducing model complexity and introducing dropout. What might be the reason and how to debug this problem?", "created_utc": 1491946143.0, "distinguished": null, "media": null, "upvote_ratio": 0.67, "num_comments": 2, "visited": false, "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64tv5x", "likes": null, "replies": "", "user_reports": [], "id": "dg5citv", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "martinarjovsky", "parent_id": "t3_64tv5x", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "If the dataset is tiny and the word vocabulary is big, there can be a LOT of parameters in the embedding matrix (namely, |V| x n_hidden). Something that you can try is to have a random projection (not learnable) from the word one-hots to n_hidden (a random fixed matrix of size |V| x n_hidden) and the embedding be simply of size n_hidden x n_hidden.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If the dataset is tiny and the word vocabulary is big, there can be a LOT of parameters in the embedding matrix (namely, |V| x n_hidden). Something that you can try is to have a random projection (not learnable) from the word one-hots to n_hidden (a random fixed matrix of size |V| x n_hidden) and the embedding be simply of size n_hidden x n_hidden.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5citv", "score_hidden": false, "stickied": false, "created": 1491993384.0, "created_utc": 1491964584.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64tv5x", "likes": null, "replies": "", "user_reports": [], "id": "dg7ca5o", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Jean-Porte", "parent_id": "t3_64tv5x", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "What's your task ?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s your task ?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7ca5o", "score_hidden": false, "stickied": false, "created": 1492103568.0, "created_utc": 1492074768.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]