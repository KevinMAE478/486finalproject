[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Research", "id": "64qby3", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 0, "report_reasons": null, "author": "NicolasGuacamole", "saved": false, "mod_reports": [], "name": "t3_64qby3", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "arxiv.org", "hidden": false, "thumbnail": "default", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "three", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/64qby3/r_170402071_convolutional_neural_pyramid_for/", "num_reports": null, "locked": false, "stickied": false, "created": 1491940248.0, "url": "https://arxiv.org/abs/1704.02071", "author_flair_text": null, "quarantine": false, "title": "[R] [1704.02071] Convolutional Neural Pyramid for Image Processing", "created_utc": 1491911448.0, "distinguished": null, "media": null, "upvote_ratio": 0.36, "num_comments": 14, "visited": false, "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": "", "user_reports": [], "id": "dg45dba", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "arXiv_abstract_bot", "parent_id": "t3_64qby3", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Title: Convolutional Neural Pyramid for Image Processing  \n\nAuthors: [Xiaoyong Shen](http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1), [Ying-Cong Chen](http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1), [Xin Tao](http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1), [Jiaya Jia](http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1)  \n\n&gt; Abstract: We propose a principled convolutional neural pyramid (CNP) framework for general low-level vision and image processing tasks. It is based on the essential finding that many applications require large receptive fields for structure understanding. But corresponding neural networks for regression either stack many layers or apply large kernels to achieve it, which is computationally very costly. Our pyramid structure can greatly enlarge the field while not sacrificing computation efficiency. Extra benefit includes adaptive network depth and progressive upsampling for quasi-realtime testing on VGA-size input. Our method profits a broad set of applications, such as depth/RGB image restoration, completion, noise/artifact removal, edge refinement, image filtering, image enhancement and colorization.  \n\n[PDF link](https://arxiv.org/pdf/1704.02071)  [Landing page](https://arxiv.org/abs/1704.02071)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Title: Convolutional Neural Pyramid for Image Processing  &lt;/p&gt;\n\n&lt;p&gt;Authors: &lt;a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\"&gt;Xiaoyong Shen&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\"&gt;Ying-Cong Chen&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\"&gt;Xin Tao&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\"&gt;Jiaya Jia&lt;/a&gt;  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Abstract: We propose a principled convolutional neural pyramid (CNP) framework for general low-level vision and image processing tasks. It is based on the essential finding that many applications require large receptive fields for structure understanding. But corresponding neural networks for regression either stack many layers or apply large kernels to achieve it, which is computationally very costly. Our pyramid structure can greatly enlarge the field while not sacrificing computation efficiency. Extra benefit includes adaptive network depth and progressive upsampling for quasi-realtime testing on VGA-size input. Our method profits a broad set of applications, such as depth/RGB image restoration, completion, noise/artifact removal, edge refinement, image filtering, image enhancement and colorization.  &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/1704.02071\"&gt;PDF link&lt;/a&gt;  &lt;a href=\"https://arxiv.org/abs/1704.02071\"&gt;Landing page&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg45dba", "score_hidden": false, "stickied": false, "created": 1491940254.0, "created_utc": 1491911454.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": "", "user_reports": [], "id": "dg4ezh2", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "uomreddit", "parent_id": "t1_dg4dh8y", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "The difference between these two lies in that multi-level architecture model as in this paper \"reserves\" the feature maps in each level and fuse them later, while multi-layer architecture model usually only takes the feature map from the last layer into account which, although, comes from the previous layers, inevitably suffer from information loss to some extent.\nBTW, One thing confuses me in the paper is in Sect. 3.3, \" In order to get 95\u00d795 receptive field, 48 3\u00d73 convolution layers are needed\", why is this true? Don't people do the pooling to downsample the feature maps after each layer?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The difference between these two lies in that multi-level architecture model as in this paper &amp;quot;reserves&amp;quot; the feature maps in each level and fuse them later, while multi-layer architecture model usually only takes the feature map from the last layer into account which, although, comes from the previous layers, inevitably suffer from information loss to some extent.\nBTW, One thing confuses me in the paper is in Sect. 3.3, &amp;quot; In order to get 95\u00d795 receptive field, 48 3\u00d73 convolution layers are needed&amp;quot;, why is this true? Don&amp;#39;t people do the pooling to downsample the feature maps after each layer?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4ezh2", "score_hidden": false, "stickied": false, "created": 1491953512.0, "created_utc": 1491924712.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4dh8y", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "radarsat1", "parent_id": "t1_dg4cuvc", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Maaaybe.. it's not clear to me how \"levels\" are different from \"layers\".  I see the \"Mapping\" connection is maybe something new, but it seems to be basically a skip connection with an embedded nonlinear transform? A bit like a \"ladder network\" maybe?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maaaybe.. it&amp;#39;s not clear to me how &amp;quot;levels&amp;quot; are different from &amp;quot;layers&amp;quot;.  I see the &amp;quot;Mapping&amp;quot; connection is maybe something new, but it seems to be basically a skip connection with an embedded nonlinear transform? A bit like a &amp;quot;ladder network&amp;quot; maybe?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4dh8y", "score_hidden": false, "stickied": false, "created": 1491951823.0, "created_utc": 1491923023.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4cuvc", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "uomreddit", "parent_id": "t1_dg477kt", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I think Figure 4 makes it clear the difference from a standard multilayer CNN, multi-level vs multi-layer.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think Figure 4 makes it clear the difference from a standard multilayer CNN, multi-level vs multi-layer.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4cuvc", "score_hidden": false, "stickied": false, "created": 1491951120.0, "created_utc": 1491922320.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": "", "user_reports": [], "id": "dg4bxld", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "radarsat1", "parent_id": "t1_dg4av83", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; If you didn't get it, just read it in more detail.\n\nOkay... thanks for the help anyway.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;If you didn&amp;#39;t get it, just read it in more detail.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Okay... thanks for the help anyway.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4bxld", "score_hidden": false, "stickied": false, "created": 1491950047.0, "created_utc": 1491921247.0, "depth": 8, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64qby3", "likes": null, "replies": "", "user_reports": [], "id": "dg5umpb", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "NotAlphaGo", "parent_id": "t1_dg4av83", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Instead of saying this you could just explain out of good will what's new.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Instead of saying this you could just explain out of good will what&amp;#39;s new.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5umpb", "score_hidden": false, "stickied": false, "created": 1492032321.0, "created_utc": 1492003521.0, "depth": 8, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4av83", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "NicolasGuacamole", "parent_id": "t1_dg4aquw", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You write directly addressing me, and you say that you skimmed it. If you didn't get it, just read it in more detail.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You write directly addressing me, and you say that you skimmed it. If you didn&amp;#39;t get it, just read it in more detail.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4av83", "score_hidden": false, "stickied": false, "created": 1491948737.0, "created_utc": 1491919937.0, "depth": 7, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4aquw", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "radarsat1", "parent_id": "t1_dg49xsq", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; read the paper.\n\nI am literally quoting from the paper in my replies, explaining that I tried to read it, and don't get it, and am asking for clarification.  But you won't accept that, okay fine.  Just ignore me then?  No need to take offense.  My comments are hardly the most harsh thing I've read in /r/machinelearning.\n\nAs for \"either way I didn't write it\", you know I'm not asking YOU specifically, right? I'm making general comments to the forum.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;read the paper.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I am literally quoting from the paper in my replies, explaining that I tried to read it, and don&amp;#39;t get it, and am asking for clarification.  But you won&amp;#39;t accept that, okay fine.  Just ignore me then?  No need to take offense.  My comments are hardly the most harsh thing I&amp;#39;ve read in &lt;a href=\"/r/machinelearning\"&gt;/r/machinelearning&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;As for &amp;quot;either way I didn&amp;#39;t write it&amp;quot;, you know I&amp;#39;m not asking YOU specifically, right? I&amp;#39;m making general comments to the forum.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4aquw", "score_hidden": false, "stickied": false, "created": 1491948586.0, "created_utc": 1491919786.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg49xsq", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "NicolasGuacamole", "parent_id": "t1_dg49m2l", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 1, "body": "Perhaps it's just your tone. Either way, I didn't write it.\n\nAs for what their contribution is, read the paper.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Perhaps it&amp;#39;s just your tone. Either way, I didn&amp;#39;t write it.&lt;/p&gt;\n\n&lt;p&gt;As for what their contribution is, read the paper.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg49xsq", "score_hidden": false, "stickied": false, "created": 1491947538.0, "created_utc": 1491918738.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dg49m2l", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "radarsat1", "parent_id": "t1_dg48go1", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I'm not shitting on it!  I'm asking what it's proposing that is new!  That is literally the most basic request possible with regards to a research paper... instead of answering my question you are getting defensive.  Why?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not shitting on it!  I&amp;#39;m asking what it&amp;#39;s proposing that is new!  That is literally the most basic request possible with regards to a research paper... instead of answering my question you are getting defensive.  Why?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg49m2l", "score_hidden": false, "stickied": false, "created": 1491947105.0, "created_utc": 1491918305.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dg48go1", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "NicolasGuacamole", "parent_id": "t1_dg48ag4", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 1, "body": "You've assumed I wrote this. I didn't.\n\nThis is just a nice, simple architecture which does feature extraction at different scales and fuses the features extracted at different scales using elementwise addition.\n\nIt performs very well across several low-level vision tasks without adaptation or hacks.\n\nIt provides a very wide receptive field.\n\nLastly, I'd like to say that your tone is really snarky. If you don't work in vision / aren't interested / didn't want to read the paper then just move on. There's no need shit on a paper when you couldn't even be arsed to read it properly.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;ve assumed I wrote this. I didn&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;This is just a nice, simple architecture which does feature extraction at different scales and fuses the features extracted at different scales using elementwise addition.&lt;/p&gt;\n\n&lt;p&gt;It performs very well across several low-level vision tasks without adaptation or hacks.&lt;/p&gt;\n\n&lt;p&gt;It provides a very wide receptive field.&lt;/p&gt;\n\n&lt;p&gt;Lastly, I&amp;#39;d like to say that your tone is really snarky. If you don&amp;#39;t work in vision / aren&amp;#39;t interested / didn&amp;#39;t want to read the paper then just move on. There&amp;#39;s no need shit on a paper when you couldn&amp;#39;t even be arsed to read it properly.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg48go1", "score_hidden": false, "stickied": false, "created": 1491945525.0, "created_utc": 1491916725.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dg48ag4", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "radarsat1", "parent_id": "t1_dg47h89", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "But like I said, from a skim of the paper I can't tell what it is doing that is new. Can you summarize?\n\nYou say that your \"pyramid structure\" is an alternative to use large kernels or many layers.  Then you go on to describe a multilayered structure where you downsample feature representations in successive layers.  How is this different from CNN + maxpooling?\n\nHonestly I just have a hard time understanding what you are describing.  You use too much ambiguous English, words like \"pyramid structure with downsampling\", rather than *math*, which would actually describe what you are doing.\n\nOh wait, I see in 3.2, \"two downsampling schemes are tested \u2013 max pooling and convolution with a 3\u00d73 kernel with stride 2.\"\n\nSo this really does literally seem to be a description of a bog-standard multilayer CNN.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;But like I said, from a skim of the paper I can&amp;#39;t tell what it is doing that is new. Can you summarize?&lt;/p&gt;\n\n&lt;p&gt;You say that your &amp;quot;pyramid structure&amp;quot; is an alternative to use large kernels or many layers.  Then you go on to describe a multilayered structure where you downsample feature representations in successive layers.  How is this different from CNN + maxpooling?&lt;/p&gt;\n\n&lt;p&gt;Honestly I just have a hard time understanding what you are describing.  You use too much ambiguous English, words like &amp;quot;pyramid structure with downsampling&amp;quot;, rather than &lt;em&gt;math&lt;/em&gt;, which would actually describe what you are doing.&lt;/p&gt;\n\n&lt;p&gt;Oh wait, I see in 3.2, &amp;quot;two downsampling schemes are tested \u2013 max pooling and convolution with a 3\u00d73 kernel with stride 2.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;So this really does literally seem to be a description of a bog-standard multilayer CNN.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg48ag4", "score_hidden": false, "stickied": false, "created": 1491945272.0, "created_utc": 1491916472.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg47h89", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "NicolasGuacamole", "parent_id": "t1_dg477kt", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It's just a pleasant architecture.\n\nDealing with scale in vision can be a pain in the arse.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s just a pleasant architecture.&lt;/p&gt;\n\n&lt;p&gt;Dealing with scale in vision can be a pain in the arse.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg47h89", "score_hidden": false, "stickied": false, "created": 1491944011.0, "created_utc": 1491915211.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dg477kt", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "radarsat1", "parent_id": "t3_64qby3", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Skimming the paper I am having a hard time understanding what is new here..\n\n&gt; Each convolution layer outputs 56 features with a 3\u00d73 kernel.  For level 0, its feature output is a 56-channel map. Other levels take the input of feature extracted from another level after downsampling.\n\nHow is this different from a standard multilayer CNN approach?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Skimming the paper I am having a hard time understanding what is new here..&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Each convolution layer outputs 56 features with a 3\u00d73 kernel.  For level 0, its feature output is a 56-channel map. Other levels take the input of feature extracted from another level after downsampling.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;How is this different from a standard multilayer CNN approach?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg477kt", "score_hidden": false, "stickied": false, "created": 1491943591.0, "created_utc": 1491914791.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}]