[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Research", "id": "64rayf", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 52, "report_reasons": null, "author": "felardos_loris", "saved": false, "mod_reports": [], "name": "t3_64rayf", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "blog.heuritech.com", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/Br8BzDtL18e8Px-FrxSkHQu-9QT8Af8CFLNtirj_yzQ.jpg?s=f28213f051f99a87f2c728170ef94735", "width": 1200, "height": 489}, "resolutions": [{"url": "https://i.redditmedia.com/Br8BzDtL18e8Px-FrxSkHQu-9QT8Af8CFLNtirj_yzQ.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=df3b23f64ae90604761cb31ddcdcad23", "width": 108, "height": 44}, {"url": "https://i.redditmedia.com/Br8BzDtL18e8Px-FrxSkHQu-9QT8Af8CFLNtirj_yzQ.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=5f27e81494725e24e297ca569981635f", "width": 216, "height": 88}, {"url": "https://i.redditmedia.com/Br8BzDtL18e8Px-FrxSkHQu-9QT8Af8CFLNtirj_yzQ.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=761aef8a7eeb416ddf695c58576bd8cd", "width": 320, "height": 130}, {"url": "https://i.redditmedia.com/Br8BzDtL18e8Px-FrxSkHQu-9QT8Af8CFLNtirj_yzQ.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=64edc213643373c8b2fe67915ecdca96", "width": 640, "height": 260}, {"url": "https://i.redditmedia.com/Br8BzDtL18e8Px-FrxSkHQu-9QT8Af8CFLNtirj_yzQ.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=960&amp;s=c0238ff8eb575bd9793c497840234aca", "width": 960, "height": 391}, {"url": "https://i.redditmedia.com/Br8BzDtL18e8Px-FrxSkHQu-9QT8Af8CFLNtirj_yzQ.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=1080&amp;s=d6321e9e402e1836d41f6bd4eee062b1", "width": 1080, "height": 440}], "variants": {}, "id": "VSBUSBEWazT0eJFwQLu9Y4OlOKXmuFkj8dGDWDxZSuY"}], "enabled": false}, "thumbnail": "https://b.thumbs.redditmedia.com/HKIp3c1FDMkS34KVJxuvjC2CpoMj6rYmDi_83wO3iBY.jpg", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "three", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "link", "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/64rayf/r_began_state_of_the_art_generation_of_faces_with/", "num_reports": null, "locked": false, "stickied": false, "created": 1491951241.0, "url": "https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/", "author_flair_text": null, "quarantine": false, "title": "[R] BEGAN: State of the art generation of faces with Generative Adversarial Networks", "created_utc": 1491922441.0, "distinguished": null, "media": null, "upvote_ratio": 0.85, "num_comments": 19, "visited": false, "subreddit_type": "public", "ups": 52}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": "", "user_reports": [], "id": "dg66ief", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "Muffinmaster19", "parent_id": "t1_dg5emh1", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Dibs on shotGAN", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Dibs on shotGAN&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg66ief", "score_hidden": false, "stickied": false, "created": 1492045822.0, "created_utc": 1492017022.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": "", "user_reports": [], "id": "dgdtchz", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "glacialOwl", "parent_id": "t1_dg5emh1", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Guess 20th Century Fox already got LOGAN :(", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Guess 20th Century Fox already got LOGAN :(&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdtchz", "score_hidden": false, "stickied": false, "created": 1492479509.0, "created_utc": 1492450709.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5emh1", "gilded": 0, "archived": false, "score": 12, "report_reasons": null, "author": "GoSergio", "parent_id": "t3_64rayf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "And so it BEGAN. Next, CardiGAN. VEGAN. SloGAN. Will it ever end...", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And so it BEGAN. Next, CardiGAN. VEGAN. SloGAN. Will it ever end...&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5emh1", "score_hidden": false, "stickied": false, "created": 1491996395.0, "created_utc": 1491967595.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 12}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": "", "user_reports": [], "id": "dg4uu2j", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "ajmooch", "parent_id": "t1_dg4nzvj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It's not \"manifold collapse\" if the manifold it collapses to is the actual distribution of the data. The fact that the pixel-wise mean face is sensible does mean that modeling faces is less challenging than something as diverse and unstructured as ImageNet, but it does *not* mean that every model trained on faces is somehow mode collapsed or that they can't actually mode collapse. Mode collapse is visually obvious and looks very, very different from the samples presented in this paper; I can dig up and post some of my own mode-collapsed examples if you'd like. Sampling from a collection of points on one manifold is almost the definition of what we want to do in generative modeling--we assume the data exists on some manifold we can approximate with our model, and we want to draw points from it!\n\nAdditionally, pixel-wise interpolations for faces aren't anywhere near as smooth as interpolations between samples of  a good model. You can easily test this in a slightly more interesting setting if you overfit a convolutional encoder and vary the downsampling factor--even with a 4-8x downsampling, interpolations in the bottleneck space just look like \"ghosting\" between images rather than the smooth interpolations you see from a good model. The interpolations are, again, smoother than they would be between imagenet images, but they really don't look anything like GAN interpolations.\n\nProducing models that can perform well at high resolution on Imagenet is presently not as much a question of the underlying fitting technique (standard GAN vs BEGAN vs WGAN vs autoregressive vs whatever) but model capacity and related peripherals. The fitting technique certainly needs to be able to guide the model distribution towards the true distribution, but the savvy scholar should note that the four highest-image-quality results in the last six months have all come from models trained with *massive* amounts of auxiliary superivision. StackGAN and Multiscale PixelCNN have descriptive captions and/or keypoint annotations, and AC-GAN/PPGN take advantage of the imagenet labels or pre-trained networks. Pix2pix, too, suffers from the need for descriptive annotations in one form to get good results (it doesn't really have a latent space you can sample from, and interpolations suffer accordingly). Getting a fully unsupervised net to produce good results on unstructured images is a question of both capacity and technique, and seeing as neither WGAN nor BEGAN present results on 256x256 Imagenet, drawing comparisons is pointless.\n\nTL;DR: just because modeling faces coherently is doable doesn't mean it's somehow a toy task useless for comparing models.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not &amp;quot;manifold collapse&amp;quot; if the manifold it collapses to is the actual distribution of the data. The fact that the pixel-wise mean face is sensible does mean that modeling faces is less challenging than something as diverse and unstructured as ImageNet, but it does &lt;em&gt;not&lt;/em&gt; mean that every model trained on faces is somehow mode collapsed or that they can&amp;#39;t actually mode collapse. Mode collapse is visually obvious and looks very, very different from the samples presented in this paper; I can dig up and post some of my own mode-collapsed examples if you&amp;#39;d like. Sampling from a collection of points on one manifold is almost the definition of what we want to do in generative modeling--we assume the data exists on some manifold we can approximate with our model, and we want to draw points from it!&lt;/p&gt;\n\n&lt;p&gt;Additionally, pixel-wise interpolations for faces aren&amp;#39;t anywhere near as smooth as interpolations between samples of  a good model. You can easily test this in a slightly more interesting setting if you overfit a convolutional encoder and vary the downsampling factor--even with a 4-8x downsampling, interpolations in the bottleneck space just look like &amp;quot;ghosting&amp;quot; between images rather than the smooth interpolations you see from a good model. The interpolations are, again, smoother than they would be between imagenet images, but they really don&amp;#39;t look anything like GAN interpolations.&lt;/p&gt;\n\n&lt;p&gt;Producing models that can perform well at high resolution on Imagenet is presently not as much a question of the underlying fitting technique (standard GAN vs BEGAN vs WGAN vs autoregressive vs whatever) but model capacity and related peripherals. The fitting technique certainly needs to be able to guide the model distribution towards the true distribution, but the savvy scholar should note that the four highest-image-quality results in the last six months have all come from models trained with &lt;em&gt;massive&lt;/em&gt; amounts of auxiliary superivision. StackGAN and Multiscale PixelCNN have descriptive captions and/or keypoint annotations, and AC-GAN/PPGN take advantage of the imagenet labels or pre-trained networks. Pix2pix, too, suffers from the need for descriptive annotations in one form to get good results (it doesn&amp;#39;t really have a latent space you can sample from, and interpolations suffer accordingly). Getting a fully unsupervised net to produce good results on unstructured images is a question of both capacity and technique, and seeing as neither WGAN nor BEGAN present results on 256x256 Imagenet, drawing comparisons is pointless.&lt;/p&gt;\n\n&lt;p&gt;TL;DR: just because modeling faces coherently is doable doesn&amp;#39;t mean it&amp;#39;s somehow a toy task useless for comparing models.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4uu2j", "score_hidden": false, "stickied": false, "created": 1491970783.0, "created_utc": 1491941983.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 7}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": "", "user_reports": [], "id": "dg70863", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "tmiano", "parent_id": "t1_dg6rmrr", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It matters because it is precisely the reason why mode collapse happens at all: The discriminator learns to map all points in the space in between manifolds to zero, which makes the gradients undefined there. This is also why WGANs (currently) are the only method shown to not have this problem (without say, adding noise to the data) - their gradients always exist and are finite everywhere. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It matters because it is precisely the reason why mode collapse happens at all: The discriminator learns to map all points in the space in between manifolds to zero, which makes the gradients undefined there. This is also why WGANs (currently) are the only method shown to not have this problem (without say, adding noise to the data) - their gradients always exist and are finite everywhere. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg70863", "score_hidden": false, "stickied": false, "created": 1492079524.0, "created_utc": 1492050724.0, "depth": 7, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6rmrr", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "rumblestiltsken", "parent_id": "t1_dg6oby0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Look at the GAN examples of bedrooms (in almost any gan paper), where you can interpolate between complex scenes. \n\nI'm not sure the idea that \"each image on the line is a natural image\" is really valuable though. How real an image looks is pretty subjective, I'm sure plenty of interpolated faces can be found that are impossible. \n\nAnd why would it matter? If you interpolate between giraffes and elephants, you get hybrids. Is that a sign that the task is different, or just that natural images are not continuously distributed? Either way, it doesn't seem to alter learnability. I can definitely learn non-continuous image distributions, like giraffe vs elephant, so why should I care?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Look at the GAN examples of bedrooms (in almost any gan paper), where you can interpolate between complex scenes. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure the idea that &amp;quot;each image on the line is a natural image&amp;quot; is really valuable though. How real an image looks is pretty subjective, I&amp;#39;m sure plenty of interpolated faces can be found that are impossible. &lt;/p&gt;\n\n&lt;p&gt;And why would it matter? If you interpolate between giraffes and elephants, you get hybrids. Is that a sign that the task is different, or just that natural images are not continuously distributed? Either way, it doesn&amp;#39;t seem to alter learnability. I can definitely learn non-continuous image distributions, like giraffe vs elephant, so why should I care?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6rmrr", "score_hidden": false, "stickied": false, "created": 1492068961.0, "created_utc": 1492040161.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6oby0", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "tmiano", "parent_id": "t1_dg5p02d", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yeah I made a mistake in that explanation. I was basically claiming the space of pictures of faces is also *convex*, which it might not be. However, once you induce a mapping from the latent space (which is convex, usually) and pictures of faces, you can then trace a straight-line path through the latent space while each generated picture on that line is also a face. This would not be possible if the space of pictures of faces was not a single (non-disjoint)  manifold. Of course, I haven't seen this tested on other data sets yet, where you trace a line between a giraffe and an elephant, for example. If there is a paper that shows you can do this while generating plausible samples in-between please let me know. ", "edited": 1492038784.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah I made a mistake in that explanation. I was basically claiming the space of pictures of faces is also &lt;em&gt;convex&lt;/em&gt;, which it might not be. However, once you induce a mapping from the latent space (which is convex, usually) and pictures of faces, you can then trace a straight-line path through the latent space while each generated picture on that line is also a face. This would not be possible if the space of pictures of faces was not a single (non-disjoint)  manifold. Of course, I haven&amp;#39;t seen this tested on other data sets yet, where you trace a line between a giraffe and an elephant, for example. If there is a paper that shows you can do this while generating plausible samples in-between please let me know. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6oby0", "score_hidden": false, "stickied": false, "created": 1492064885.0, "created_utc": 1492036085.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5p02d", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "rumblestiltsken", "parent_id": "t1_dg4nzvj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt;very likely that faces in general (as long as they are cropped / centered) are likely to lie on a single manifold\n\nwhat do you mean by this? I haven't heard this terminology before, to say that any space with smooth \"real\" transitions is a \"single manifold\". Image space, particularly when we are talking about deep learning, is fluid. The idea of a \"single manifold\" is pretty meaningless when the shape of a manifold is decided by your function. If all faces can be on a manifold, then so can all animals, right?\n\nDo you just mean that the space of all cropped faces is a lower dimensional space than the space of all animals? ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;very likely that faces in general (as long as they are cropped / centered) are likely to lie on a single manifold&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;what do you mean by this? I haven&amp;#39;t heard this terminology before, to say that any space with smooth &amp;quot;real&amp;quot; transitions is a &amp;quot;single manifold&amp;quot;. Image space, particularly when we are talking about deep learning, is fluid. The idea of a &amp;quot;single manifold&amp;quot; is pretty meaningless when the shape of a manifold is decided by your function. If all faces can be on a manifold, then so can all animals, right?&lt;/p&gt;\n\n&lt;p&gt;Do you just mean that the space of all cropped faces is a lower dimensional space than the space of all animals? &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5p02d", "score_hidden": false, "stickied": false, "created": 1492020713.0, "created_utc": 1491991913.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4nzvj", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "tmiano", "parent_id": "t1_dg4kfb9", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "The problem with face datasets is that it is very likely that faces in general (as long as they are cropped / centered) are likely to lie on a single manifold. This is easy to show since you can almost always find a smooth transition between any two faces with the midpoint of transition also being a face. Where that would *not* be the case for other types of data, for example if each class were a different type of animal. So these types of datasets aren't great for showing that a particular technique does not mode collapse. Another issue is that \"mode\" collapse isn't really the best description for the phenomenon, a better term would be \"manifold collapse\", to indicate that in practice you're not always outputting the same exact point, but rather a collection of points from one manifold. I think that's likely to be the case here. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The problem with face datasets is that it is very likely that faces in general (as long as they are cropped / centered) are likely to lie on a single manifold. This is easy to show since you can almost always find a smooth transition between any two faces with the midpoint of transition also being a face. Where that would &lt;em&gt;not&lt;/em&gt; be the case for other types of data, for example if each class were a different type of animal. So these types of datasets aren&amp;#39;t great for showing that a particular technique does not mode collapse. Another issue is that &amp;quot;mode&amp;quot; collapse isn&amp;#39;t really the best description for the phenomenon, a better term would be &amp;quot;manifold collapse&amp;quot;, to indicate that in practice you&amp;#39;re not always outputting the same exact point, but rather a collection of points from one manifold. I think that&amp;#39;s likely to be the case here. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4nzvj", "score_hidden": false, "stickied": false, "created": 1491963391.0, "created_utc": 1491934591.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 7}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4kfb9", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "ajmooch", "parent_id": "t1_dg4igen", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "That's also a dubious claim, the paper presents a relatively small sample that displays more than enough diversity in pose, lighting, features, etc. that normally aren't present when we observe mode collapse. Bias in the data also tends to get represented in the model, by the way, so given that there's a strong possibility that young female faces are over-represented in the training set (and maybe in \"all HQ face images on the internet\") I wouldn't count the ratio of male/female faces in [Figure 2(b)](https://arxiv.org/abs/1703.10717) as indicative of mode collapse.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s also a dubious claim, the paper presents a relatively small sample that displays more than enough diversity in pose, lighting, features, etc. that normally aren&amp;#39;t present when we observe mode collapse. Bias in the data also tends to get represented in the model, by the way, so given that there&amp;#39;s a strong possibility that young female faces are over-represented in the training set (and maybe in &amp;quot;all HQ face images on the internet&amp;quot;) I wouldn&amp;#39;t count the ratio of male/female faces in &lt;a href=\"https://arxiv.org/abs/1703.10717\"&gt;Figure 2(b)&lt;/a&gt; as indicative of mode collapse.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4kfb9", "score_hidden": false, "stickied": false, "created": 1491959474.0, "created_utc": 1491930674.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 9}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4igen", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "tmiano", "parent_id": "t1_dg4gve7", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It's important to point out too that WGANs don't suffer from mode collapse while the fact that BEGANs produce mostly young, female and average looking faces is a sign of heavy mode collapse. \n\nNot intended to be a criticism. There are balances and tradeoffs between the degree of mode collapse and the degree of distribution coverage. I tend to find that mode collapsey GANs are often better able to produce more *realistic* looking samples even if there is less diversity. ", "edited": 1491934865.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s important to point out too that WGANs don&amp;#39;t suffer from mode collapse while the fact that BEGANs produce mostly young, female and average looking faces is a sign of heavy mode collapse. &lt;/p&gt;\n\n&lt;p&gt;Not intended to be a criticism. There are balances and tradeoffs between the degree of mode collapse and the degree of distribution coverage. I tend to find that mode collapsey GANs are often better able to produce more &lt;em&gt;realistic&lt;/em&gt; looking samples even if there is less diversity. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4igen", "score_hidden": false, "stickied": false, "created": 1491957329.0, "created_utc": 1491928529.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 6}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4gve7", "gilded": 0, "archived": false, "score": 23, "report_reasons": null, "author": "ajmooch", "parent_id": "t3_64rayf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "While the results are good, calling anything in the current generative image landscape \"state of the art\" is dubious even if you do a comparison with the metrics we do have (SSIM, recon losses, Inception scores...a set of unbiased human evaluators are probably still the best we could ask for at the moment). Actually, BEGAN's Inception scores on CIFAR-10 are lower than both [mine](https://arxiv.org/abs/1609.07093) and [DFM by David Warde-Farley](https://openreview.net/forum?id=S1X7nhsxl&amp;noteId=S1X7nhsxl), (which also provides some neat insight into what the inception score is actually measuring IMO moreso than indicating the superiority of one framework over another).\n\nWhat's more, the results presented are from an internally collected, (yet?) unreleased dataset, so comparing them to most other face generators trained on celebA  ignores a hugely important part of the equation. Perhaps most importantly, they have almost twice as many total images as in celebA (definitely twice if they trained on 360k) and the standard aligned-cropped version of celebA most everyone uses has tons of artifacts from the way it was prepared.\n\nIt's also worth pointing out that [dribnet has had high quality 128x128 GAN faces for over 6 months](https://twitter.com/dribnet/status/788349921157980161).\n\nI (perhaps obviously) like BEGAN and this blog post is fine enough, but I disagree with the choice of words for the title.", "edited": 1491927200.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;While the results are good, calling anything in the current generative image landscape &amp;quot;state of the art&amp;quot; is dubious even if you do a comparison with the metrics we do have (SSIM, recon losses, Inception scores...a set of unbiased human evaluators are probably still the best we could ask for at the moment). Actually, BEGAN&amp;#39;s Inception scores on CIFAR-10 are lower than both &lt;a href=\"https://arxiv.org/abs/1609.07093\"&gt;mine&lt;/a&gt; and &lt;a href=\"https://openreview.net/forum?id=S1X7nhsxl&amp;amp;noteId=S1X7nhsxl\"&gt;DFM by David Warde-Farley&lt;/a&gt;, (which also provides some neat insight into what the inception score is actually measuring IMO moreso than indicating the superiority of one framework over another).&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s more, the results presented are from an internally collected, (yet?) unreleased dataset, so comparing them to most other face generators trained on celebA  ignores a hugely important part of the equation. Perhaps most importantly, they have almost twice as many total images as in celebA (definitely twice if they trained on 360k) and the standard aligned-cropped version of celebA most everyone uses has tons of artifacts from the way it was prepared.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also worth pointing out that &lt;a href=\"https://twitter.com/dribnet/status/788349921157980161\"&gt;dribnet has had high quality 128x128 GAN faces for over 6 months&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I (perhaps obviously) like BEGAN and this blog post is fine enough, but I disagree with the choice of words for the title.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4gve7", "score_hidden": false, "stickied": false, "created": 1491955599.0, "created_utc": 1491926799.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 23}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": "", "user_reports": [], "id": "dg4ud3t", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "zergling103", "parent_id": "t3_64rayf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I'd love to see this applied to more complex datasets like bedrooms. Faces don't vary nearly as much in structure.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d love to see this applied to more complex datasets like bedrooms. Faces don&amp;#39;t vary nearly as much in structure.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4ud3t", "score_hidden": false, "stickied": false, "created": 1491970258.0, "created_utc": 1491941458.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": "", "user_reports": [], "id": "dg6z5h3", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "AshkenaziJew", "parent_id": "t1_dg6dkzm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Because their Inception score (pass result into Inception net to classify, good looking images should have low entropy label distribution) didn't beat a [previous work](https://openreview.net/pdf?id=S1X7nhsxl) that proposed essentially a denoising version of their model with auxiliary losses on CIFAR-10.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because their Inception score (pass result into Inception net to classify, good looking images should have low entropy label distribution) didn&amp;#39;t beat a &lt;a href=\"https://openreview.net/pdf?id=S1X7nhsxl\"&gt;previous work&lt;/a&gt; that proposed essentially a denoising version of their model with auxiliary losses on CIFAR-10.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6z5h3", "score_hidden": false, "stickied": false, "created": 1492078194.0, "created_utc": 1492049394.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6dkzm", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "anonDogeLover", "parent_id": "t3_64rayf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Has this been trained on anything else? Why just faces?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Has this been trained on anything else? Why just faces?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6dkzm", "score_hidden": false, "stickied": false, "created": 1492053038.0, "created_utc": 1492024238.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": "", "user_reports": [], "id": "dg5nrqh", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "felardos_loris", "parent_id": "t1_dg55k3i", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Fixed it! Thank you very much!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fixed it! Thank you very much!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5nrqh", "score_hidden": false, "stickied": false, "created": 1492016857.0, "created_utc": 1491988057.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg55k3i", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Ablexxive", "parent_id": "t3_64rayf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "This is a great overview, it helps to break down each of the parts into details. \n\nOne note, in the 'complete BEGAN objective' section, the proportional control algorithm is incorrect. Should be a [gamma\\*L(x) - L(G(z))] between the two losses instead of [gamma\\*L(x) \\* L(G(z))]. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a great overview, it helps to break down each of the parts into details. &lt;/p&gt;\n\n&lt;p&gt;One note, in the &amp;#39;complete BEGAN objective&amp;#39; section, the proportional control algorithm is incorrect. Should be a [gamma*L(x) - L(G(z))] between the two losses instead of [gamma*L(x) * L(G(z))]. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg55k3i", "score_hidden": false, "stickied": false, "created": 1491984178.0, "created_utc": 1491955378.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": "", "user_reports": [], "id": "dg7jnra", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Kiheumi", "parent_id": "t3_64rayf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Here is implementaion of BEGAN. https://github.com/Heumi/BEGAN-tensorflow. \nThis paper's advantage is generator never fail. Because K_t make game fair. But still sensitive to hyper-parameter like weight decay, filter number size and gamma (my experience). Anyway, I'm sure that this is the state of the art GAN method now.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here is implementaion of BEGAN. &lt;a href=\"https://github.com/Heumi/BEGAN-tensorflow\"&gt;https://github.com/Heumi/BEGAN-tensorflow&lt;/a&gt;. \nThis paper&amp;#39;s advantage is generator never fail. Because K_t make game fair. But still sensitive to hyper-parameter like weight decay, filter number size and gamma (my experience). Anyway, I&amp;#39;m sure that this is the state of the art GAN method now.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7jnra", "score_hidden": false, "stickied": false, "created": 1492119326.0, "created_utc": 1492090526.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rayf", "likes": null, "replies": "", "user_reports": [], "id": "dg4ss13", "gilded": 0, "archived": false, "score": -1, "report_reasons": null, "author": "INTP008", "parent_id": "t3_64rayf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "ML for fashion? Really?? Lol...", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ML for fashion? Really?? Lol...&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4ss13", "score_hidden": false, "stickied": false, "created": 1491968526.0, "created_utc": 1491939726.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": -1}}], "after": null, "before": null}}]