[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Research", "id": "64yjn7", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 6, "report_reasons": null, "author": "bobchennan", "saved": false, "mod_reports": [], "name": "t3_64yjn7", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "arxiv.org", "hidden": false, "thumbnail": "default", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "three", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/64yjn7/r170403453_the_space_of_transferable_adversarial/", "num_reports": null, "locked": false, "stickied": false, "created": 1492036765.0, "url": "https://arxiv.org/abs/1704.03453", "author_flair_text": null, "quarantine": false, "title": "[R][1704.03453] The Space of Transferable Adversarial Examples", "created_utc": 1492007965.0, "distinguished": null, "media": null, "upvote_ratio": 0.72, "num_comments": 3, "visited": false, "subreddit_type": "public", "ups": 6}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64yjn7", "likes": null, "replies": "", "user_reports": [], "id": "dg5y4qz", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "arXiv_abstract_bot", "parent_id": "t3_64yjn7", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Title: The Space of Transferable Adversarial Examples  \n\nAuthors: [Florian Tram\u00e8r](http://arxiv.org/find/stat/1/au:+Tramer_F/0/1/0/all/0/1), [Nicolas Papernot](http://arxiv.org/find/stat/1/au:+Papernot_N/0/1/0/all/0/1), [Ian Goodfellow](http://arxiv.org/find/stat/1/au:+Goodfellow_I/0/1/0/all/0/1), [Dan Boneh](http://arxiv.org/find/stat/1/au:+Boneh_D/0/1/0/all/0/1), [Patrick McDaniel](http://arxiv.org/find/stat/1/au:+McDaniel_P/0/1/0/all/0/1)  \n\n&gt; Abstract: Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. Adversarial examples are known to transfer across models: a same perturbed input is often misclassified by different models despite being generated to mislead a specific architecture. This phenomenon enables simple yet powerful black-box attacks against deployed ML systems.   &gt; In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large dimensionality and that a significant fraction of this space is shared between different models, thus enabling transferability.   &gt; The dimensionality of the transferred adversarial subspace implies that the decision boundaries learned by different models are eerily close in the input domain, when moving away from data points in adversarial directions. A first quantitative analysis of the similarity of different models' decision boundaries reveals that these boundaries are actually close in arbitrary directions, whether adversarial or benign.   &gt; We conclude with a formal study of the limits of transferability. We show (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of tasks for which transferability fails to hold. This suggests the existence of defenses making models robust to transferability attacks---even when the model is not robust to its own adversarial examples.  \n\n[PDF link](https://arxiv.org/pdf/1704.03453)  [Landing page](https://arxiv.org/abs/1704.03453)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Title: The Space of Transferable Adversarial Examples  &lt;/p&gt;\n\n&lt;p&gt;Authors: &lt;a href=\"http://arxiv.org/find/stat/1/au:+Tramer_F/0/1/0/all/0/1\"&gt;Florian Tram\u00e8r&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/stat/1/au:+Papernot_N/0/1/0/all/0/1\"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/stat/1/au:+Goodfellow_I/0/1/0/all/0/1\"&gt;Ian Goodfellow&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/stat/1/au:+Boneh_D/0/1/0/all/0/1\"&gt;Dan Boneh&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/stat/1/au:+McDaniel_P/0/1/0/all/0/1\"&gt;Patrick McDaniel&lt;/a&gt;  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Abstract: Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. Adversarial examples are known to transfer across models: a same perturbed input is often misclassified by different models despite being generated to mislead a specific architecture. This phenomenon enables simple yet powerful black-box attacks against deployed ML systems.   &amp;gt; In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large dimensionality and that a significant fraction of this space is shared between different models, thus enabling transferability.   &amp;gt; The dimensionality of the transferred adversarial subspace implies that the decision boundaries learned by different models are eerily close in the input domain, when moving away from data points in adversarial directions. A first quantitative analysis of the similarity of different models&amp;#39; decision boundaries reveals that these boundaries are actually close in arbitrary directions, whether adversarial or benign.   &amp;gt; We conclude with a formal study of the limits of transferability. We show (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of tasks for which transferability fails to hold. This suggests the existence of defenses making models robust to transferability attacks---even when the model is not robust to its own adversarial examples.  &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/1704.03453\"&gt;PDF link&lt;/a&gt;  &lt;a href=\"https://arxiv.org/abs/1704.03453\"&gt;Landing page&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5y4qz", "score_hidden": false, "stickied": false, "created": 1492036781.0, "created_utc": 1492007981.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64yjn7", "likes": null, "replies": "", "user_reports": [], "id": "dg63w2m", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "pull_request", "parent_id": "t3_64yjn7", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Is there a reason: https://arxiv.org/abs/1610.08401 is not cited here?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is there a reason: &lt;a href=\"https://arxiv.org/abs/1610.08401\"&gt;https://arxiv.org/abs/1610.08401&lt;/a&gt; is not cited here?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg63w2m", "score_hidden": false, "stickied": false, "created": 1492043114.0, "created_utc": 1492014314.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64yjn7", "likes": null, "replies": "", "user_reports": [], "id": "dg7bhzm", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "WenfeiXie", "parent_id": "t3_64yjn7", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "How important is this paper, in terms of addressing adversarial attacks?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How important is this paper, in terms of addressing adversarial attacks?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7bhzm", "score_hidden": false, "stickied": false, "created": 1492101160.0, "created_utc": 1492072360.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]