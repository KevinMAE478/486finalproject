[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using LSTM in tensorflow to address a seq2seq problem where input is 1000-length sequence data and outputting 1000-length corresponding values as predictions. &lt;/p&gt;\n\n&lt;p&gt;My current architecture has 5 LSTM layers as we found accuracy improved with number of layers. and right now I have [200, 100, 50, 25, 10] as cell sizes for these 5 layers. Here is a &lt;a href=\"http://imgur.com/XKxzg7F\"&gt;plot!&lt;/a&gt; of training and val cost (MSE). We first use dropout keep_prob of 0.5 at each layer, we trained to a point where the training cost plateau at 0.01. Then I changed dropout keep_prob to 0.8, the training cost decreased to 0.003. However, the test cost has been at 0.014 for a while. I was wondering if there is anything I can do to bring down the val cost a bit more? Or that you think the val cost is about as low as it should be (Is there a statistical way to justify this)? &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "I am using LSTM in tensorflow to address a seq2seq problem where input is 1000-length sequence data and outputting 1000-length corresponding values as predictions. \n\nMy current architecture has 5 LSTM layers as we found accuracy improved with number of layers. and right now I have [200, 100, 50, 25, 10] as cell sizes for these 5 layers. Here is a [plot!](http://imgur.com/XKxzg7F) of training and val cost (MSE). We first use dropout keep_prob of 0.5 at each layer, we trained to a point where the training cost plateau at 0.01. Then I changed dropout keep_prob to 0.8, the training cost decreased to 0.003. However, the test cost has been at 0.014 for a while. I was wondering if there is anything I can do to bring down the val cost a bit more? Or that you think the val cost is about as low as it should be (Is there a statistical way to justify this)? \n\nThanks.", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discusssion", "id": "65wq1o", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 1, "report_reasons": null, "author": "wencc", "saved": false, "mod_reports": [], "name": "t3_65wq1o", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/0dsR2cT4Pjs4DEScZcL0qGGpEAYGxZ3Foi5XDvwcoOs.jpg?s=675cac361071d5a6b5e12c3079944b65", "width": 2100, "height": 816}, "resolutions": [{"url": "https://i.redditmedia.com/0dsR2cT4Pjs4DEScZcL0qGGpEAYGxZ3Foi5XDvwcoOs.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=aaf9e27c310a31dcfabdb519d1d49940", "width": 108, "height": 41}, {"url": "https://i.redditmedia.com/0dsR2cT4Pjs4DEScZcL0qGGpEAYGxZ3Foi5XDvwcoOs.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=3308451c2943389c730bc765af7b6aef", "width": 216, "height": 83}, {"url": "https://i.redditmedia.com/0dsR2cT4Pjs4DEScZcL0qGGpEAYGxZ3Foi5XDvwcoOs.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=1522b98a2b38807f186c6279f87e5f6c", "width": 320, "height": 124}, {"url": "https://i.redditmedia.com/0dsR2cT4Pjs4DEScZcL0qGGpEAYGxZ3Foi5XDvwcoOs.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=50130e7f68aa5b6c2d3a56e15949b7b0", "width": 640, "height": 248}, {"url": "https://i.redditmedia.com/0dsR2cT4Pjs4DEScZcL0qGGpEAYGxZ3Foi5XDvwcoOs.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=960&amp;s=8c5327c27185652f110db2fa2e01a6d6", "width": 960, "height": 373}, {"url": "https://i.redditmedia.com/0dsR2cT4Pjs4DEScZcL0qGGpEAYGxZ3Foi5XDvwcoOs.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=1080&amp;s=be3e5848eeeba7e973af9948ef6e207a", "width": 1080, "height": 419}], "variants": {}, "id": "Rabc4tjiZmQBivhC2d82z9UQ5qteoHVElbxolaCFNpQ"}], "enabled": false}, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "self", "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65wq1o/d_ways_to_bring_down_validation_cost_in/", "num_reports": null, "locked": false, "stickied": false, "created": 1492476489.0, "url": "https://www.reddit.com/r/MachineLearning/comments/65wq1o/d_ways_to_bring_down_validation_cost_in/", "author_flair_text": null, "quarantine": false, "title": "[D] Ways to bring down validation cost in multilayer LSTM training", "created_utc": 1492447689.0, "distinguished": null, "media": null, "upvote_ratio": 1.0, "num_comments": 2, "visited": false, "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65wq1o", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65wq1o", "likes": null, "replies": "", "user_reports": [], "id": "dgejvdj", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "wencc", "parent_id": "t1_dgdu73c", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I am only applying dropout to the outputs. So I use\n    layer_cell = tf.contrib.rnn.DropoutWrapper(layer_cell, output_keep_prob=keep_prob)\nfor each layer.\n\nThe paper looks interesting. I would definitely look into it.\n\nI haven't thought about skip connections, it's an interesting idea. I am curious about what's the benefit of skip connections in LSTM?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am only applying dropout to the outputs. So I use\n    layer_cell = tf.contrib.rnn.DropoutWrapper(layer_cell, output_keep_prob=keep_prob)\nfor each layer.&lt;/p&gt;\n\n&lt;p&gt;The paper looks interesting. I would definitely look into it.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t thought about skip connections, it&amp;#39;s an interesting idea. I am curious about what&amp;#39;s the benefit of skip connections in LSTM?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgejvdj", "score_hidden": false, "stickied": false, "created": 1492511242.0, "created_utc": 1492482442.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdu73c", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "pavelchristof", "parent_id": "t3_65wq1o", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Are you applying dropout just to the outputs or also the state, like in https://arxiv.org/abs/1512.05287? \n\nAlso I'd add skip connections between the layers. For example use cell sizes [128] * 10 and wrap all but the first layer with ResidualWrapper:\n\n    cells = [tf.contrib.rnn.LSTMCell(**lstm_args)]\n    for _ in range(9):\n      cells.append(tf.contrib.rnn.ResidualWrapper(\n          tf.contrib.rnn.LSTMCell(**lstm_args)))\n    cell = tf.contrib.rnn.MultiRNNCell(cells)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you applying dropout just to the outputs or also the state, like in &lt;a href=\"https://arxiv.org/abs/1512.05287\"&gt;https://arxiv.org/abs/1512.05287&lt;/a&gt;? &lt;/p&gt;\n\n&lt;p&gt;Also I&amp;#39;d add skip connections between the layers. For example use cell sizes [128] * 10 and wrap all but the first layer with ResidualWrapper:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;cells = [tf.contrib.rnn.LSTMCell(**lstm_args)]\nfor _ in range(9):\n  cells.append(tf.contrib.rnn.ResidualWrapper(\n      tf.contrib.rnn.LSTMCell(**lstm_args)))\ncell = tf.contrib.rnn.MultiRNNCell(cells)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdu73c", "score_hidden": false, "stickied": false, "created": 1492480442.0, "created_utc": 1492451642.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]