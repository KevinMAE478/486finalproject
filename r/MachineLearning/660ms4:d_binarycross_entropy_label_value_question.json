[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m new to machine learning field and was looking at autoencoder in this blog &lt;a href=\"https://blog.keras.io/building-autoencoders-in-keras.html\"&gt;https://blog.keras.io/building-autoencoders-in-keras.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a little confused about why using binary cross entropy as cost function. &lt;/p&gt;\n\n&lt;p&gt;In the training data, each value is between 0 and 1. But what I understand is when using binary cross entropy, the label is 0 or 1.&lt;/p&gt;\n\n&lt;p&gt;Can anyone correct me if I am wrong?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "Hi, I'm new to machine learning field and was looking at autoencoder in this blog https://blog.keras.io/building-autoencoders-in-keras.html\n\nI'm a little confused about why using binary cross entropy as cost function. \n\nIn the training data, each value is between 0 and 1. But what I understand is when using binary cross entropy, the label is 0 or 1.\n\nCan anyone correct me if I am wrong?\n\nThanks", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "660ms4", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 2, "report_reasons": null, "author": "anericanohuang", "saved": false, "mod_reports": [], "name": "t3_660ms4", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/9CYnIrVeGeLyhZ0cPa9k0B_7BqX_-cjLXHGHXTBpVC0.jpg?s=31fcf3d14ad6bef61eb3c42e2f22b942", "width": 800, "height": 511}, "resolutions": [{"url": "https://i.redditmedia.com/9CYnIrVeGeLyhZ0cPa9k0B_7BqX_-cjLXHGHXTBpVC0.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=51b330594a96f7f5cfb7e8127dcbefca", "width": 108, "height": 68}, {"url": "https://i.redditmedia.com/9CYnIrVeGeLyhZ0cPa9k0B_7BqX_-cjLXHGHXTBpVC0.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=cdd58a3269f95eb6cce63d6ed6d5a962", "width": 216, "height": 137}, {"url": "https://i.redditmedia.com/9CYnIrVeGeLyhZ0cPa9k0B_7BqX_-cjLXHGHXTBpVC0.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=2471826c6850386a00be9a27721112b9", "width": 320, "height": 204}, {"url": "https://i.redditmedia.com/9CYnIrVeGeLyhZ0cPa9k0B_7BqX_-cjLXHGHXTBpVC0.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=bfa078d63d0557fb3f44c8fd2a75becf", "width": 640, "height": 408}], "variants": {}, "id": "n5bKSvFVIq8dSM_4-j5Xg_g0LhA-y_Nl9KNYJSIaWoo"}], "enabled": false}, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "self", "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/660ms4/d_binarycross_entropy_label_value_question/", "num_reports": null, "locked": false, "stickied": false, "created": 1492517322.0, "url": "https://www.reddit.com/r/MachineLearning/comments/660ms4/d_binarycross_entropy_label_value_question/", "author_flair_text": null, "quarantine": false, "title": "[D] binary-cross entropy label value question", "created_utc": 1492488522.0, "distinguished": null, "media": null, "upvote_ratio": 1.0, "num_comments": 2, "visited": false, "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_660ms4", "likes": null, "replies": "", "user_reports": [], "id": "dgepdp2", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "aviniumau", "parent_id": "t3_660ms4", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "http://neuralnetworksanddeeplearning.com/chap3.html\n\n&gt; ...the cross-entropy is small if \u03c3(z)\u2248y\u03c3(z)\u2248y for all training inputs. The argument relied on y being equal to either 0 or 1. This is usually true in classification problems, but for other problems (e.g., regression problems) y can sometimes take values intermediate between 0 and 1....the cross-entropy is still minimized when \u03c3(z)=y\u03c3(z)=y for all training inputs. \n&gt; The quantity \u2212[ylny+(1\u2212y)ln(1\u2212y)]\u2212[yln\u2061y+(1\u2212y)ln\u2061(1\u2212y)] is sometimes known as the binary entropy.\n\n(I removed the equations but you can follow the link to work through the maths if you want).\n\nSo in other words, binary cross-entropy can be applied to an autoencoder because it minimizes the difference between the reconstruction and the input.\n\nIn the purely binary case, then binary cross-entropy loss penalizes a white-pixel output when the input was black, and vice versa. \n\nIf the inputs/autoencoded outputs are real-valued pixel intensities normalized between 0 and 1, then the same will apply - but it will also penalize a \"very dark grey\" pixel output when the input was \"very light grey\".", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"http://neuralnetworksanddeeplearning.com/chap3.html\"&gt;http://neuralnetworksanddeeplearning.com/chap3.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;...the cross-entropy is small if \u03c3(z)\u2248y\u03c3(z)\u2248y for all training inputs. The argument relied on y being equal to either 0 or 1. This is usually true in classification problems, but for other problems (e.g., regression problems) y can sometimes take values intermediate between 0 and 1....the cross-entropy is still minimized when \u03c3(z)=y\u03c3(z)=y for all training inputs. \nThe quantity \u2212[ylny+(1\u2212y)ln(1\u2212y)]\u2212[yln\u2061y+(1\u2212y)ln\u2061(1\u2212y)] is sometimes known as the binary entropy.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;(I removed the equations but you can follow the link to work through the maths if you want).&lt;/p&gt;\n\n&lt;p&gt;So in other words, binary cross-entropy can be applied to an autoencoder because it minimizes the difference between the reconstruction and the input.&lt;/p&gt;\n\n&lt;p&gt;In the purely binary case, then binary cross-entropy loss penalizes a white-pixel output when the input was black, and vice versa. &lt;/p&gt;\n\n&lt;p&gt;If the inputs/autoencoded outputs are real-valued pixel intensities normalized between 0 and 1, then the same will apply - but it will also penalize a &amp;quot;very dark grey&amp;quot; pixel output when the input was &amp;quot;very light grey&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgepdp2", "score_hidden": false, "stickied": false, "created": 1492518799.0, "created_utc": 1492489999.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_660ms4", "likes": null, "replies": "", "user_reports": [], "id": "dgepvj4", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "rui_", "parent_id": "t3_660ms4", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I suggest reading about the definition of cross entropy and as well as the notion of KL divergence between two distributions.\n\nUsually people learn about cross entropy within the context of classification, where the data distribution is discrete (and often binary). But it's important to bear in mind that cross entropy applies so long as you're thinking about minimizing the distance between two distributions! :)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I suggest reading about the definition of cross entropy and as well as the notion of KL divergence between two distributions.&lt;/p&gt;\n\n&lt;p&gt;Usually people learn about cross entropy within the context of classification, where the data distribution is discrete (and often binary). But it&amp;#39;s important to bear in mind that cross entropy applies so long as you&amp;#39;re thinking about minimizing the distance between two distributions! :)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgepvj4", "score_hidden": false, "stickied": false, "created": 1492519572.0, "created_utc": 1492490772.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}]