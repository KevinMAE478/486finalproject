[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "News", "id": "64xalr", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 138, "report_reasons": null, "author": "wb14123", "saved": false, "mod_reports": [], "name": "t3_64xalr", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "opensource.googleblog.com", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/fUp3RTOuBqc5fk0i3WpbPGweRl_ETGPWzwk7f1NF-DE.jpg?s=152e5e124b61a23bda4c9c4e4060e7c6", "width": 200, "height": 200}, "resolutions": [{"url": "https://i.redditmedia.com/fUp3RTOuBqc5fk0i3WpbPGweRl_ETGPWzwk7f1NF-DE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=13efe9729ef1a6b7ea47080d8686f037", "width": 108, "height": 108}], "variants": {}, "id": "6NlnAaGfxcrmD-tzx6sapN7FXxED23clZMbub2dIRZ0"}], "enabled": false}, "thumbnail": "https://a.thumbs.redditmedia.com/Y_mltIVoN2xPpzhftDjNf2eiAUwBTF_ZMuvoNkbd-n8.jpg", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "two", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "link", "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/64xalr/n_google_open_sources_a_seq2seq_framework_in/", "num_reports": null, "locked": false, "stickied": false, "created": 1492021278.0, "url": "https://opensource.googleblog.com/2017/04/tf-seq2seq-sequence-to-sequence-framework-in-tensorflow.html", "author_flair_text": null, "quarantine": false, "title": "[N] Google open sources a seq2seq framework in Tensorflow", "created_utc": 1491992478.0, "distinguished": null, "media": null, "upvote_ratio": 0.95, "num_comments": 13, "visited": false, "subreddit_type": "public", "ups": 138}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": "", "user_reports": [], "id": "dg5rxvb", "gilded": 0, "archived": false, "score": 13, "report_reasons": null, "author": "harharveryfunny", "parent_id": "t3_64xalr", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It would be nice to have more detailed visualizations of the models/variants supported by this framework.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It would be nice to have more detailed visualizations of the models/variants supported by this framework.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5rxvb", "score_hidden": false, "stickied": false, "created": 1492027922.0, "created_utc": 1491999122.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 13}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": "", "user_reports": [], "id": "dg609v5", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "gizcard", "parent_id": "t1_dg5wck8", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yes, but the hard part is of course in details.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, but the hard part is of course in details.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg609v5", "score_hidden": false, "stickied": false, "created": 1492039195.0, "created_utc": 1492010395.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": "", "user_reports": [], "id": "dg6130d", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "frownyface", "parent_id": "t1_dg5wck8", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "They more or less give that as an example of what's possible in the blog post. \n\n&gt; In addition to machine translation, tf-seq2seq can also be applied to any other sequence-to-sequence task (i.e. learning to produce an output sequence given an input sequence), including machine summarization, image captioning, speech recognition, and conversational modeling.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;They more or less give that as an example of what&amp;#39;s possible in the blog post. &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;In addition to machine translation, tf-seq2seq can also be applied to any other sequence-to-sequence task (i.e. learning to produce an output sequence given an input sequence), including machine summarization, image captioning, speech recognition, and conversational modeling.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6130d", "score_hidden": false, "stickied": false, "created": 1492040080.0, "created_utc": 1492011280.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": "", "user_reports": [], "id": "dg64g74", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "kkastner", "parent_id": "t1_dg5wck8", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Jan Chorowski has a lot of papers [along these lines](https://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1), and [Listen Attend Spell (Chan et. al.)](https://arxiv.org/abs/1508.01211) is also a demonstration of such a system. Usually a modified attention is needed, and directly from samples is probably not computationally feasible. Most systems use high level features like MFCC, or at least spectrograms as input for the audio part. You can also see CTC as a joint decoder + loss, specifically for problems with output size &lt; input size. Treating the convolutional or recurrent parts as the \"encoder\" and CTC as the decoder, these CTC based approaches end up looking a lot like seq2seq/enc-dec as well - which describes a *lot* of modern approaches for speech recognition.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Jan Chorowski has a lot of papers &lt;a href=\"https://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1\"&gt;along these lines&lt;/a&gt;, and &lt;a href=\"https://arxiv.org/abs/1508.01211\"&gt;Listen Attend Spell (Chan et. al.)&lt;/a&gt; is also a demonstration of such a system. Usually a modified attention is needed, and directly from samples is probably not computationally feasible. Most systems use high level features like MFCC, or at least spectrograms as input for the audio part. You can also see CTC as a joint decoder + loss, specifically for problems with output size &amp;lt; input size. Treating the convolutional or recurrent parts as the &amp;quot;encoder&amp;quot; and CTC as the decoder, these CTC based approaches end up looking a lot like seq2seq/enc-dec as well - which describes a &lt;em&gt;lot&lt;/em&gt; of modern approaches for speech recognition.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg64g74", "score_hidden": false, "stickied": false, "created": 1492043697.0, "created_utc": 1492014897.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5wck8", "gilded": 0, "archived": false, "score": 10, "report_reasons": null, "author": "fake-shoes", "parent_id": "t3_64xalr", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "This is pretty cool. I'm curious if rather than text to text for sequences, you could treat audio files as a time-sequence to output text to use this to create a Speech-To-Text model. Would that work?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is pretty cool. I&amp;#39;m curious if rather than text to text for sequences, you could treat audio files as a time-sequence to output text to use this to create a Speech-To-Text model. Would that work?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5wck8", "score_hidden": false, "stickied": false, "created": 1492034630.0, "created_utc": 1492005830.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 10}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": "", "user_reports": [], "id": "dg71gy5", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "huyouare", "parent_id": "t1_dg6jd3o", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It seems to me that this framework (tf-seq2seq) is more of a black-box interface similar to building models in Caffe. After installing, you define the model with a YAML/JSON file. You then can train, test, and visualize without touching the code. The article mentions that the framework's versatility allowed them to achieve the results in their \"Massive Exploration of Neural Machine Translation\nArchitectures\" paper. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It seems to me that this framework (tf-seq2seq) is more of a black-box interface similar to building models in Caffe. After installing, you define the model with a YAML/JSON file. You then can train, test, and visualize without touching the code. The article mentions that the framework&amp;#39;s versatility allowed them to achieve the results in their &amp;quot;Massive Exploration of Neural Machine Translation\nArchitectures&amp;quot; paper. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg71gy5", "score_hidden": false, "stickied": false, "created": 1492081144.0, "created_utc": 1492052344.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 4}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6jd3o", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "huyouare", "parent_id": "t3_64xalr", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Can someone summarize the benefits of this over the seq2seq methods in tf.contrib?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can someone summarize the benefits of this over the seq2seq methods in tf.contrib?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6jd3o", "score_hidden": false, "stickied": false, "created": 1492058990.0, "created_utc": 1492030190.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": "", "user_reports": [], "id": "dg5xumt", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "sbt_", "parent_id": "t1_dg5wpvu", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You are taking this a bit out of the context ;). They refer to their research in https://arxiv.org/abs/1609.08144\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You are taking this a bit out of the context ;). They refer to their research in &lt;a href=\"https://arxiv.org/abs/1609.08144\"&gt;https://arxiv.org/abs/1609.08144&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5xumt", "score_hidden": false, "stickied": false, "created": 1492036453.0, "created_utc": 1492007653.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 7}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": "", "user_reports": [], "id": "dg61fjs", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Jean-Porte", "parent_id": "t1_dg5wpvu", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Really decent Tensorflow seq2seq were available too", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Really decent Tensorflow seq2seq were available too&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg61fjs", "score_hidden": false, "stickied": false, "created": 1492040464.0, "created_utc": 1492011664.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5wpvu", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "metacurse", "parent_id": "t3_64xalr", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "What do they mean by \n\n&gt; While GNMT achieved huge improvements in translation quality, its impact was limited by the fact that the framework for training these models was unavailable to external researchers.\n\nMy seq2seq models in Theano and PyTorch work just fine", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What do they mean by &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;While GNMT achieved huge improvements in translation quality, its impact was limited by the fact that the framework for training these models was unavailable to external researchers.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;My seq2seq models in Theano and PyTorch work just fine&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5wpvu", "score_hidden": false, "stickied": false, "created": 1492035098.0, "created_utc": 1492006298.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": "", "user_reports": [], "id": "dg64l4t", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "lysecret", "parent_id": "t3_64xalr", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Hey des anyone know if this can do many to one? Should be right? ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey des anyone know if this can do many to one? Should be right? &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg64l4t", "score_hidden": false, "stickied": false, "created": 1492043842.0, "created_utc": 1492015042.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64xalr", "likes": null, "replies": "", "user_reports": [], "id": "dg7f2vv", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Jean-Porte", "parent_id": "t1_dg737x0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Maybe the beam search overfits the likelihood, yielding sentences with low likelihood but corresponding to a sharp area of the likelihood surface, while moderate beam size yield sentences with a slightly higher likelihood corresponding to a smoother area in the likelihood surface", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe the beam search overfits the likelihood, yielding sentences with low likelihood but corresponding to a sharp area of the likelihood surface, while moderate beam size yield sentences with a slightly higher likelihood corresponding to a smoother area in the likelihood surface&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7f2vv", "score_hidden": false, "stickied": false, "created": 1492111523.0, "created_utc": 1492082723.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg737x0", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "changoplatanero", "parent_id": "t3_64xalr", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Its weird to me that when the beam size is too large then the performance goes down. Is this a quirk of bleu score or is there some explanation?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Its weird to me that when the beam size is too large then the performance goes down. Is this a quirk of bleu score or is there some explanation?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg737x0", "score_hidden": false, "stickied": false, "created": 1492083581.0, "created_utc": 1492054781.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]