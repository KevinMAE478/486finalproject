[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I put together some math to show what I&amp;#39;m talking about: &lt;a href=\"https://drive.google.com/file/d/0BwbWRPtraa2zeDhaUWFUVl94ZUk/view?usp=sharing\"&gt;https://drive.google.com/file/d/0BwbWRPtraa2zeDhaUWFUVl94ZUk/view?usp=sharing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TL;DR: the earlier the timestep, the more number of forget gate terms present in our derivative that multiply together. If one of these is equal or close to 0, then the whole gradient dies. How is this not an issue, if we train the forget gate [weights] simultaneously? Even if we set a really large forget bias, as the training progresses it&amp;#39;ll correct this and make the forget gate at a timestep closer to what is optimal. Eg. if learn that f_10 should be 0, then the whole thing dies, even though we&amp;#39;re still making contributions albeit small.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "I put together some math to show what I'm talking about: https://drive.google.com/file/d/0BwbWRPtraa2zeDhaUWFUVl94ZUk/view?usp=sharing\n\nTL;DR: the earlier the timestep, the more number of forget gate terms present in our derivative that multiply together. If one of these is equal or close to 0, then the whole gradient dies. How is this not an issue, if we train the forget gate [weights] simultaneously? Even if we set a really large forget bias, as the training progresses it'll correct this and make the forget gate at a timestep closer to what is optimal. Eg. if learn that f_10 should be 0, then the whole thing dies, even though we're still making contributions albeit small.\n\nAm I missing something here?\n", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "655iih", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 20, "report_reasons": null, "author": "sup6978", "saved": false, "mod_reports": [], "name": "t3_655iih", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/LSCkD_zjb5quafYbF2kFVZ4r2ROb1RrNzC7PvZW8sw8.jpg?s=6eece84306db3a7e12a14a8d7358c2b5", "width": 1200, "height": 630}, "resolutions": [{"url": "https://i.redditmedia.com/LSCkD_zjb5quafYbF2kFVZ4r2ROb1RrNzC7PvZW8sw8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=e0cfd7f8b557dac0f9dd4e2c5afdf6e2", "width": 108, "height": 56}, {"url": "https://i.redditmedia.com/LSCkD_zjb5quafYbF2kFVZ4r2ROb1RrNzC7PvZW8sw8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=b45455123c41a09695764613d0344781", "width": 216, "height": 113}, {"url": "https://i.redditmedia.com/LSCkD_zjb5quafYbF2kFVZ4r2ROb1RrNzC7PvZW8sw8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=1b5240626c785fff56c4f236fab24b28", "width": 320, "height": 168}, {"url": "https://i.redditmedia.com/LSCkD_zjb5quafYbF2kFVZ4r2ROb1RrNzC7PvZW8sw8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=9cf2b22b5075350966a7a82546032447", "width": 640, "height": 336}, {"url": "https://i.redditmedia.com/LSCkD_zjb5quafYbF2kFVZ4r2ROb1RrNzC7PvZW8sw8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=960&amp;s=85c5bc11a9e41085c6aec617cc430973", "width": 960, "height": 504}, {"url": "https://i.redditmedia.com/LSCkD_zjb5quafYbF2kFVZ4r2ROb1RrNzC7PvZW8sw8.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=1080&amp;s=1360a81cf06bf65a14cbd2be03bd0a93", "width": 1080, "height": 567}], "variants": {}, "id": "i5acoG9PkPrLmh0opVfKE4odpYvl0xzaM_6_9ChDm5g"}], "enabled": false}, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": 1492091038.0, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "self", "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/655iih/d_why_doesnt_lstm_forget_gate_cause_a/", "num_reports": null, "locked": false, "stickied": false, "created": 1492119500.0, "url": "https://www.reddit.com/r/MachineLearning/comments/655iih/d_why_doesnt_lstm_forget_gate_cause_a/", "author_flair_text": null, "quarantine": false, "title": "[D] Why doesn't LSTM forget gate cause a vanishing/dying gradient?", "created_utc": 1492090700.0, "distinguished": null, "media": null, "upvote_ratio": 0.76, "num_comments": 13, "visited": false, "subreddit_type": "public", "ups": 20}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": "", "user_reports": [], "id": "dg7kp3c", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg7khrs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": " Thank you :)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you :)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7kp3c", "score_hidden": false, "stickied": false, "created": 1492120674.0, "created_utc": 1492091874.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": "", "user_reports": [], "id": "dg9f5et", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "yetipirate", "parent_id": "t1_dg9c5la", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "So for sequences with longer range signal init the reset and update bias to be large? Hmm", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So for sequences with longer range signal init the reset and update bias to be large? Hmm&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9f5et", "score_hidden": false, "stickied": false, "created": 1492218469.0, "created_utc": 1492189669.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9c5la", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "pranv", "parent_id": "t1_dg93ei2", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "yeah, for all theoretical purposes and intents, GRUs are a special case of LSTMs where the input and forget gates are coupled and there is no output gate.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah, for all theoretical purposes and intents, GRUs are a special case of LSTMs where the input and forget gates are coupled and there is no output gate.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9c5la", "score_hidden": false, "stickied": false, "created": 1492215124.0, "created_utc": 1492186324.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg93ei2", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "yetipirate", "parent_id": "t1_dg7khrs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Do grus have this issue?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Do grus have this issue?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg93ei2", "score_hidden": false, "stickied": false, "created": 1492203497.0, "created_utc": 1492174697.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg7khrs", "gilded": 0, "archived": false, "score": 30, "report_reasons": null, "author": "pranv", "parent_id": "t3_655iih", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It does happen - introducing forget gates takes out the guarantee that LSTM can persist gradients and not vanish. If we know that there would be long delays in feedback, we should initialize the forget gate bias to a large value so that the forget gate is 1 early on during the training for a period of time. A lot of papers do this and even mention it explicitly and this value can be considered as a hyper parameter. This is the cost that we pay for the capability of \"learning to forget\".\n\nWhy doesn't this make it worse or same as a vanilla RNN? It is because the gradient decay is much worse in vanilla RNN and is is much easier in an LSTM to overcome this by training (\"Learning not to forget\").", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It does happen - introducing forget gates takes out the guarantee that LSTM can persist gradients and not vanish. If we know that there would be long delays in feedback, we should initialize the forget gate bias to a large value so that the forget gate is 1 early on during the training for a period of time. A lot of papers do this and even mention it explicitly and this value can be considered as a hyper parameter. This is the cost that we pay for the capability of &amp;quot;learning to forget&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Why doesn&amp;#39;t this make it worse or same as a vanilla RNN? It is because the gradient decay is much worse in vanilla RNN and is is much easier in an LSTM to overcome this by training (&amp;quot;Learning not to forget&amp;quot;).&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7khrs", "score_hidden": false, "stickied": false, "created": 1492120417.0, "created_utc": 1492091617.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 30}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": "", "user_reports": [], "id": "dga41t2", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg9xmy4", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yup -- it's for one cell.\n\nThanks! That makes sense.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yup -- it&amp;#39;s for one cell.&lt;/p&gt;\n\n&lt;p&gt;Thanks! That makes sense.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dga41t2", "score_hidden": false, "stickied": false, "created": 1492249946.0, "created_utc": 1492221146.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9xmy4", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "tshadley", "parent_id": "t1_dg8o6ya", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "\n&gt; It makes sense that the forget gate exists, but during training would it not interrupt gradient flow?\n\nYes, 1 forget-gate would interrupt gradient flow to 1 cell and that is a good thing if the network has learned that gradient from the future has no benefit to that particular cell.\n\n&gt; Definitely, but if you look at the PDF I put in the post, can you see that, for example, if one of the forget gates approximates zero, then suddenly one of the cell states isn't making any contributions at all to the gradients to update W_xi/other weights/etc.? Is that not an issue?\n\nI assume your equation is for 1 cell.  But LSTM typically is a vector of many cells so having 1 cell shut off the gradient is fine if that cell only cares about certain limited context.  The other cells will go one using the full gradient (assuming their forget-gates have not attentuated it).   In practice, different cells learn different ranges of context, some short, some long.   Am I getting your point?\n\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;It makes sense that the forget gate exists, but during training would it not interrupt gradient flow?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Yes, 1 forget-gate would interrupt gradient flow to 1 cell and that is a good thing if the network has learned that gradient from the future has no benefit to that particular cell.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Definitely, but if you look at the PDF I put in the post, can you see that, for example, if one of the forget gates approximates zero, then suddenly one of the cell states isn&amp;#39;t making any contributions at all to the gradients to update W_xi/other weights/etc.? Is that not an issue?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I assume your equation is for 1 cell.  But LSTM typically is a vector of many cells so having 1 cell shut off the gradient is fine if that cell only cares about certain limited context.  The other cells will go one using the full gradient (assuming their forget-gates have not attentuated it).   In practice, different cells learn different ranges of context, some short, some long.   Am I getting your point?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9xmy4", "score_hidden": false, "stickied": false, "created": 1492241314.0, "created_utc": 1492212514.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8o6ya", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg8fqpz", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thank you for replying. I want to pick your brain about this, because I'm super curious!\n\nIt makes sense that the forget gate exists, but during training would it not interrupt gradient flow?\n\n&gt; For backprop, those cells don't care about future sentences, they only care about the current one so shutting off gradient from the future allows them to learn faster by focusing on more narrow context.\n\nDefinitely, but if you look at the PDF I put in the post, can you see that, for example, if one of the forget gates approximates zero, then suddenly one of the cell states isn't making any contributions at all to the gradients to update W_xi/other weights/etc.? Is that not an issue? ", "edited": 1492141273.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you for replying. I want to pick your brain about this, because I&amp;#39;m super curious!&lt;/p&gt;\n\n&lt;p&gt;It makes sense that the forget gate exists, but during training would it not interrupt gradient flow?&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;For backprop, those cells don&amp;#39;t care about future sentences, they only care about the current one so shutting off gradient from the future allows them to learn faster by focusing on more narrow context.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Definitely, but if you look at the PDF I put in the post, can you see that, for example, if one of the forget gates approximates zero, then suddenly one of the cell states isn&amp;#39;t making any contributions at all to the gradients to update W_xi/other weights/etc.? Is that not an issue? &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8o6ya", "score_hidden": false, "stickied": false, "created": 1492167945.0, "created_utc": 1492139145.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8fqpz", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "tshadley", "parent_id": "t3_655iih", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; Even if we set a really large forget bias, as the training progresses it'll correct this and make the forget gate at a timestep closer to what is optimal. Eg. if learn that f_10 should be 0, then the whole thing dies, even though we're still making contributions albeit small.\n\nLearning that f_10 should be (near) 0 means that (nearly) erasing that cell is improving learning in some way.  For example, suppose you have a language model learning something about subject/predicate.  At the end of sentence marker (a period say), you'd want some forget-gates to go almost to zero, clearing cells that were tracking something about the current subject and current predicate.   For backprop, those cells don't care about future sentences, they only care about the current one so shutting off  gradient from the future allows them to learn faster by focusing on more narrow context.\n[Gers 1999](https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf) calls it \"releasing resources\".", "edited": 1492127544.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Even if we set a really large forget bias, as the training progresses it&amp;#39;ll correct this and make the forget gate at a timestep closer to what is optimal. Eg. if learn that f_10 should be 0, then the whole thing dies, even though we&amp;#39;re still making contributions albeit small.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Learning that f_10 should be (near) 0 means that (nearly) erasing that cell is improving learning in some way.  For example, suppose you have a language model learning something about subject/predicate.  At the end of sentence marker (a period say), you&amp;#39;d want some forget-gates to go almost to zero, clearing cells that were tracking something about the current subject and current predicate.   For backprop, those cells don&amp;#39;t care about future sentences, they only care about the current one so shutting off  gradient from the future allows them to learn faster by focusing on more narrow context.\n&lt;a href=\"https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf\"&gt;Gers 1999&lt;/a&gt; calls it &amp;quot;releasing resources&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8fqpz", "score_hidden": false, "stickied": false, "created": 1492156138.0, "created_utc": 1492127338.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": "", "user_reports": [], "id": "dg8x4ld", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg8x25g", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Right, but as training progresses we'll update the forget gate so that it's corrected, and then the gradients would vanish. I guess the time for that to happen is enough for us though\n\nJust curious, would you mind taking a look at my two threads here, about something similar. If you know the answer I'll give you gold :)\n\n* https://www.reddit.com/r/MachineLearning/comments/65005v/d_why_is_the_derivative_of_the_lstm_cell_state/\n* https://www.reddit.com/r/MachineLearning/comments/655tvn/d_how_common_is_truncated_cell_state_backprop/\n", "edited": 1492158456.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Right, but as training progresses we&amp;#39;ll update the forget gate so that it&amp;#39;s corrected, and then the gradients would vanish. I guess the time for that to happen is enough for us though&lt;/p&gt;\n\n&lt;p&gt;Just curious, would you mind taking a look at my two threads here, about something similar. If you know the answer I&amp;#39;ll give you gold :)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/MachineLearning/comments/65005v/d_why_is_the_derivative_of_the_lstm_cell_state/\"&gt;https://www.reddit.com/r/MachineLearning/comments/65005v/d_why_is_the_derivative_of_the_lstm_cell_state/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/MachineLearning/comments/655tvn/d_how_common_is_truncated_cell_state_backprop/\"&gt;https://www.reddit.com/r/MachineLearning/comments/655tvn/d_how_common_is_truncated_cell_state_backprop/&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8x4ld", "score_hidden": false, "stickied": false, "created": 1492186334.0, "created_utc": 1492157534.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8x25g", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "RaionTategami", "parent_id": "t3_655iih", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "This is why you should initialize the gate so that the forget gate are initially 0, I.e. remember everything.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is why you should initialize the gate so that the forget gate are initially 0, I.e. remember everything.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8x25g", "score_hidden": false, "stickied": false, "created": 1492186130.0, "created_utc": 1492157330.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655iih", "likes": null, "replies": "", "user_reports": [], "id": "dg7m7l9", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg7m0fy", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Right", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Right&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7m7l9", "score_hidden": false, "stickied": false, "created": 1492122522.0, "created_utc": 1492093722.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg7m0fy", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "jostmey", "parent_id": "t3_655iih", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I think you are right, the forget gate basically shuts down that unit and kills the gradient. Information can flow from other units through the input gate to reinitialize the unit, but the same problem starts to happen again with the input gates. Perhaps my intuition on this is wrong, and someone smarter than me can give a better explanation.\n\nI think almost every recurrent neural network architecture has an attention span beyond which it cannot learn.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think you are right, the forget gate basically shuts down that unit and kills the gradient. Information can flow from other units through the input gate to reinitialize the unit, but the same problem starts to happen again with the input gates. Perhaps my intuition on this is wrong, and someone smarter than me can give a better explanation.&lt;/p&gt;\n\n&lt;p&gt;I think almost every recurrent neural network architecture has an attention span beyond which it cannot learn.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7m0fy", "score_hidden": false, "stickied": false, "created": 1492122290.0, "created_utc": 1492093490.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]