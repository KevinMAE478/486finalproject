[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a large set of documents, usually 500-2,000 words each, and for several different labels, there are about 20-100 samples with those labels, and hundreds to millions more that should be labeled but do not have that label. Over time, I expect new labels to be created and new documents to be added to the data set, although relatively few old labels added to any documents. It is possible that some documents will have many labels or possibly no labels. It is also possible for a label to be a subclass of another label, though not manually indicated.&lt;/p&gt;\n\n&lt;p&gt;I have a keyword tagging system in place that utilizes TF-IDF metrics to extract ngrams from the documents, which is my current feature set from them.&lt;/p&gt;\n\n&lt;p&gt;My initial thought is to use Naive Bayes classifiers (one for each label) and perform ROC analysis to see how well it classifies the category. However, if, say, 5% of the data should actually be labeled with Label A, and the amount of data supplied with Label A amounts to 1% of the data, then I would expect an ideal false discovery rate of 80%. On top of that, since the prior distribution is unknown, dealing with this issue is even more difficult.&lt;/p&gt;\n\n&lt;p&gt;Another idea I had is to use the vectors of word/ngram counts and look at the distribution of the cosine similarities of the documents within the sample and use a simple one-sided significance test with p-values to determine if they are similar enough. This relies heavily on the idea that the ngram vectors for each label are dissimilar enough to consider this form of hypothesis testing. A big problem with this is that I am heavily relying on the assumption that similarity is a highly discriminative metric, and because labels can be extremely unbalanced (by up to 5 orders of magnitude), I may need to employ additional methods, such as limiting the number of labels (perhaps by only taking the top N labels by p-value).&lt;/p&gt;\n\n&lt;p&gt;The training portion of the algorithm does not need to be extremely fast, but the labeling portion should be very quick once the document is parsed/tokenized.&lt;/p&gt;\n\n&lt;p&gt;There is a greater penalty for mislabeling a document than leaving it unlabeled, but future, human-feedback data may be provided to indicate incorrect labels. I will not have access to anything of the sort in the meantime, though.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "I have a large set of documents, usually 500-2,000 words each, and for several different labels, there are about 20-100 samples with those labels, and hundreds to millions more that should be labeled but do not have that label. Over time, I expect new labels to be created and new documents to be added to the data set, although relatively few old labels added to any documents. It is possible that some documents will have many labels or possibly no labels. It is also possible for a label to be a subclass of another label, though not manually indicated.\n\nI have a keyword tagging system in place that utilizes TF-IDF metrics to extract ngrams from the documents, which is my current feature set from them.\n\nMy initial thought is to use Naive Bayes classifiers (one for each label) and perform ROC analysis to see how well it classifies the category. However, if, say, 5% of the data should actually be labeled with Label A, and the amount of data supplied with Label A amounts to 1% of the data, then I would expect an ideal false discovery rate of 80%. On top of that, since the prior distribution is unknown, dealing with this issue is even more difficult.\n\nAnother idea I had is to use the vectors of word/ngram counts and look at the distribution of the cosine similarities of the documents within the sample and use a simple one-sided significance test with p-values to determine if they are similar enough. This relies heavily on the idea that the ngram vectors for each label are dissimilar enough to consider this form of hypothesis testing. A big problem with this is that I am heavily relying on the assumption that similarity is a highly discriminative metric, and because labels can be extremely unbalanced (by up to 5 orders of magnitude), I may need to employ additional methods, such as limiting the number of labels (perhaps by only taking the top N labels by p-value).\n\nThe training portion of the algorithm does not need to be extremely fast, but the labeling portion should be very quick once the document is parsed/tokenized.\n\nThere is a greater penalty for mislabeling a document than leaving it unlabeled, but future, human-feedback data may be provided to indicate incorrect labels. I will not have access to anything of the sort in the meantime, though.", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "65e91x", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 2, "report_reasons": null, "author": "antirabbit", "saved": false, "mod_reports": [], "name": "t3_65e91x", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65e91x/d_how_can_i_perform_multilabel_classification_if/", "num_reports": null, "locked": false, "stickied": false, "created": 1492223066.0, "url": "https://www.reddit.com/r/MachineLearning/comments/65e91x/d_how_can_i_perform_multilabel_classification_if/", "author_flair_text": null, "quarantine": false, "title": "[D] How can I perform multi-label classification if many labels are missing?", "created_utc": 1492194266.0, "distinguished": null, "media": null, "upvote_ratio": 0.67, "num_comments": 7, "visited": false, "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65e91x", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65e91x", "likes": null, "replies": "", "user_reports": [], "id": "dgazdhw", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "antirabbit", "parent_id": "t1_dg9wstz", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "The clusters could help, but in the end, I need to be able to add new labels to the dataset at any time (usually, but not necessarily, by inserting new documents that are tagged with said label). And I need to be able to adapt to new topics actually appearing in the data set.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The clusters could help, but in the end, I need to be able to add new labels to the dataset at any time (usually, but not necessarily, by inserting new documents that are tagged with said label). And I need to be able to adapt to new topics actually appearing in the data set.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgazdhw", "score_hidden": false, "stickied": false, "created": 1492309736.0, "created_utc": 1492280936.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9wstz", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "afrogohan", "parent_id": "t3_65e91x", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I'm not familiar with your dataset, but can a clustering/topic modeling approach be applied. Take the output, the topics/clusters found and associate them with your labels. I guess, it would depend if your labels are easily mappable to the topics.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not familiar with your dataset, but can a clustering/topic modeling approach be applied. Take the output, the topics/clusters found and associate them with your labels. I guess, it would depend if your labels are easily mappable to the topics.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9wstz", "score_hidden": false, "stickied": false, "created": 1492240207.0, "created_utc": 1492211407.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65e91x", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65e91x", "likes": null, "replies": "", "user_reports": [], "id": "dgb40ub", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "antirabbit", "parent_id": "t1_dgazz4g", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I'll take a look at it. Right now I am trying to implement something relatively lightweight, and a complex neural network might be a bit much, but I guess I'll find it interesting either way. Thanks.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll take a look at it. Right now I am trying to implement something relatively lightweight, and a complex neural network might be a bit much, but I guess I&amp;#39;ll find it interesting either way. Thanks.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb40ub", "score_hidden": false, "stickied": false, "created": 1492316016.0, "created_utc": 1492287216.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgazz4g", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "domkkirke", "parent_id": "t3_65e91x", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Look at the Ladder networks!! ;)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Look at the Ladder networks!! ;)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgazz4g", "score_hidden": false, "stickied": false, "created": 1492310567.0, "created_utc": 1492281767.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65e91x", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65e91x", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65e91x", "likes": null, "replies": "", "user_reports": [], "id": "dgdtj2b", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Wocto", "parent_id": "t1_dgdtb2c", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "In a neural net approach, simply setting the weights that correspond to the missing values to 0", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In a neural net approach, simply setting the weights that correspond to the missing values to 0&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdtj2b", "score_hidden": false, "stickied": false, "created": 1492479708.0, "created_utc": 1492450908.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdtb2c", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "antirabbit", "parent_id": "t1_dg9ytau", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "What do you mean by weight mask?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What do you mean by weight mask?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdtb2c", "score_hidden": false, "stickied": false, "created": 1492479467.0, "created_utc": 1492450667.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9ytau", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Wocto", "parent_id": "t3_65e91x", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Use a loss function that ignores the missing values, e.g. with a weight mask. Then balance the classes somehow, by oversampling the low smaller class or something. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Use a loss function that ignores the missing values, e.g. with a weight mask. Then balance the classes somehow, by oversampling the low smaller class or something. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9ytau", "score_hidden": false, "stickied": false, "created": 1492242864.0, "created_utc": 1492214064.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]