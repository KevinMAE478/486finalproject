[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I keep seeing this online, on Quora and this subreddit but I don&amp;#39;t get it. Here&amp;#39;s some basic math to show otherwise:&lt;/p&gt;\n\n&lt;p&gt;We use the equation: c_t = f \u2299 c_t-1 + i \u2299 g&lt;/p&gt;\n\n&lt;p&gt;Now, we want to compute dc_t/dc_t-1. Apparently it&amp;#39;s equal to the forget gate.&lt;/p&gt;\n\n&lt;p&gt;First of all, how can this be so if we have the i and g gates, and i and g are both dependent on the previous hidden state, which is in turn dependent on the previous cell state? Chain rule would extend for longer and we&amp;#39;d have more derivative terms in there.&lt;/p&gt;\n\n&lt;p&gt;Now, even if we were to ignore i and g, and simplify some of our expressions for convenience sake, what about this:&lt;/p&gt;\n\n&lt;p&gt;d/dc (c \u2299 f) = d/dc(f)&lt;/p&gt;\n\n&lt;p&gt;since f is also dependent on the previous hidden state and thus previous cell state:&lt;/p&gt;\n\n&lt;p&gt;= d/dc(sigmoid(w * h))&lt;/p&gt;\n\n&lt;p&gt;= sigmoid\u2019(w * h) * w * d/dc(h)&lt;/p&gt;\n\n&lt;p&gt;= sigmoid\u2019(w * h) * w * d/dc(tanh(o * c))&lt;/p&gt;\n\n&lt;p&gt;= sigmoid\u2019(w * h) * w * o * tanh\u2019(o * c))&lt;/p&gt;\n\n&lt;p&gt;Not nearly &amp;quot;f&amp;quot;, which is sigmoid(w * h)&lt;/p&gt;\n\n&lt;p&gt;You can assume w denotes w_fh, and that there&amp;#39;s no input at this timestep nor bias, or something&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "I keep seeing this online, on Quora and this subreddit but I don't get it. Here's some basic math to show otherwise:\n\nWe use the equation: c\\_t = f \u2299 c\\_t-1 + i \u2299 g\n\nNow, we want to compute dc\\_t/dc\\_t-1. Apparently it's equal to the forget gate.\n\nFirst of all, how can this be so if we have the i and g gates, and i and g are both dependent on the previous hidden state, which is in turn dependent on the previous cell state? Chain rule would extend for longer and we'd have more derivative terms in there.\n\nNow, even if we were to ignore i and g, and simplify some of our expressions for convenience sake, what about this:\n\nd/dc (c \u2299 f) = d/dc(f)\n\nsince f is also dependent on the previous hidden state and thus previous cell state:\n\n= d/dc(sigmoid(w * h))\n\n= sigmoid\u2019(w * h) * w * d/dc(h)\n\n= sigmoid\u2019(w * h) * w * d/dc(tanh(o * c))\n\n= sigmoid\u2019(w * h) * w * o * tanh\u2019(o * c))\n\nNot nearly \"f\", which is sigmoid(w * h)\n\nYou can assume w denotes w_fh, and that there's no input at this timestep nor bias, or something", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "65005v", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 2, "report_reasons": null, "author": "sup6978", "saved": false, "mod_reports": [], "name": "t3_65005v", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": 1492023641.0, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65005v/d_why_is_the_derivative_of_the_lstm_cell_state/", "num_reports": null, "locked": false, "stickied": false, "created": 1492050453.0, "url": "https://www.reddit.com/r/MachineLearning/comments/65005v/d_why_is_the_derivative_of_the_lstm_cell_state/", "author_flair_text": null, "quarantine": false, "title": "[D] Why is the derivative of the LSTM cell state w.r.t. to the previous cell state equal to the forget gate?", "created_utc": 1492021653.0, "distinguished": null, "media": null, "upvote_ratio": 0.56, "num_comments": 12, "visited": false, "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": "", "user_reports": [], "id": "dg7fll1", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg7b8kn", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Not sure why you were downvoted", "edited": 1492157557.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not sure why you were downvoted&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7fll1", "score_hidden": false, "stickied": false, "created": 1492112685.0, "created_utc": 1492083885.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg7b8kn", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "fishiwhj", "parent_id": "t1_dg6vkx6", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "why downvote?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;why downvote?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7b8kn", "score_hidden": false, "stickied": false, "created": 1492100386.0, "created_utc": 1492071586.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6vkx6", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "fishiwhj", "parent_id": "t3_65005v", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "That's because the original LSTM use a trick called truncated backprop, in which the error signals can not be passed through the input/forget/output/ gate and the cell input. We have:\n\nc_t = i * s_t + f * c_t-1\n\nh_t = o * tanh(c_t)\n\ndc_t/dc_t-1 = \nf + \n\ndf/dc_t-1 * c_t-1 +\n\ndi/dc_t-1 * s_t +\n\ni * ds_t/dc_t-1\n\n=(truncated) f\n\nwhere df/dc_t-1, di/dc_t-1 and ds_t/dc_t-1 are truncated to 0.\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s because the original LSTM use a trick called truncated backprop, in which the error signals can not be passed through the input/forget/output/ gate and the cell input. We have:&lt;/p&gt;\n\n&lt;p&gt;c_t = i * s_t + f * c_t-1&lt;/p&gt;\n\n&lt;p&gt;h_t = o * tanh(c_t)&lt;/p&gt;\n\n&lt;p&gt;dc_t/dc_t-1 = \nf + &lt;/p&gt;\n\n&lt;p&gt;df/dc_t-1 * c_t-1 +&lt;/p&gt;\n\n&lt;p&gt;di/dc_t-1 * s_t +&lt;/p&gt;\n\n&lt;p&gt;i * ds_t/dc_t-1&lt;/p&gt;\n\n&lt;p&gt;=(truncated) f&lt;/p&gt;\n\n&lt;p&gt;where df/dc_t-1, di/dc_t-1 and ds_t/dc_t-1 are truncated to 0.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6vkx6", "score_hidden": false, "stickied": false, "created": 1492073758.0, "created_utc": 1492044958.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": "", "user_reports": [], "id": "dg6d8nb", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "owenwp", "parent_id": "t3_65005v", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Conceptually, any inputs that happened in the distant past affect both the current cell state and the previous cell state in the same way, so the current and previous would not have any difference between each other that varies by inputs from the distant past.  \n\nThis is also an essential property of a Markov chain, that the current state is completely described by the previous state and the state transition (in this case the state transition being the act of selectively forgetting).  Without this property, the problem would be computationally intractable, because the \"state\" would essentially be an unbounded stream of the entire input history.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Conceptually, any inputs that happened in the distant past affect both the current cell state and the previous cell state in the same way, so the current and previous would not have any difference between each other that varies by inputs from the distant past.  &lt;/p&gt;\n\n&lt;p&gt;This is also an essential property of a Markov chain, that the current state is completely described by the previous state and the state transition (in this case the state transition being the act of selectively forgetting).  Without this property, the problem would be computationally intractable, because the &amp;quot;state&amp;quot; would essentially be an unbounded stream of the entire input history.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6d8nb", "score_hidden": false, "stickied": false, "created": 1492052676.0, "created_utc": 1492023876.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": "", "user_reports": [], "id": "dg7gcqi", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg7g80d", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "TL;DR: I don't mean to say f is literally a constant, I mean to say when we differentiate w.r.t. c_t-1 we treat it as a constant, and not as a function of c_t-1, which is what it actually is.\n\nIf you don't treat them as constant while differentiating, don't we have to backprop through everything? Let's imagine everything is 1-D:\n\nc_t = f * c_t-1 + i * g\n\nSince f, i, and g are functions of h_t-1, and h_t-1 is a function of c_t-1, then f, i, and g are functions of c_t-1\n\nSo even if we were to get the partial derivative:\n\n\u2202c_t/\u2202c_t-1 = \u2202f/\u2202c_t-1 + \u2202/\u2202c_t-1 (i * g)\n\nIf we treated f, i, and g as constants, and don't backprop through them, then we do get f as the gradient here.\n\nI see re. the bias. That makes sense.", "edited": 1492096849.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: I don&amp;#39;t mean to say f is literally a constant, I mean to say when we differentiate w.r.t. c_t-1 we treat it as a constant, and not as a function of c_t-1, which is what it actually is.&lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t treat them as constant while differentiating, don&amp;#39;t we have to backprop through everything? Let&amp;#39;s imagine everything is 1-D:&lt;/p&gt;\n\n&lt;p&gt;c_t = f * c_t-1 + i * g&lt;/p&gt;\n\n&lt;p&gt;Since f, i, and g are functions of h_t-1, and h_t-1 is a function of c_t-1, then f, i, and g are functions of c_t-1&lt;/p&gt;\n\n&lt;p&gt;So even if we were to get the partial derivative:&lt;/p&gt;\n\n&lt;p&gt;\u2202c_t/\u2202c_t-1 = \u2202f/\u2202c_t-1 + \u2202/\u2202c_t-1 (i * g)&lt;/p&gt;\n\n&lt;p&gt;If we treated f, i, and g as constants, and don&amp;#39;t backprop through them, then we do get f as the gradient here.&lt;/p&gt;\n\n&lt;p&gt;I see re. the bias. That makes sense.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7gcqi", "score_hidden": false, "stickied": false, "created": 1492114196.0, "created_utc": 1492085396.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg7g80d", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "r_dipietro", "parent_id": "t1_dg7fofm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "There's no need to treat them as constant. I recommend going through an example where everything is 1-D. Yes gradient components from the past will die extra quickly if f is small. This is why people initialize the forget gate bias to be high at the start of training.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s no need to treat them as constant. I recommend going through an example where everything is 1-D. Yes gradient components from the past will die extra quickly if f is small. This is why people initialize the forget gate bias to be high at the start of training.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7g80d", "score_hidden": false, "stickied": false, "created": 1492113950.0, "created_utc": 1492085150.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg7fofm", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg6fqtb", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yep. Partial derivative is f. I was thinking -- since f, i, and g are functionally dependent on c_t-1 ultimately, we would need to backprop through these. But as another commenter pointed out, we treat f, i, and g as constants b/c otherwise our gradients would be massively long terms at large timesteps.\n\nWhat happens when components of f are close to zero? Don't these die off? How do we prevent it? Also, since f is continuous -- sigmoid --\n it could be any value between 0-1, and this could vanish? ie. if you're taking the derivative of c_t w.r.t. c_t-50 or something.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yep. Partial derivative is f. I was thinking -- since f, i, and g are functionally dependent on c_t-1 ultimately, we would need to backprop through these. But as another commenter pointed out, we treat f, i, and g as constants b/c otherwise our gradients would be massively long terms at large timesteps.&lt;/p&gt;\n\n&lt;p&gt;What happens when components of f are close to zero? Don&amp;#39;t these die off? How do we prevent it? Also, since f is continuous -- sigmoid --\n it could be any value between 0-1, and this could vanish? ie. if you&amp;#39;re taking the derivative of c_t w.r.t. c_t-50 or something.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7fofm", "score_hidden": false, "stickied": false, "created": 1492112852.0, "created_utc": 1492084052.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6fqtb", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "r_dipietro", "parent_id": "t3_65005v", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "The partial derivative of c_t as a function of f, c_t-1, i, and g with respect to c_t-1 is f, or more precisely the matrix of partials is diag(f). The full derivative is not f, as you're pointing out.\n\nI'm guessing you're looking for a justification of how LSTM partially (not entirely, as some incorrect blog posts claim) avoids the vanishing gradient problem. It's because the \"path\" involving that partial is often that of least resistance (when components of f are close to 1).\n\nSelf plug: if you want more rigor, section 3 of https://arxiv.org/abs/1702.07805 is very relevant.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The partial derivative of c_t as a function of f, c_t-1, i, and g with respect to c_t-1 is f, or more precisely the matrix of partials is diag(f). The full derivative is not f, as you&amp;#39;re pointing out.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m guessing you&amp;#39;re looking for a justification of how LSTM partially (not entirely, as some incorrect blog posts claim) avoids the vanishing gradient problem. It&amp;#39;s because the &amp;quot;path&amp;quot; involving that partial is often that of least resistance (when components of f are close to 1).&lt;/p&gt;\n\n&lt;p&gt;Self plug: if you want more rigor, section 3 of &lt;a href=\"https://arxiv.org/abs/1702.07805\"&gt;https://arxiv.org/abs/1702.07805&lt;/a&gt; is very relevant.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6fqtb", "score_hidden": false, "stickied": false, "created": 1492055253.0, "created_utc": 1492026453.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": "", "user_reports": [], "id": "dg6hb2s", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "r_dipietro", "parent_id": "t1_dg6fqk0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "What!? :). I think you're trying to stuff LSTM's h_t into the formula that's often stated for simple RNNs (originally from Bengio's slides I think). That derivation no longer holds with LSTM. There are multiple paths that connect the LSTM's c_t-1 to c_t, not just one path as in simple RNNs.\n\nEdit: P.S. I'm not the one who downvoted you :). I think this confusion is common because most explanations of LSTM are extremely handwavy.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What!? :). I think you&amp;#39;re trying to stuff LSTM&amp;#39;s h_t into the formula that&amp;#39;s often stated for simple RNNs (originally from Bengio&amp;#39;s slides I think). That derivation no longer holds with LSTM. There are multiple paths that connect the LSTM&amp;#39;s c_t-1 to c_t, not just one path as in simple RNNs.&lt;/p&gt;\n\n&lt;p&gt;Edit: P.S. I&amp;#39;m not the one who downvoted you :). I think this confusion is common because most explanations of LSTM are extremely handwavy.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6hb2s", "score_hidden": false, "stickied": false, "created": 1492056853.0, "created_utc": 1492028053.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": "", "user_reports": [], "id": "dg6vt0k", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg6fqk0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I never stated that c\\_t is a function of c\\_t. I say explicitly that it's a function of c\\_t-1. \n\n&gt; So d/dct (c{t+1})=ft is a function of c{t-1} but not c_t\n\nThis cannot be right. c{t+1} = f{t+1} * c{t}. f{t+1} is functionally dependent on h{t}, which is functionally dependent on c{t}. c\\_{t+1} doesn't depend on f\\_t, it depends on f\\_{t+1}", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I never stated that c_t is a function of c_t. I say explicitly that it&amp;#39;s a function of c_t-1. &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;So d/dct (c{t+1})=ft is a function of c{t-1} but not c_t&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This cannot be right. c{t+1} = f{t+1} * c{t}. f{t+1} is functionally dependent on h{t}, which is functionally dependent on c{t}. c_{t+1} doesn&amp;#39;t depend on f_t, it depends on f_{t+1}&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6vt0k", "score_hidden": false, "stickied": false, "created": 1492074033.0, "created_utc": 1492045233.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65005v", "likes": null, "replies": "", "user_reports": [], "id": "dg73ho0", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg6fqk0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "No worries! Thanks for contributing anyways", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No worries! Thanks for contributing anyways&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg73ho0", "score_hidden": false, "stickied": false, "created": 1492083982.0, "created_utc": 1492055182.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6fqk0", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Mandrathax", "parent_id": "t3_65005v", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "~~I think what's confusing you here is the time index.~~\n\n~~In BPTT over a sequence of length T, when applying the chain rule you consider the T variables h_1, h_2, ..., h_T.~~\n\n~~So c_t depends of f_t, which is a function of h_{t-1} (and therefore c_{t-1}) but not c_t.~~\n\n~~So d/dc_t (c_{t+1})=f_t is a function of c_{t-1} but not c_t~~\n\nEDIT : should read and think twice before answering, thanks for pointing out that this is incorrect", "edited": 1492050541.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;del&gt;I think what&amp;#39;s confusing you here is the time index.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;In BPTT over a sequence of length T, when applying the chain rule you consider the T variables h_1, h_2, ..., h_T.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;So c&lt;em&gt;t depends of f_t, which is a function of h&lt;/em&gt;{t-1} (and therefore c_{t-1}) but not c_t.&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;So d/dc&lt;em&gt;t (c&lt;/em&gt;{t+1})=f&lt;em&gt;t is a function of c&lt;/em&gt;{t-1} but not c_t&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;EDIT : should read and think twice before answering, thanks for pointing out that this is incorrect&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6fqk0", "score_hidden": false, "stickied": false, "created": 1492055245.0, "created_utc": 1492026445.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]