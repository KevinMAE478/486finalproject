[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Project", "id": "656pvf", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 220, "report_reasons": null, "author": "sup6978", "saved": false, "mod_reports": [], "name": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "ayearofai.com", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/ETPijBlqs9M0ABnUgMvQB62MU2fGstckKuHBkKjaoyI.jpg?s=f42b2a15dffa71719ff7262f3f0fc06b", "width": 1200, "height": 544}, "resolutions": [{"url": "https://i.redditmedia.com/ETPijBlqs9M0ABnUgMvQB62MU2fGstckKuHBkKjaoyI.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=e2e1ec8a7ca771453ffa021c3dc29131", "width": 108, "height": 48}, {"url": "https://i.redditmedia.com/ETPijBlqs9M0ABnUgMvQB62MU2fGstckKuHBkKjaoyI.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=08754faab1f5f02cd35abf47012a57f4", "width": 216, "height": 97}, {"url": "https://i.redditmedia.com/ETPijBlqs9M0ABnUgMvQB62MU2fGstckKuHBkKjaoyI.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=68ad8a509d29e6cb5f9ed10b7b5977eb", "width": 320, "height": 145}, {"url": "https://i.redditmedia.com/ETPijBlqs9M0ABnUgMvQB62MU2fGstckKuHBkKjaoyI.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=1fc90e101bdbd56edbdc01d73d2e99a1", "width": 640, "height": 290}, {"url": "https://i.redditmedia.com/ETPijBlqs9M0ABnUgMvQB62MU2fGstckKuHBkKjaoyI.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=960&amp;s=af3fab94d35f96a6c9c79a6b4615d314", "width": 960, "height": 435}, {"url": "https://i.redditmedia.com/ETPijBlqs9M0ABnUgMvQB62MU2fGstckKuHBkKjaoyI.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=1080&amp;s=9caf86d95ebda9e15d2535c5ce610019", "width": 1080, "height": 489}], "variants": {}, "id": "9u72PiN5UC7d6qacXW-D1uPEiuahTsHu0mJZUxCE18Y"}], "enabled": false}, "thumbnail": "https://b.thumbs.redditmedia.com/IcbW64fIc8zG7_aQ4ZokgZPdoWoOJb4lj10aGzQNlTg.jpg", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "four", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "link", "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/656pvf/p_the_most_comprehensive_yet_simple_and_fun/", "num_reports": null, "locked": false, "stickied": false, "created": 1492131302.0, "url": "https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b", "author_flair_text": null, "quarantine": false, "title": "[P] The most comprehensive yet simple and fun RNN/LSTM tutorial on the Internet.", "created_utc": 1492102502.0, "distinguished": null, "media": null, "upvote_ratio": 0.89, "num_comments": 56, "visited": false, "subreddit_type": "public", "ups": 220}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg8pdvo", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg8pcn5", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; The one advice I gave regarding this article is you should have made this article as a 3 or 4 parts. It would have made this much more readable and referenceable.\n\nDefinitely will do this next time, esp. when we move off Medium.\n\nThank you!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The one advice I gave regarding this article is you should have made this article as a 3 or 4 parts. It would have made this much more readable and referenceable.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Definitely will do this next time, esp. when we move off Medium.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8pdvo", "score_hidden": false, "stickied": false, "created": 1492169787.0, "created_utc": 1492140987.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8pcn5", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "commafighter", "parent_id": "t1_dg88u44", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Your article is way better than any other article available in the internet for a novice. your article really is very comprehensive and your explanations are also very good. I always had this confusion regrading the *time* component  in RNN. I thought they literally add time to the RNN algorithm, now only I understood its the sequence of execution of code.\n\nThe one advice I can give regarding this article is you should have made this article as a 3 or 4 parts. It would have made this much more readable and referenceable.\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your article is way better than any other article available in the internet for a novice. your article really is very comprehensive and your explanations are also very good. I always had this confusion regrading the &lt;em&gt;time&lt;/em&gt; component  in RNN. I thought they literally add time to the RNN algorithm, now only I understood its the sequence of execution of code.&lt;/p&gt;\n\n&lt;p&gt;The one advice I can give regarding this article is you should have made this article as a 3 or 4 parts. It would have made this much more readable and referenceable.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8pcn5", "score_hidden": false, "stickied": false, "created": 1492169733.0, "created_utc": 1492140933.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg8udai", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "FermiAnyon", "parent_id": "t1_dg8a7yi", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Excellent!  I won't get put down here after a few months.  It probably has to be in something at least computer related... SHIT!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Excellent!  I won&amp;#39;t get put down here after a few months.  It probably has to be in something at least computer related... SHIT!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8udai", "score_hidden": false, "stickied": false, "created": 1492178963.0, "created_utc": 1492150163.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg8uj8d", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg8thmr", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You contradict yourself -- the negative comments were not/are not constructive at all. And after actually asking what I could improve on, not one negative commenter gave any feedback. Two of the negative comments were made by a troll account (go through their history). One of the users made a snarky/useless remark, got downvotes, and deleted it.\n\nSo I would say it goes both ways.\n\nWould like to hear your reply to this.\n", "edited": 1492187838.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You contradict yourself -- the negative comments were not/are not constructive at all. And after actually asking what I could improve on, not one negative commenter gave any feedback. Two of the negative comments were made by a troll account (go through their history). One of the users made a snarky/useless remark, got downvotes, and deleted it.&lt;/p&gt;\n\n&lt;p&gt;So I would say it goes both ways.&lt;/p&gt;\n\n&lt;p&gt;Would like to hear your reply to this.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8uj8d", "score_hidden": false, "stickied": false, "created": 1492179347.0, "created_utc": 1492150547.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg9dj5k", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dg8thmr", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "[deleted]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9dj5k", "score_hidden": false, "stickied": false, "created": 1492216643.0, "created_utc": 1492187843.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8thmr", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "GoSergio", "parent_id": "t1_dg8a7yi", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 1, "body": "The negative comments are there for a reason. Ok, they were negative so they got downvoted to death, and the most upvoted comment now is just some guy saying \"cool!\". Sorry, but this sub is really going to shit...", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The negative comments are there for a reason. Ok, they were negative so they got downvoted to death, and the most upvoted comment now is just some guy saying &amp;quot;cool!&amp;quot;. Sorry, but this sub is really going to shit...&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8thmr", "score_hidden": false, "stickied": false, "created": 1492177041.0, "created_utc": 1492148241.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 0}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg8mmnf", "gilded": 0, "archived": false, "score": 28, "report_reasons": null, "author": "veggiedefender", "parent_id": "t1_dg8lmyi", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "This is a sub for machine learning, actually. If you want intellectuals, browse /r/iamverysmart", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a sub for machine learning, actually. If you want intellectuals, browse &lt;a href=\"/r/iamverysmart\"&gt;/r/iamverysmart&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8mmnf", "score_hidden": false, "stickied": false, "created": 1492165654.0, "created_utc": 1492136854.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 28}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg9ex8l", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "Eternahl", "parent_id": "t1_dg8lmyi", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You can be intellectual, mathematically inclined or whatever and don't have a PhD. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can be intellectual, mathematically inclined or whatever and don&amp;#39;t have a PhD. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9ex8l", "score_hidden": false, "stickied": false, "created": 1492218207.0, "created_utc": 1492189407.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8lmyi", "gilded": 0, "archived": false, "score": -34, "report_reasons": null, "author": "AshkenaziJew", "parent_id": "t1_dg8a7yi", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "And rightfully so. This is a sub for intellectuals, not brainlets.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And rightfully so. This is a sub for intellectuals, not brainlets.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8lmyi", "score_hidden": false, "stickied": false, "created": 1492164260.0, "created_utc": 1492135460.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": -34}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8a7yi", "gilded": 0, "archived": false, "score": 25, "report_reasons": null, "author": "deepNeural", "parent_id": "t1_dg88u44", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Great job, don't let all those other peeps get to you. Pretty much anyone without a phd next to his name gets put down here.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Great job, don&amp;#39;t let all those other peeps get to you. Pretty much anyone without a phd next to his name gets put down here.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8a7yi", "score_hidden": false, "stickied": false, "created": 1492148878.0, "created_utc": 1492120078.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 25}}], "after": null, "before": null}}, "user_reports": [], "id": "dg88u44", "gilded": 0, "archived": false, "score": 21, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg8044h", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I only ended up using info from 1 thread. (only about 2 sentences, out of an 84 min read). And no I didn't verbatim copy + paste anything.\n\nSo I've added 1 Reddit reference to the article, and cut down those explanations / the \"in my own words\" aspect so the reader will go directly to the Reddit comment to read instead.", "edited": 1492236001.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I only ended up using info from 1 thread. (only about 2 sentences, out of an 84 min read). And no I didn&amp;#39;t verbatim copy + paste anything.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve added 1 Reddit reference to the article, and cut down those explanations / the &amp;quot;in my own words&amp;quot; aspect so the reader will go directly to the Reddit comment to read instead.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg88u44", "score_hidden": false, "stickied": false, "created": 1492147170.0, "created_utc": 1492118370.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 21}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8044h", "gilded": 0, "archived": false, "score": 34, "report_reasons": null, "author": "metacurse", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Did you just copy and paste from many reddit threads that you started without attribution? \n\nThe most important learning exercise to go through when learning about LSTMs is about how to cite people properly!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Did you just copy and paste from many reddit threads that you started without attribution? &lt;/p&gt;\n\n&lt;p&gt;The most important learning exercise to go through when learning about LSTMs is about how to cite people properly!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8044h", "score_hidden": false, "stickied": false, "created": 1492137492.0, "created_utc": 1492108692.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 34}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg96oy8", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg96kob", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Will let you know when I publish changes.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Will let you know when I publish changes.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg96oy8", "score_hidden": false, "stickied": false, "created": 1492208492.0, "created_utc": 1492179692.0, "depth": 7, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg96kob", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "InProx_Ichlife", "parent_id": "t1_dg949a5", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Cool, I will.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Cool, I will.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg96kob", "score_hidden": false, "stickied": false, "created": 1492208333.0, "created_utc": 1492179533.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg949a5", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg911z4", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Hey. I've figured it out. I was wrong. I'm going to update the article now -- check back in an hour or so. Re-read Part II of LSTMs then.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey. I&amp;#39;ve figured it out. I was wrong. I&amp;#39;m going to update the article now -- check back in an hour or so. Re-read Part II of LSTMs then.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg949a5", "score_hidden": false, "stickied": false, "created": 1492204953.0, "created_utc": 1492176153.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg911z4", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "InProx_Ichlife", "parent_id": "t1_dg90e07", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Sorry, I've recently started studying RNNs so I don't have any extra insight. Does the original LSTM paper not address this issue?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry, I&amp;#39;ve recently started studying RNNs so I don&amp;#39;t have any extra insight. Does the original LSTM paper not address this issue?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg911z4", "score_hidden": false, "stickied": false, "created": 1492198569.0, "created_utc": 1492169769.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg90e07", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg9033f", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You're absolutely right. I should've waited for the second thread to get answers, and followed up on it with different people I know, before posting.\n\nDo you have any insight regarding this? I'm trying to get an answer ASAP so I know if that part of the article is misleading anyone.\n\nThanks a lot!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;re absolutely right. I should&amp;#39;ve waited for the second thread to get answers, and followed up on it with different people I know, before posting.&lt;/p&gt;\n\n&lt;p&gt;Do you have any insight regarding this? I&amp;#39;m trying to get an answer ASAP so I know if that part of the article is misleading anyone.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg90e07", "score_hidden": false, "stickied": false, "created": 1492196786.0, "created_utc": 1492167986.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg9cvis", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg9033f", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Hey, if you're interested you can read the updated LSTM Part II section.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey, if you&amp;#39;re interested you can read the updated LSTM Part II section.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9cvis", "score_hidden": false, "stickied": false, "created": 1492215901.0, "created_utc": 1492187101.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9033f", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "InProx_Ichlife", "parent_id": "t1_dg8pj6l", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Sorry but perhaps you shouldn't include parts you are not really sure or don't have a deep understanding in a guide. It might mislead people if it's not accurate.  \n  \nI liked the post a lot though (especially the LSTM cell illustrations); this is just a constructive critisim!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry but perhaps you shouldn&amp;#39;t include parts you are not really sure or don&amp;#39;t have a deep understanding in a guide. It might mislead people if it&amp;#39;s not accurate.  &lt;/p&gt;\n\n&lt;p&gt;I liked the post a lot though (especially the LSTM cell illustrations); this is just a constructive critisim!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9033f", "score_hidden": false, "stickied": false, "created": 1492195885.0, "created_utc": 1492167085.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg94c9l", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg91y4e", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Hey. I figured it all out. A brain fart led to this. I saw in the original/first LSTM paper they do this (that is, cutting off gradient signals at gates, which gave it 100% credibility to me, since thats the most legit source), but then I accidentally applied the calculus incorrectly -- should've used the product rule where I used the chain rule. Coincidentally, what I concluded frmo this brain fart calculus led to an issue that the paper offerred a solution for -- but in a diff context. then I looked back at it and I realized that I messed up.\n\nI'm updating it all now. I figured it out. Will let you know when I update it.\n\nThank you so much and I'm sorry for this screw up!", "edited": 1492179729.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey. I figured it all out. A brain fart led to this. I saw in the original/first LSTM paper they do this (that is, cutting off gradient signals at gates, which gave it 100% credibility to me, since thats the most legit source), but then I accidentally applied the calculus incorrectly -- should&amp;#39;ve used the product rule where I used the chain rule. Coincidentally, what I concluded frmo this brain fart calculus led to an issue that the paper offerred a solution for -- but in a diff context. then I looked back at it and I realized that I messed up.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m updating it all now. I figured it out. Will let you know when I update it.&lt;/p&gt;\n\n&lt;p&gt;Thank you so much and I&amp;#39;m sorry for this screw up!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg94c9l", "score_hidden": false, "stickied": false, "created": 1492205084.0, "created_utc": 1492176284.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg91y4e", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "sorrge", "parent_id": "t1_dg8pj6l", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Ok, I have read the new text and I *think* I see the point you are trying to make, although your math notation is lacking (the prime notation is not used for partial derivatives). Maybe it makes sense to do it like you say. However, could you point to a source more reliable than reddit that introduces this concept and demonstrates that it works (ideally, demonstrates that it is superior to the normal gradient)? For instance, I briefly checked Sutskever's PhD thesis that is titled \"TRAINING RECURRENT NEURAL NETWORKS\", and didn't find any mention of this technique. I have looked at the LSTM implementation in Tensorflow, class BasicLSTMCell in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py - and it seems to be a straightforward implementation of the LSTM formulas, without any adjustments of the gradient, which is automatically derived and thus includes all backpropagations. Furthermore that implementation cites the paper https://arxiv.org/abs/1409.2329 which doesn't mention adjusted gradients for the gates, and the implementation of the paper https://github.com/wojzaremba/lstm/blob/master/main.lua also simply calls Torch function backward() for the model to do the backpropagation, meaning that the differentiation is performed automatically without any special treatment of the gates.\n\nIn short, you need to find good sources for all your material. The best would be for you to implement all things you mention and show how one method is better than the other. If what you have described is indeed a helpful trick which is not currently published properly, you are even in a good position to write a paper about it.", "edited": 1492175888.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ok, I have read the new text and I &lt;em&gt;think&lt;/em&gt; I see the point you are trying to make, although your math notation is lacking (the prime notation is not used for partial derivatives). Maybe it makes sense to do it like you say. However, could you point to a source more reliable than reddit that introduces this concept and demonstrates that it works (ideally, demonstrates that it is superior to the normal gradient)? For instance, I briefly checked Sutskever&amp;#39;s PhD thesis that is titled &amp;quot;TRAINING RECURRENT NEURAL NETWORKS&amp;quot;, and didn&amp;#39;t find any mention of this technique. I have looked at the LSTM implementation in Tensorflow, class BasicLSTMCell in &lt;a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\"&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py&lt;/a&gt; - and it seems to be a straightforward implementation of the LSTM formulas, without any adjustments of the gradient, which is automatically derived and thus includes all backpropagations. Furthermore that implementation cites the paper &lt;a href=\"https://arxiv.org/abs/1409.2329\"&gt;https://arxiv.org/abs/1409.2329&lt;/a&gt; which doesn&amp;#39;t mention adjusted gradients for the gates, and the implementation of the paper &lt;a href=\"https://github.com/wojzaremba/lstm/blob/master/main.lua\"&gt;https://github.com/wojzaremba/lstm/blob/master/main.lua&lt;/a&gt; also simply calls Torch function backward() for the model to do the backpropagation, meaning that the differentiation is performed automatically without any special treatment of the gates.&lt;/p&gt;\n\n&lt;p&gt;In short, you need to find good sources for all your material. The best would be for you to implement all things you mention and show how one method is better than the other. If what you have described is indeed a helpful trick which is not currently published properly, you are even in a good position to write a paper about it.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg91y4e", "score_hidden": false, "stickied": false, "created": 1492200656.0, "created_utc": 1492171856.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 4}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8pj6l", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "[deleted]", "parent_id": "t1_dg8ezql", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "[deleted]", "edited": 1492158388.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8pj6l", "score_hidden": false, "stickied": false, "created": 1492170018.0, "created_utc": 1492141218.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg9cri4", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg8ezql", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Hey. Have updated the article. If you're interested, you can re-read LSTMs Part II. For your first concern, I did proper research into truncated backprop and used this source: http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf. For point #2, I talked to an ML researcher and he told me why it leads to that derivative (or, rather, that \"effective\" derivative -- you'll see what I mean once you read). I expressed it in math\n\nThanks for the help!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey. Have updated the article. If you&amp;#39;re interested, you can re-read LSTMs Part II. For your first concern, I did proper research into truncated backprop and used this source: &lt;a href=\"http://www.cs.utoronto.ca/%7Eilya/pubs/ilya_sutskever_phd_thesis.pdf\"&gt;http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf&lt;/a&gt;. For point #2, I talked to an ML researcher and he told me why it leads to that derivative (or, rather, that &amp;quot;effective&amp;quot; derivative -- you&amp;#39;ll see what I mean once you read). I expressed it in math&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9cri4", "score_hidden": false, "stickied": false, "created": 1492215780.0, "created_utc": 1492186980.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8ezql", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "sorrge", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Some points you may want to re-check or find a good source:\n\n- Definition of truncated BPTT: I'm not sure if your substitution formula is correct. Isn't it usually assumed that the internal state at time t-T is constant?\n\n- You say that we \"don't backprop\" through f, i and g gates. This is the first time I hear about this, surely this is not a standard technique. How do you imagine their respective matrices (W_xf, W_hf etc.) are trained then? Did you confuse it with \"peephole connections\" technique somehow?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Some points you may want to re-check or find a good source:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Definition of truncated BPTT: I&amp;#39;m not sure if your substitution formula is correct. Isn&amp;#39;t it usually assumed that the internal state at time t-T is constant?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;You say that we &amp;quot;don&amp;#39;t backprop&amp;quot; through f, i and g gates. This is the first time I hear about this, surely this is not a standard technique. How do you imagine their respective matrices (W_xf, W_hf etc.) are trained then? Did you confuse it with &amp;quot;peephole connections&amp;quot; technique somehow?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8ezql", "score_hidden": false, "stickied": false, "created": 1492155140.0, "created_utc": 1492126340.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg9cwh9", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg8hs9a", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "No worries. If you're interested you can read the updated LSTM Part II section. I had to make a fair bit of updates because I got somethign wrong there!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No worries. If you&amp;#39;re interested you can read the updated LSTM Part II section. I had to make a fair bit of updates because I got somethign wrong there!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9cwh9", "score_hidden": false, "stickied": false, "created": 1492215930.0, "created_utc": 1492187130.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8hs9a", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "jazzkingrt", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thank you it was helpful.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you it was helpful.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8hs9a", "score_hidden": false, "stickied": false, "created": 1492158960.0, "created_utc": 1492130160.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dgbcjs1", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dgb5kv7", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thank You! I will join as a freshman in September (finishing up high school). Co-author is also a high school student and will join either Yale or UPenn as a freshman.", "edited": 1492323527.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank You! I will join as a freshman in September (finishing up high school). Co-author is also a high school student and will join either Yale or UPenn as a freshman.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbcjs1", "score_hidden": false, "stickied": false, "created": 1492328018.0, "created_utc": 1492299218.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb5kv7", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "PURELY_TO_VOTE", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Sorry but is the author of this a Stanford freshman? \n\nI am impressed! And vaguely frightened.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry but is the author of this a Stanford freshman? &lt;/p&gt;\n\n&lt;p&gt;I am impressed! And vaguely frightened.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb5kv7", "score_hidden": false, "stickied": false, "created": 1492318126.0, "created_utc": 1492289326.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg9dhh0", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg9ddq1", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I mean to say it doesn't change my perception of / belief in what I'm doing.", "edited": 1492188528.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I mean to say it doesn&amp;#39;t change my perception of / belief in what I&amp;#39;m doing.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9dhh0", "score_hidden": false, "stickied": false, "created": 1492216591.0, "created_utc": 1492187791.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg9ddq1", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "GoSergio", "parent_id": "t1_dg8v4uv", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "So you \"don't give a fuck\", but you wrote a wall of text to reply to my one-word comment? Really?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So you &amp;quot;don&amp;#39;t give a fuck&amp;quot;, but you wrote a wall of text to reply to my one-word comment? Really?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9ddq1", "score_hidden": false, "stickied": false, "created": 1492216470.0, "created_utc": 1492187670.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8v4uv", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg8ti86", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Past articles where I have used a similar writing style, people have commented saying it was the only article in which they could understand fully or the best article they found to follow.\n\nThere's an added benefit in conveying complex topics simply and even in a fun, lighthearted manner, to make things accessible for people on a wider scope. In fact, the motivation behind this blog was so I could teach my iOS developer friends, since I came originally from an iOS dev background (when I was in middle/early high school). Lowering barriers to entry is not something that should be looked down upon; it's my personal vision for this blog, which is one of my side projects.\n\nIf I don't deliver on this goal, and there's concrete reasons for it, then I'm happy to hear criticisms. But I frankly don't give a fuck if you sarcastically make fun  of the writing style.", "edited": 1492152420.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Past articles where I have used a similar writing style, people have commented saying it was the only article in which they could understand fully or the best article they found to follow.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s an added benefit in conveying complex topics simply and even in a fun, lighthearted manner, to make things accessible for people on a wider scope. In fact, the motivation behind this blog was so I could teach my iOS developer friends, since I came originally from an iOS dev background (when I was in middle/early high school). Lowering barriers to entry is not something that should be looked down upon; it&amp;#39;s my personal vision for this blog, which is one of my side projects.&lt;/p&gt;\n\n&lt;p&gt;If I don&amp;#39;t deliver on this goal, and there&amp;#39;s concrete reasons for it, then I&amp;#39;m happy to hear criticisms. But I frankly don&amp;#39;t give a fuck if you sarcastically make fun  of the writing style.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8v4uv", "score_hidden": false, "stickied": false, "created": 1492180791.0, "created_utc": 1492151991.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8ti86", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "GoSergio", "parent_id": "t1_dg865ki", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "So cute &lt;3&lt;3", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So cute &amp;lt;3&amp;lt;3&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8ti86", "score_hidden": false, "stickied": false, "created": 1492177074.0, "created_utc": 1492148274.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dg865ki", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "VieTaTo_GUFARE", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I love the Pro-tips! So cute!!\n(I'm not sarcastic...)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I love the Pro-tips! So cute!!\n(I&amp;#39;m not sarcastic...)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg865ki", "score_hidden": false, "stickied": false, "created": 1492144067.0, "created_utc": 1492115267.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg7zpef", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "finallyifoundvalidUN", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Cool!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Cool!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7zpef", "score_hidden": false, "stickied": false, "created": 1492137048.0, "created_utc": 1492108248.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg94djm", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg930jz", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Hey. I made a big mistake with the article, that stemmed from a huge brain fart I never looked back to check on. With LSTMs Part II, forget everything you read. I'm updating it now, will let you know when I publish changes. Thank you!", "edited": 1492179717.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey. I made a big mistake with the article, that stemmed from a huge brain fart I never looked back to check on. With LSTMs Part II, forget everything you read. I&amp;#39;m updating it now, will let you know when I publish changes. Thank you!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg94djm", "score_hidden": false, "stickied": false, "created": 1492205143.0, "created_utc": 1492176343.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg9cwtb", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg930jz", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Hey, if you're interested you can read the updated LSTM Part II section.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey, if you&amp;#39;re interested you can read the updated LSTM Part II section.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9cwtb", "score_hidden": false, "stickied": false, "created": 1492215941.0, "created_utc": 1492187141.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg930jz", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "mind_juice", "parent_id": "t1_dg92hy7", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Can you look at [this](http://imgur.com/a/xE58D) and tell if I understand back-propogation through time correctly?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can you look at &lt;a href=\"http://imgur.com/a/xE58D\"&gt;this&lt;/a&gt; and tell if I understand back-propogation through time correctly?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg930jz", "score_hidden": false, "stickied": false, "created": 1492202790.0, "created_utc": 1492173990.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg92hy7", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg92b0i", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "If you have long gradient terms, you probably have the vanishing gradient problem, unless you can guarantee each of those terms are around 1 each. If they're not, it'll explode or vanish.\n\nFor example, let's say the gradient term = k_1 * k_2 * k_3 * ... * k_100. 100 terms in this product. That's long.\n\nIf each of these terms is, let's say, around 0.5, then you have 0.5^100 = some absurdly low number. If you have each term be, anything like around, 1.5, then you have 1.5^100 which is some absurdly high number.\n\nBy introducing tanh/sigmoid derivatives as products in these, this is a major issue, because the max output of these derivatives is like 0.25 if I remember correctly. So you could be having 0.25^100 in that expression.", "edited": 1492188591.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you have long gradient terms, you probably have the vanishing gradient problem, unless you can guarantee each of those terms are around 1 each. If they&amp;#39;re not, it&amp;#39;ll explode or vanish.&lt;/p&gt;\n\n&lt;p&gt;For example, let&amp;#39;s say the gradient term = k_1 * k_2 * k_3 * ... * k_100. 100 terms in this product. That&amp;#39;s long.&lt;/p&gt;\n\n&lt;p&gt;If each of these terms is, let&amp;#39;s say, around 0.5, then you have 0.5&lt;sup&gt;100&lt;/sup&gt; = some absurdly low number. If you have each term be, anything like around, 1.5, then you have 1.5&lt;sup&gt;100&lt;/sup&gt; which is some absurdly high number.&lt;/p&gt;\n\n&lt;p&gt;By introducing tanh/sigmoid derivatives as products in these, this is a major issue, because the max output of these derivatives is like 0.25 if I remember correctly. So you could be having 0.25&lt;sup&gt;100&lt;/sup&gt; in that expression.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg92hy7", "score_hidden": false, "stickied": false, "created": 1492201803.0, "created_utc": 1492173003.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg92b0i", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "mind_juice", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt;The length of the gradient terms would keep growing and growing, and we\u2019d be simultaneously introducing non-linearities like tanh and sigmoid and their derivatives into the gradient terms, along with inputs and weights. Over time this will either explode or vanish\u200a\u2014\u200ait would be \u201ccomputationally intractable\u201d.\n\nI can't understand this. Doesn't chain rule take care of this long series of gradient terms? What is exploding/vanishing: the length of gradient terms or the gradients?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The length of the gradient terms would keep growing and growing, and we\u2019d be simultaneously introducing non-linearities like tanh and sigmoid and their derivatives into the gradient terms, along with inputs and weights. Over time this will either explode or vanish\u200a\u2014\u200ait would be \u201ccomputationally intractable\u201d.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I can&amp;#39;t understand this. Doesn&amp;#39;t chain rule take care of this long series of gradient terms? What is exploding/vanishing: the length of gradient terms or the gradients?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg92b0i", "score_hidden": false, "stickied": false, "created": 1492201413.0, "created_utc": 1492172613.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dgct700", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dgcokgq", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; I'm a bit confused about your LSTM Part II section. In the GIF with the gradients over time, we see that the values for the LSTM get smaller and smaller and vanish. Is this because eventually, the forget gates will all activate and thus, the gradients get smaller and smaller? And in any case where the forget gate becomes 0, we cannot recover that gradient and hence, we still have the vanishing gradient problem?\n\nYes, but in a sense it's not necessarily a bad thing -- if that cell state has been forgotten at some point in time, then we don't want gradients after the cell state to flow back to it (obviously, because it's not making any contribution). After 128 timesteps, it's unlikely that we're going to be remembering information from the first few timesteps.  \n\nAnd the thing is that this GIF depends on the application at hand. I'm not sure what it is, but what we forget/how much depends on that as well.\n\n&gt; Also, the reason it is slower than the RNN is because we are learning when to forget past information right?\n\nSlower to vanish? Really it's because the RNN gradients have a bunch of tanh/sigmoid function derivatives and weightsin them. If you know about tanh or sigmoid then you know that their derivative has a maximum value of 0.25, so when these and the weights chain up they begin to vanish. Like 0.25^100 = an absurdly small number, if you were backpropping through 100 timesteps.\n\nIt's slower to vanish b/c of the LSTM architecture in general. If you didn't have the forget gate at all I'd think no vanishing happens at all. If you add the forget gate, for good reason, there is some vanishing going on but b/c of the nature of the memory cell it's not a bad thing -- it's the *point* of the forget gate.\n\nThe issue comes that we want to make sure that the forget gates are not initialized to 0 in the start of training, so we won't completely block learning.\n\nI'm going to update to make it more clear.", "edited": 1492407424.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I&amp;#39;m a bit confused about your LSTM Part II section. In the GIF with the gradients over time, we see that the values for the LSTM get smaller and smaller and vanish. Is this because eventually, the forget gates will all activate and thus, the gradients get smaller and smaller? And in any case where the forget gate becomes 0, we cannot recover that gradient and hence, we still have the vanishing gradient problem?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Yes, but in a sense it&amp;#39;s not necessarily a bad thing -- if that cell state has been forgotten at some point in time, then we don&amp;#39;t want gradients after the cell state to flow back to it (obviously, because it&amp;#39;s not making any contribution). After 128 timesteps, it&amp;#39;s unlikely that we&amp;#39;re going to be remembering information from the first few timesteps.  &lt;/p&gt;\n\n&lt;p&gt;And the thing is that this GIF depends on the application at hand. I&amp;#39;m not sure what it is, but what we forget/how much depends on that as well.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Also, the reason it is slower than the RNN is because we are learning when to forget past information right?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Slower to vanish? Really it&amp;#39;s because the RNN gradients have a bunch of tanh/sigmoid function derivatives and weightsin them. If you know about tanh or sigmoid then you know that their derivative has a maximum value of 0.25, so when these and the weights chain up they begin to vanish. Like 0.25&lt;sup&gt;100&lt;/sup&gt; = an absurdly small number, if you were backpropping through 100 timesteps.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s slower to vanish b/c of the LSTM architecture in general. If you didn&amp;#39;t have the forget gate at all I&amp;#39;d think no vanishing happens at all. If you add the forget gate, for good reason, there is some vanishing going on but b/c of the nature of the memory cell it&amp;#39;s not a bad thing -- it&amp;#39;s the &lt;em&gt;point&lt;/em&gt; of the forget gate.&lt;/p&gt;\n\n&lt;p&gt;The issue comes that we want to make sure that the forget gates are not initialized to 0 in the start of training, so we won&amp;#39;t completely block learning.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to update to make it more clear.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgct700", "score_hidden": false, "stickied": false, "created": 1492416969.0, "created_utc": 1492388169.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgcokgq", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "justiliang", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thanks for the insightful article on RNN/LSTM, it was a great read! I have a couple questions regarding LSTMs.\n\nI'm a bit confused about your LSTM Part II section. In the GIF with the gradients over time, we see that the values for the LSTM get smaller and smaller and vanish. Is this because eventually, the forget gates will all activate and thus, the gradients get smaller and smaller? And in any case where the forget gate becomes 0, we cannot recover that gradient and hence, we still have the vanishing gradient problem?\n\nAlso, the reason it is slower than the RNN is because we are learning when to forget past information right?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the insightful article on RNN/LSTM, it was a great read! I have a couple questions regarding LSTMs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a bit confused about your LSTM Part II section. In the GIF with the gradients over time, we see that the values for the LSTM get smaller and smaller and vanish. Is this because eventually, the forget gates will all activate and thus, the gradients get smaller and smaller? And in any case where the forget gate becomes 0, we cannot recover that gradient and hence, we still have the vanishing gradient problem?&lt;/p&gt;\n\n&lt;p&gt;Also, the reason it is slower than the RNN is because we are learning when to forget past information right?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgcokgq", "score_hidden": false, "stickied": false, "created": 1492410628.0, "created_utc": 1492381828.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg9epjd", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "radarthreat", "parent_id": "t1_dg8lxo0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Reddit does have a 'Save' function, you know.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Reddit does have a &amp;#39;Save&amp;#39; function, you know.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9epjd", "score_hidden": false, "stickied": false, "created": 1492217965.0, "created_utc": 1492189165.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 4}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8lxo0", "gilded": 0, "archived": false, "score": -3, "report_reasons": null, "author": "slim-jong-un", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 1, "body": "Please ignore me - I'm only posting this because I want to come back and read this at a more convenient time. Thanks got posting this!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Please ignore me - I&amp;#39;m only posting this because I want to come back and read this at a more convenient time. Thanks got posting this!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8lxo0", "score_hidden": false, "stickied": false, "created": 1492164690.0, "created_utc": 1492135890.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": -3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg89tv8", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg89nqn", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yea, I'm real -- for all of it. What's concretely wrong with it so I can update? It doesn't read academically cause I came from the iOS dev community and wrote a lot of this so my iOS dev friends will be interested and understand. This is just our writing style on the blog, and for the other articles it was super well received, so I'm just continuing with the pattern. I didn't aim to sacrifice content; I tried to make it as comprehensive as possible, cause personally when I was learning there were a lot of different resources I had to read through to get a full understanding. I also had a fair number of unanswered questions eg. popular lectures like CS231n left out talking about the impact of forget gates on vanishing gradients, and when I did the math it seemed to me like it was a problem.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yea, I&amp;#39;m real -- for all of it. What&amp;#39;s concretely wrong with it so I can update? It doesn&amp;#39;t read academically cause I came from the iOS dev community and wrote a lot of this so my iOS dev friends will be interested and understand. This is just our writing style on the blog, and for the other articles it was super well received, so I&amp;#39;m just continuing with the pattern. I didn&amp;#39;t aim to sacrifice content; I tried to make it as comprehensive as possible, cause personally when I was learning there were a lot of different resources I had to read through to get a full understanding. I also had a fair number of unanswered questions eg. popular lectures like CS231n left out talking about the impact of forget gates on vanishing gradients, and when I did the math it seemed to me like it was a problem.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg89tv8", "score_hidden": false, "stickied": false, "created": 1492148387.0, "created_utc": 1492119587.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 9}}], "after": null, "before": null}}, "user_reports": [], "id": "dg89nqn", "gilded": 0, "archived": false, "score": -21, "report_reasons": null, "author": "INTP008", "parent_id": "t1_dg88zgz", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Like, all of it? Are you even for real?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Like, all of it? Are you even for real?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg89nqn", "score_hidden": false, "stickied": false, "created": 1492148172.0, "created_utc": 1492119372.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": -21}}], "after": null, "before": null}}, "user_reports": [], "id": "dg88zgz", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg86hyc", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "The whole article? Which parts are pretentious and which parts are shitty, so I can edit", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The whole article? Which parts are pretentious and which parts are shitty, so I can edit&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg88zgz", "score_hidden": false, "stickied": false, "created": 1492147350.0, "created_utc": 1492118550.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg86hyc", "gilded": 0, "archived": false, "score": -15, "report_reasons": null, "author": "smartsometimes", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Maybe a \"whoosh\" moment, but this isn't meant to be serious, is it? Because... it's pretentious and shitty...", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe a &amp;quot;whoosh&amp;quot; moment, but this isn&amp;#39;t meant to be serious, is it? Because... it&amp;#39;s pretentious and shitty...&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg86hyc", "score_hidden": false, "stickied": false, "created": 1492144457.0, "created_utc": 1492115657.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": -15}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg7xmjp", "gilded": 0, "archived": false, "score": 27, "report_reasons": null, "author": "precise_taciturn", "parent_id": "t1_dg7vkwx", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You tried", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You tried&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7xmjp", "score_hidden": false, "stickied": false, "created": 1492134847.0, "created_utc": 1492106047.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 27}}], "after": null, "before": null}}, "user_reports": [], "id": "dg7vkwx", "gilded": 0, "archived": false, "score": -8, "report_reasons": null, "author": "autotldr", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "This is the best tl;dr I could make, [original](https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b) reduced by 98%. (I'm a bot)\n*****\n&gt; The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state&amp;sup1; of the recurrent neural network to be one where the most likely candidate word is &amp;quot;Two&amp;quot;.\n\n&gt; Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps t and fixed layers \u2113, just make sure you understand that this is not always the case.\n\n&gt; I&amp;#039;m going to adapt a super dumbed down one from Andrej Karpathy&amp;#039;s Stanford CS231n RNN lecture, where a one to many &amp;quot;Character level language model&amp;quot; single layer recurrent neural network needs to output &amp;quot;Hello&amp;quot;.\n\n\n*****\n[**Extended Summary**](http://np.reddit.com/r/autotldr/comments/656v5h/p_the_most_comprehensive_yet_simple_and_fun/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ \"Version 1.65, ~101474 tl;drs so far.\") | [Theory](http://np.reddit.com/r/autotldr/comments/31bfht/theory_autotldr_concept/) | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr \"PM's and comments are monitored, constructive feedback is welcome.\") | *Top* *keywords*: **RNN**^#1 **output**^#2 **hidden**^#3 **input**^#4 **timestep**^#5", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is the best tl;dr I could make, &lt;a href=\"https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b\"&gt;original&lt;/a&gt; reduced by 98%. (I&amp;#39;m a bot)&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state&amp;sup1; of the recurrent neural network to be one where the most likely candidate word is &amp;quot;Two&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps t and fixed layers \u2113, just make sure you understand that this is not always the case.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#039;m going to adapt a super dumbed down one from Andrej Karpathy&amp;#039;s Stanford CS231n RNN lecture, where a one to many &amp;quot;Character level language model&amp;quot; single layer recurrent neural network needs to output &amp;quot;Hello&amp;quot;.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;a href=\"http://np.reddit.com/r/autotldr/comments/656v5h/p_the_most_comprehensive_yet_simple_and_fun/\"&gt;&lt;strong&gt;Extended Summary&lt;/strong&gt;&lt;/a&gt; | &lt;a href=\"http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/\" title=\"Version 1.65, ~101474 tl;drs so far.\"&gt;FAQ&lt;/a&gt; | &lt;a href=\"http://np.reddit.com/r/autotldr/comments/31bfht/theory_autotldr_concept/\"&gt;Theory&lt;/a&gt; | &lt;a href=\"http://np.reddit.com/message/compose?to=%23autotldr\" title=\"PM&amp;#39;s and comments are monitored, constructive feedback is welcome.\"&gt;Feedback&lt;/a&gt; | &lt;em&gt;Top&lt;/em&gt; &lt;em&gt;keywords&lt;/em&gt;: &lt;strong&gt;RNN&lt;/strong&gt;&lt;sup&gt;#1&lt;/sup&gt; &lt;strong&gt;output&lt;/strong&gt;&lt;sup&gt;#2&lt;/sup&gt; &lt;strong&gt;hidden&lt;/strong&gt;&lt;sup&gt;#3&lt;/sup&gt; &lt;strong&gt;input&lt;/strong&gt;&lt;sup&gt;#4&lt;/sup&gt; &lt;strong&gt;timestep&lt;/strong&gt;&lt;sup&gt;#5&lt;/sup&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7vkwx", "score_hidden": false, "stickied": false, "created": 1492132672.0, "created_utc": 1492103872.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": -8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg8jxj3", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "ATownStomp", "parent_id": "t1_dg8a2ai", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You're still in high school? \n\nUgh. I've gotta pick up the pace.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;re still in high school? &lt;/p&gt;\n\n&lt;p&gt;Ugh. I&amp;#39;ve gotta pick up the pace.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8jxj3", "score_hidden": false, "stickied": false, "created": 1492161898.0, "created_utc": 1492133098.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 8}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8a2ai", "gilded": 0, "archived": false, "score": 7, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg88fgq", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Hey, what in particular makes you say that? I'm committed to making sure it's something beginners can and should read. \n\nThe Stanford remark is funny. To be fair, AI is also not my biggest expertise -- I'm still learning about the things I write about, and we're still in high school! I'll be learning more _at_ Stanford.", "edited": 1492120587.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey, what in particular makes you say that? I&amp;#39;m committed to making sure it&amp;#39;s something beginners can and should read. &lt;/p&gt;\n\n&lt;p&gt;The Stanford remark is funny. To be fair, AI is also not my biggest expertise -- I&amp;#39;m still learning about the things I write about, and we&amp;#39;re still in high school! I&amp;#39;ll be learning more &lt;em&gt;at&lt;/em&gt; Stanford.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8a2ai", "score_hidden": false, "stickied": false, "created": 1492148683.0, "created_utc": 1492119883.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 7}}], "after": null, "before": null}}, "user_reports": [], "id": "dg88fgq", "gilded": 0, "archived": false, "score": -7, "report_reasons": null, "author": "[deleted]", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "[deleted]", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg88fgq", "score_hidden": false, "stickied": false, "created": 1492146680.0, "created_utc": 1492117880.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": -7}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg9pc8p", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sbt_", "parent_id": "t1_dg8mqj9", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I am not a big fan of Medium, but I think you could still break it up nicely there. E.g.,\n\n\n- Rohan &amp; Lenny #3: An Introduction to Recurrent Neural Networks Part I\n    - Intro: In this blog post, we are going to introduce x\n    - Outro: In part II (insert link to part II here), we will learn how to do y with RNNs...\n\n- Rohan &amp; Lenny #3: An Introduction to Recurrent Neural Networks Part II\n    - Intro: In part I (insert link to part I here), we learned about x. We learned that bla bla. In this part, we will ...\n    - ...\n...\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am not a big fan of Medium, but I think you could still break it up nicely there. E.g.,&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Rohan &amp;amp; Lenny #3: An Introduction to Recurrent Neural Networks Part I&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Intro: In this blog post, we are going to introduce x&lt;/li&gt;\n&lt;li&gt;Outro: In part II (insert link to part II here), we will learn how to do y with RNNs...&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Rohan &amp;amp; Lenny #3: An Introduction to Recurrent Neural Networks Part II&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Intro: In part I (insert link to part I here), we learned about x. We learned that bla bla. In this part, we will ...&lt;/li&gt;\n&lt;li&gt;...\n...&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9pc8p", "score_hidden": false, "stickied": false, "created": 1492230585.0, "created_utc": 1492201785.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8mqj9", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg8edib", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yea, but we want to move off Medium next time and then, if we have a megapost, do that. Currently it would look sort of strange on our site to have 3-4 separate articles, just given how Medium is designed/works.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yea, but we want to move off Medium next time and then, if we have a megapost, do that. Currently it would look sort of strange on our site to have 3-4 separate articles, just given how Medium is designed/works.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8mqj9", "score_hidden": false, "stickied": false, "created": 1492165810.0, "created_utc": 1492137010.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8edib", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "thecity2", "parent_id": "t1_dg8a1hd", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It's not too late. You could break this up into 3 or 4 parts easily.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not too late. You could break this up into 3 or 4 parts easily.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8edib", "score_hidden": false, "stickied": false, "created": 1492154326.0, "created_utc": 1492125526.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg8a1hd", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg7zsfs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Sorry for the length. We want to break it up into multiple \"parts\" next time we do a post this large.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry for the length. We want to break it up into multiple &amp;quot;parts&amp;quot; next time we do a post this large.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8a1hd", "score_hidden": false, "stickied": false, "created": 1492148655.0, "created_utc": 1492119855.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_656pvf", "likes": null, "replies": "", "user_reports": [], "id": "dg8a0py", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "sup6978", "parent_id": "t1_dg819ch", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It definitely is written for noobs! That's sort of the point. It's also sort of how we (writers of this blog) learn more about things we are interested in. What parts in particular are inaccurate? It doesn't help to claim the whole thing is inaccurate, because I don't believe that to be true (unless I can be proved wrong).", "edited": 1492120169.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It definitely is written for noobs! That&amp;#39;s sort of the point. It&amp;#39;s also sort of how we (writers of this blog) learn more about things we are interested in. What parts in particular are inaccurate? It doesn&amp;#39;t help to claim the whole thing is inaccurate, because I don&amp;#39;t believe that to be true (unless I can be proved wrong).&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8a0py", "score_hidden": false, "stickied": false, "created": 1492148626.0, "created_utc": 1492119826.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 9}}], "after": null, "before": null}}, "user_reports": [], "id": "dg819ch", "gilded": 0, "archived": false, "score": -21, "report_reasons": null, "author": "INTP008", "parent_id": "t1_dg7zsfs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Lmao. It's written by noobs, for noobs. There's nothing to comprehend.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Lmao. It&amp;#39;s written by noobs, for noobs. There&amp;#39;s nothing to comprehend.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg819ch", "score_hidden": false, "stickied": false, "created": 1492138719.0, "created_utc": 1492109919.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": -21}}], "after": null, "before": null}}, "user_reports": [], "id": "dg7zsfs", "gilded": 0, "archived": false, "score": -7, "report_reasons": null, "author": "Artgor", "parent_id": "t3_656pvf", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I'm really impressed with this wall of text...\n\nI'll be sure to read and comprehend it. Or at least try.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m really impressed with this wall of text...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be sure to read and comprehend it. Or at least try.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7zsfs", "score_hidden": false, "stickied": false, "created": 1492137140.0, "created_utc": 1492108340.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": -7}}], "after": null, "before": null}}]