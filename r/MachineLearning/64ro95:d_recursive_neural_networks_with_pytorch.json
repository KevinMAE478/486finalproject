[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "64ro95", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 87, "report_reasons": null, "author": "evc123", "saved": false, "mod_reports": [], "name": "t3_64ro95", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "devblogs.nvidia.com", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/rysMEmnb_pEc9OiBvdolK4d0Y0krd-yV8zRhtNbRWfE.jpg?s=02e8049304df9a8bdd25360daea3513a", "width": 716, "height": 460}, "resolutions": [{"url": "https://i.redditmedia.com/rysMEmnb_pEc9OiBvdolK4d0Y0krd-yV8zRhtNbRWfE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=24a81b6dc055698ed04cc0af61b0b42f", "width": 108, "height": 69}, {"url": "https://i.redditmedia.com/rysMEmnb_pEc9OiBvdolK4d0Y0krd-yV8zRhtNbRWfE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=6b6e41ffc564f56af235b8327786118d", "width": 216, "height": 138}, {"url": "https://i.redditmedia.com/rysMEmnb_pEc9OiBvdolK4d0Y0krd-yV8zRhtNbRWfE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=2145290937c035161de24594655d473b", "width": 320, "height": 205}, {"url": "https://i.redditmedia.com/rysMEmnb_pEc9OiBvdolK4d0Y0krd-yV8zRhtNbRWfE.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=612d29f11ef72eea5c021b7cc105b5fd", "width": 640, "height": 411}], "variants": {}, "id": "tVxdHC2ibiiTVGbaSoIx3ArWx8uciTTtb7NY5CWqtns"}], "enabled": false}, "thumbnail": "https://a.thumbs.redditmedia.com/9epyPVq0h3o3tDJ8ixukZRIfgc83ufdCkf0iKia5go4.jpg", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "link", "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/64ro95/d_recursive_neural_networks_with_pytorch/", "num_reports": null, "locked": false, "stickied": false, "created": 1491954769.0, "url": "https://devblogs.nvidia.com/parallelforall/recursive-neural-networks-pytorch/", "author_flair_text": null, "quarantine": false, "title": "[D] Recursive Neural Networks with PyTorch", "created_utc": 1491925969.0, "distinguished": null, "media": null, "upvote_ratio": 0.86, "num_comments": 31, "visited": false, "subreddit_type": "public", "ups": 87}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg6306j", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sbt_", "parent_id": "t1_dg607uo", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "That's interesting, thanks for sharing that info! But still, that fact doesn't make PyTorch any better or worse than chainer :)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s interesting, thanks for sharing that info! But still, that fact doesn&amp;#39;t make PyTorch any better or worse than chainer :)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6306j", "score_hidden": false, "stickied": false, "created": 1492042174.0, "created_utc": 1492013374.0, "depth": 6, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg607uo", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "r-sync", "parent_id": "t1_dg5zjsz", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "here's the actual history of it's design:\n\n&gt; The design was initially seeded from three libraries: torch-autograd, Chainer, LuaTorch-nn. \nThen we iterated over it for over a month between Sam Gross, Adam Paszke, me, Adam Lerer, Zeming Lin with occasional input from pretty much everyone. We initially didn't have a functional interface at all (F.relu() for example), and Sergey Zagoruyko had pestered us to death until we saw value in it, and hurriedly wrote it / committed it in the last minute.\n\n\nReference:\nhttps://discuss.pytorch.org/t/pytorch-tutorial-for-deep-learning-researchers/1001/3?u=smth", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;here&amp;#39;s the actual history of it&amp;#39;s design:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The design was initially seeded from three libraries: torch-autograd, Chainer, LuaTorch-nn. \nThen we iterated over it for over a month between Sam Gross, Adam Paszke, me, Adam Lerer, Zeming Lin with occasional input from pretty much everyone. We initially didn&amp;#39;t have a functional interface at all (F.relu() for example), and Sergey Zagoruyko had pestered us to death until we saw value in it, and hurriedly wrote it / committed it in the last minute.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Reference:\n&lt;a href=\"https://discuss.pytorch.org/t/pytorch-tutorial-for-deep-learning-researchers/1001/3?u=smth\"&gt;https://discuss.pytorch.org/t/pytorch-tutorial-for-deep-learning-researchers/1001/3?u=smth&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg607uo", "score_hidden": false, "stickied": false, "created": 1492039133.0, "created_utc": 1492010333.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5zjsz", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "duschendestroyer", "parent_id": "t1_dg5xyuw", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "PyTorch is basically a copy of chainer with a different backend. It actually started out as a chainer fork.\n\nReference: https://twitter.com/jekbradbury/status/821786330459836416", "edited": 1492013387.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;PyTorch is basically a copy of chainer with a different backend. It actually started out as a chainer fork.&lt;/p&gt;\n\n&lt;p&gt;Reference: &lt;a href=\"https://twitter.com/jekbradbury/status/821786330459836416\"&gt;https://twitter.com/jekbradbury/status/821786330459836416&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5zjsz", "score_hidden": false, "stickied": false, "created": 1492038391.0, "created_utc": 1492009591.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5xyuw", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "sbt_", "parent_id": "t1_dg5nmsj", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Agree, chainer is a nice library with a very elegant API. However, if PyTorch works fine for someone, I don't see a killer feature there that would justify the switch.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Agree, chainer is a nice library with a very elegant API. However, if PyTorch works fine for someone, I don&amp;#39;t see a killer feature there that would justify the switch.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5xyuw", "score_hidden": false, "stickied": false, "created": 1492036590.0, "created_utc": 1492007790.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5nmsj", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "yanad", "parent_id": "t1_dg53epk", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Did you try chainer ?\n\nIt is not a famous one but it is actually quite good ! You can either use pre-made links or build your own in a rather easy way. Besides, the framework implements a trainer to automatically train your network (by specifying some hyperparameters of course). ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Did you try chainer ?&lt;/p&gt;\n\n&lt;p&gt;It is not a famous one but it is actually quite good ! You can either use pre-made links or build your own in a rather easy way. Besides, the framework implements a trainer to automatically train your network (by specifying some hyperparameters of course). &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5nmsj", "score_hidden": false, "stickied": false, "created": 1492016410.0, "created_utc": 1491987610.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg53epk", "gilded": 0, "archived": false, "score": 13, "report_reasons": null, "author": "digitalOctopus", "parent_id": "t1_dg52ho3", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "While it undoubtedly comes down to needs and tastes, I agree with you. I've tried and tried to get the hang of TensorFlow, and I see what people like about it, but pyTorch to me is so much easier to read and understand. Plus you get the benefit of being able to build dynamic graphs.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;While it undoubtedly comes down to needs and tastes, I agree with you. I&amp;#39;ve tried and tried to get the hang of TensorFlow, and I see what people like about it, but pyTorch to me is so much easier to read and understand. Plus you get the benefit of being able to build dynamic graphs.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg53epk", "score_hidden": false, "stickied": false, "created": 1491981261.0, "created_utc": 1491952461.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 13}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg9g0gy", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "iamspro", "parent_id": "t1_dg58zcn", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I can't speak for machine performance, but in terms of programmer performance it's gotta be 10x", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I can&amp;#39;t speak for machine performance, but in terms of programmer performance it&amp;#39;s gotta be 10x&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg9g0gy", "score_hidden": false, "stickied": false, "created": 1492219460.0, "created_utc": 1492190660.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg58zcn", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "SuperFX", "parent_id": "t1_dg52ho3", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "How's its performance for RNNs vs. TF?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How&amp;#39;s its performance for RNNs vs. TF?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg58zcn", "score_hidden": false, "stickied": false, "created": 1491988667.0, "created_utc": 1491959867.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 6}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg5npab", "gilded": 0, "archived": false, "score": 18, "report_reasons": null, "author": "inkognit", "parent_id": "t1_dg5c17e", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "1. No need for placeholders, everything is a tensor.\n2. You can debug it with a regular python debugger.\n3. You can go almost as high level as keras and as low level as pure Theano / Tensorflow without breaking a sweat.\n4. (bonus) Forget about scans and tf.while_loop's -&gt; you can use python for's and control flow operations because the graph is built dynamically ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;No need for placeholders, everything is a tensor.&lt;/li&gt;\n&lt;li&gt;You can debug it with a regular python debugger.&lt;/li&gt;\n&lt;li&gt;You can go almost as high level as keras and as low level as pure Theano / Tensorflow without breaking a sweat.&lt;/li&gt;\n&lt;li&gt;(bonus) Forget about scans and tf.while_loop&amp;#39;s -&amp;gt; you can use python for&amp;#39;s and control flow operations because the graph is built dynamically &lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5npab", "score_hidden": false, "stickied": false, "created": 1492016631.0, "created_utc": 1491987831.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 18}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg6byq8", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "johnQuincyLadams", "parent_id": "t1_dg5c17e", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "\n1. Class-based containers (nn.Module, nn.Sequential) provide an obvious, reusable and reproducible way to structure and train entire models - models are easily shared and integrated into larger codebases. No need to pick between the 30 different ways to structure TF models you see out there. This also makes reading others' code so much easier because you know exactly where to look instead of bouncing around everywhere!\n2. Data loading/feeding (in parallel) is intuitive, easy to use, and is much more amenable to uncommon/custom preprocessing/transforms. No need for queues, etc.\n3. Shared memory w/ numpy that make conversion back and forth essentially free. This is a good crutch for beginners/numpy users or to quickly integrate numpy into a pipeline then move to pure pytorch later.\n4. (bonus - opinion) SO much easier to read and navigate the source code!!!!\n\nWhen the pytorch tensor code becomes more mature (still missing some key functions), I definitely see it as a complete alternative to numpy with gpu/autograd support.", "edited": 1492037903.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;Class-based containers (nn.Module, nn.Sequential) provide an obvious, reusable and reproducible way to structure and train entire models - models are easily shared and integrated into larger codebases. No need to pick between the 30 different ways to structure TF models you see out there. This also makes reading others&amp;#39; code so much easier because you know exactly where to look instead of bouncing around everywhere!&lt;/li&gt;\n&lt;li&gt;Data loading/feeding (in parallel) is intuitive, easy to use, and is much more amenable to uncommon/custom preprocessing/transforms. No need for queues, etc.&lt;/li&gt;\n&lt;li&gt;Shared memory w/ numpy that make conversion back and forth essentially free. This is a good crutch for beginners/numpy users or to quickly integrate numpy into a pipeline then move to pure pytorch later.&lt;/li&gt;\n&lt;li&gt;(bonus - opinion) SO much easier to read and navigate the source code!!!!&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;When the pytorch tensor code becomes more mature (still missing some key functions), I definitely see it as a complete alternative to numpy with gpu/autograd support.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6byq8", "score_hidden": false, "stickied": false, "created": 1492051350.0, "created_utc": 1492022550.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 4}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5c17e", "gilded": 0, "archived": false, "score": 8, "report_reasons": null, "author": "mimighost", "parent_id": "t1_dg52ho3", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Mind using 3 sentences to convert a TF clan member? :)\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Mind using 3 sentences to convert a TF clan member? :)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5c17e", "score_hidden": false, "stickied": false, "created": 1491992704.0, "created_utc": 1491963904.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 8}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg60kvg", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "jrao1", "parent_id": "t1_dg5qb4o", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; Whereas in Keras the loss function is specified inside a string\n\nHuh? This is supposed to be a convenience, you are free to use the actual functions inside keras.objectives\n\n&gt; And since you implement your own training loop you can for example write code to display the loss for epoch in a live-updating graph, and you can see how the loss is changing over epochs during training, rather than the silly loss per epoch being printed on each line as in Keras.\n\nI'm not sure what this means, you are looking for a way to access the loss per epoch and use it in your own graph or something? Seems to me this can be done easily with a callback.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Whereas in Keras the loss function is specified inside a string&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Huh? This is supposed to be a convenience, you are free to use the actual functions inside keras.objectives&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;And since you implement your own training loop you can for example write code to display the loss for epoch in a live-updating graph, and you can see how the loss is changing over epochs during training, rather than the silly loss per epoch being printed on each line as in Keras.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I&amp;#39;m not sure what this means, you are looking for a way to access the loss per epoch and use it in your own graph or something? Seems to me this can be done easily with a callback.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg60kvg", "score_hidden": false, "stickied": false, "created": 1492039528.0, "created_utc": 1492010728.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg5y8lk", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "tryndisskilled", "parent_id": "t1_dg5wpyn", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Oh, yeah it won't autocomplete it indeed. Well it's more of an inconvenience than anything else to be honest.\n\nBut then again, all these frameworks are free to use (I mean come on, it's actually awesome given the effort some communities go through to build them, and what we can achieve with one), so there will always be little differences like this until we find a common framework (will we ever?).", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh, yeah it won&amp;#39;t autocomplete it indeed. Well it&amp;#39;s more of an inconvenience than anything else to be honest.&lt;/p&gt;\n\n&lt;p&gt;But then again, all these frameworks are free to use (I mean come on, it&amp;#39;s actually awesome given the effort some communities go through to build them, and what we can achieve with one), so there will always be little differences like this until we find a common framework (will we ever?).&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5y8lk", "score_hidden": false, "stickied": false, "created": 1492036904.0, "created_utc": 1492008104.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5wpyn", "gilded": 0, "archived": false, "score": 0, "report_reasons": null, "author": "ajibjanvar", "parent_id": "t1_dg5tiqp", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I meant when you do something like this:\n```\nlm.compile(optimizer=RMSprop(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n```\nthe loss is specified as a string, and the IDE would not provide the legit values of the string.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I meant when you do something like this:\n&lt;code&gt;\nlm.compile(optimizer=RMSprop(lr=0.1), loss=&amp;#39;categorical_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;])\n&lt;/code&gt;\nthe loss is specified as a string, and the IDE would not provide the legit values of the string.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5wpyn", "score_hidden": false, "stickied": false, "created": 1492035100.0, "created_utc": 1492006300.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 0}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5tiqp", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "tryndisskilled", "parent_id": "t1_dg5qb4o", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "As a Keras user (which is also my first experience with a ML framework), I agree with you about the lack of flexibility of Keras. Although it is possible to write your own callbacks (to display the loss in another way for example), it would indeed be much easier to do it directly in your very own fitting loop.\n\nBut I didn't really get it when you say that loss functions in Keras are \"specified inside a string\"? You can very well define your custom loss function and apply it when you use the\n\n    model.compile(loss, optimizer) \nmethod.", "edited": 1492008165.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As a Keras user (which is also my first experience with a ML framework), I agree with you about the lack of flexibility of Keras. Although it is possible to write your own callbacks (to display the loss in another way for example), it would indeed be much easier to do it directly in your very own fitting loop.&lt;/p&gt;\n\n&lt;p&gt;But I didn&amp;#39;t really get it when you say that loss functions in Keras are &amp;quot;specified inside a string&amp;quot;? You can very well define your custom loss function and apply it when you use the&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;model.compile(loss, optimizer) \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;method.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5tiqp", "score_hidden": false, "stickied": false, "created": 1492030645.0, "created_utc": 1492001845.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5qb4o", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "ajibjanvar", "parent_id": "t1_dg52ho3", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I really like PyTorch too! I've been wanting to get deep into one of the ML frameworks to build RNNs and after considering TF and Keras, I found PyTorch to be the most intuitive and easy to get into. Besides the advantages others mention below, I like the format of their (very active) discussion forum, which is based on the Discourse framework, not the awful Google groups format that TF uses. \n\nI also like that, unlike say Keras, all of the various options are actual methods, and not  string values. E.g to specify a specific loss function in PyTorch you invoke the corresponding method, and hour IDE (PyCharm or Jupyter in my case) will show you all possible completions. Whereas in Keras the loss function is specified inside a string and your IDE will not help you. \n\nBesides, it's trivial to code up a class implementing your desired NN configuration. And since you implement your own training loop you can for example write code to display the loss for epoch in a live-updating graph, and you can see how the loss is changing over epochs during training, rather than the silly loss per epoch being printed on each line as in Keras. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I really like PyTorch too! I&amp;#39;ve been wanting to get deep into one of the ML frameworks to build RNNs and after considering TF and Keras, I found PyTorch to be the most intuitive and easy to get into. Besides the advantages others mention below, I like the format of their (very active) discussion forum, which is based on the Discourse framework, not the awful Google groups format that TF uses. &lt;/p&gt;\n\n&lt;p&gt;I also like that, unlike say Keras, all of the various options are actual methods, and not  string values. E.g to specify a specific loss function in PyTorch you invoke the corresponding method, and hour IDE (PyCharm or Jupyter in my case) will show you all possible completions. Whereas in Keras the loss function is specified inside a string and your IDE will not help you. &lt;/p&gt;\n\n&lt;p&gt;Besides, it&amp;#39;s trivial to code up a class implementing your desired NN configuration. And since you implement your own training loop you can for example write code to display the loss for epoch in a live-updating graph, and you can see how the loss is changing over epochs during training, rather than the silly loss per epoch being printed on each line as in Keras. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5qb4o", "score_hidden": false, "stickied": false, "created": 1492024386.0, "created_utc": 1491995586.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg6e70b", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "johnQuincyLadams", "parent_id": "t1_dg6cb19", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "m8 I think the openness of the DL community has spoiled us. The lack of openness and reproducibility w.r.t. experiments, data, and code in psychology, neuroscience, biology etc might give you an aneurism.", "edited": 1492025057.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;m8 I think the openness of the DL community has spoiled us. The lack of openness and reproducibility w.r.t. experiments, data, and code in psychology, neuroscience, biology etc might give you an aneurism.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6e70b", "score_hidden": false, "stickied": false, "created": 1492053665.0, "created_utc": 1492024865.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg6da6i", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "sbt_", "parent_id": "t1_dg6cb19", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "&gt; And DeepMind's research is not reproducible\n\nYou mean because of missing information (i.e., research articles)? I think they publish a decent chunk (with a few months or years delay) which makes sense in the context of doing research for a company. \n\nI don't disagree with the initial comment, but I think that's more like a \"framework for researchers *in academia*,\" since Google (and other companies) also have implementations/applications in mind, or what people often call \"in/for production\"\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;And DeepMind&amp;#39;s research is not reproducible&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You mean because of missing information (i.e., research articles)? I think they publish a decent chunk (with a few months or years delay) which makes sense in the context of doing research for a company. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t disagree with the initial comment, but I think that&amp;#39;s more like a &amp;quot;framework for researchers &lt;em&gt;in academia&lt;/em&gt;,&amp;quot; since Google (and other companies) also have implementations/applications in mind, or what people often call &amp;quot;in/for production&amp;quot;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6da6i", "score_hidden": false, "stickied": false, "created": 1492052722.0, "created_utc": 1492023922.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg6cb19", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "INTP008", "parent_id": "t1_dg634q4", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "More PR people than researchers. They're all about optimizing for media coverage. Research is totally secondary to that.\n\nAnd DeepMind's research is not reproducible, so it barely qualifies as research.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;More PR people than researchers. They&amp;#39;re all about optimizing for media coverage. Research is totally secondary to that.&lt;/p&gt;\n\n&lt;p&gt;And DeepMind&amp;#39;s research is not reproducible, so it barely qualifies as research.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6cb19", "score_hidden": false, "stickied": false, "created": 1492051699.0, "created_utc": 1492022899.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg634q4", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "sbt_", "parent_id": "t1_dg55rjm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "So the folks at Deepmind are not true researchers? :P ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So the folks at Deepmind are not true researchers? :P &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg634q4", "score_hidden": false, "stickied": false, "created": 1492042311.0, "created_utc": 1492013511.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 4}}], "after": null, "before": null}}, "user_reports": [], "id": "dg55rjm", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "INTP008", "parent_id": "t1_dg52ho3", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "PyTorch is the best. It's as good as Lasagne. It's a framework for true researchers", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;PyTorch is the best. It&amp;#39;s as good as Lasagne. It&amp;#39;s a framework for true researchers&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg55rjm", "score_hidden": false, "stickied": false, "created": 1491984452.0, "created_utc": 1491955652.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 6}}], "after": null, "before": null}}, "user_reports": [], "id": "dg52ho3", "gilded": 0, "archived": false, "score": 22, "report_reasons": null, "author": "inkognit", "parent_id": "t3_64ro95", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "PyTorch is simply the best ML framework I've seen so far. Its simplicity is what makes it so good.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;PyTorch is simply the best ML framework I&amp;#39;ve seen so far. Its simplicity is what makes it so good.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg52ho3", "score_hidden": false, "stickied": false, "created": 1491980029.0, "created_utc": 1491951229.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 22}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg679ca", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "metacurse", "parent_id": "t1_dg63v1w", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Cool. Thanks!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Cool. Thanks!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg679ca", "score_hidden": false, "stickied": false, "created": 1492046577.0, "created_utc": 1492017777.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg63v1w", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "RaionTategami", "parent_id": "t1_dg5nzf5", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Depends what you mean by bottleneck? You only need to parse the sentences once. So it's preprocessing that does not need to be done over and over at training time. At test time it actually learnt to do the parsing itself.\nOr do you mean that it's limited by human feature engineering? Where this is deep learning so we should be learning the structures. I strongly think this, and there was a DeepMind paper that actually used a SPINN like model and used reinforce to *learn* how to parse sentences.", "edited": 1492019909.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Depends what you mean by bottleneck? You only need to parse the sentences once. So it&amp;#39;s preprocessing that does not need to be done over and over at training time. At test time it actually learnt to do the parsing itself.\nOr do you mean that it&amp;#39;s limited by human feature engineering? Where this is deep learning so we should be learning the structures. I strongly think this, and there was a DeepMind paper that actually used a SPINN like model and used reinforce to &lt;em&gt;learn&lt;/em&gt; how to parse sentences.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg63v1w", "score_hidden": false, "stickied": false, "created": 1492043084.0, "created_utc": 1492014284.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5nzf5", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "metacurse", "parent_id": "t1_dg5fbwe", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Hmm, thanks for that. I missed the detail. So given a sentence, we need to first construct a parse tree with a parser and then feed it to SPINN? Does that mean that it is (in some capacity) bottle-necked by the parser?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hmm, thanks for that. I missed the detail. So given a sentence, we need to first construct a parse tree with a parser and then feed it to SPINN? Does that mean that it is (in some capacity) bottle-necked by the parser?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5nzf5", "score_hidden": false, "stickied": false, "created": 1492017539.0, "created_utc": 1491988739.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5fbwe", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "RaionTategami", "parent_id": "t1_dg4kvcs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "How are they approximations? The hard pushing and popping in SPINN are supervised by parse trees.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How are they approximations? The hard pushing and popping in SPINN are supervised by parse trees.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5fbwe", "score_hidden": false, "stickied": false, "created": 1491997486.0, "created_utc": 1491968686.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg5w48e", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "evc123", "parent_id": "t1_dg5gr33", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Soujuergenmith Schmidhintalahuber", "edited": 1492006661.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Soujuergenmith Schmidhintalahuber&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5w48e", "score_hidden": false, "stickied": false, "created": 1492034338.0, "created_utc": 1492005538.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg5nx63", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "metacurse", "parent_id": "t1_dg5gr33", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Yeah, sorry about that - it just didn't come up at the top of my head. I've edited my post\n\n&gt; i expect someone to point out that this stuff used to exist in the 90s\n\nWould they be right or wrong?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, sorry about that - it just didn&amp;#39;t come up at the top of my head. I&amp;#39;ve edited my post&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;i expect someone to point out that this stuff used to exist in the 90s&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Would they be right or wrong?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5nx63", "score_hidden": false, "stickied": false, "created": 1492017340.0, "created_utc": 1491988540.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5gr33", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "r-sync", "parent_id": "t1_dg4kvcs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "not to be pedantic, but didn't the StackRNN paper come before Grefenstette's work? https://arxiv.org/abs/1503.01007\n(i expect someone to point out that this stuff used to exist in the 90s)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;not to be pedantic, but didn&amp;#39;t the StackRNN paper come before Grefenstette&amp;#39;s work? &lt;a href=\"https://arxiv.org/abs/1503.01007\"&gt;https://arxiv.org/abs/1503.01007&lt;/a&gt;\n(i expect someone to point out that this stuff used to exist in the 90s)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5gr33", "score_hidden": false, "stickied": false, "created": 1491999838.0, "created_utc": 1491971038.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4kvcs", "gilded": 0, "archived": false, "score": 4, "report_reasons": null, "author": "metacurse", "parent_id": "t3_64ro95", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Are there RNN models that produce Tree Structured outputs?\n\nAlso, why doesn't SPINN use a differentiable stack that Armand Joulin, Edward Grefenstette and others have developed? It would give lower variance gradients compared to other approximations right?\n\nEdit: StackRNN by Joulin and Mikolov came slightly before the Grefenstette et al work ", "edited": 1491988627.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are there RNN models that produce Tree Structured outputs?&lt;/p&gt;\n\n&lt;p&gt;Also, why doesn&amp;#39;t SPINN use a differentiable stack that Armand Joulin, Edward Grefenstette and others have developed? It would give lower variance gradients compared to other approximations right?&lt;/p&gt;\n\n&lt;p&gt;Edit: StackRNN by Joulin and Mikolov came slightly before the Grefenstette et al work &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4kvcs", "score_hidden": false, "stickied": false, "created": 1491959966.0, "created_utc": 1491931166.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 4}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64ro95", "likes": null, "replies": "", "user_reports": [], "id": "dg5o7or", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Kaixhin", "parent_id": "t3_64ro95", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "This is a great writeup - with only a basic knowledge of RNNs and NLP, I can still understand what this complex model is doing. Which is a testament to the writing, the readability of the code (which is really nice and efficient), and of course PyTorch, which makes code like this possible without sacrificing performance.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a great writeup - with only a basic knowledge of RNNs and NLP, I can still understand what this complex model is doing. Which is a testament to the writing, the readability of the code (which is really nice and efficient), and of course PyTorch, which makes code like this possible without sacrificing performance.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5o7or", "score_hidden": false, "stickied": false, "created": 1492018260.0, "created_utc": 1491989460.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}]