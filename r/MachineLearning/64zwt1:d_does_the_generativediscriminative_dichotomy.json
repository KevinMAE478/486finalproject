[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In supervised learning, we can make two equivalent statements of what each of the generative and discriminative models represent.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Generative models:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;[Verbal statement]&lt;/em&gt; models that represent how the data is generated.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;[Mathematical statement]&lt;/em&gt; models that estimate the joint distribution P(X,Y)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Discriminative models:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;[Verbal statement]&lt;/em&gt; models that discriminate between the data.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;[Mathematical statement]&lt;/em&gt; models that estimate the conditional distribution P(Y|X)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In unsupervised learning, it&amp;#39;s clear that the mathematical statements do not hold; because there is no Y in this case. However, can the verbal statements of generative/discriminative models hold in an unsupervised context?&lt;/p&gt;\n\n&lt;p&gt;My first thought was K-means vs Gaussian Mixture Model (GMM). K-means may practically seem like a discriminative model, as the end goal is usually to assign (aka discriminate) data points into specific clusters, while in GMMs we explicitly model P(X) and then use it to assign data points to clusters. But K-means is just a special case of GMMs where the covariance of the Gaussians is fixed and uniform mixing coefficients is assumed, so the dichotomy may not be valid in this case.&lt;/p&gt;\n\n&lt;p&gt;But what about something like PCA vs Probabilistic PCA? Can the dichotomy hold in that case? and In general, through the whole realm of &amp;quot;not that well-defined&amp;quot; unsupervised problems, is there&amp;#39;s a place for the generative/discriminative dichotomy?   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "In supervised learning, we can make two equivalent statements of what each of the generative and discriminative models represent.\n\n**Generative models:**\n\n* *[Verbal statement]* models that represent how the data is generated.\n* *[Mathematical statement]* models that estimate the joint distribution P(X,Y)\n\n**Discriminative models:** \n\n* *[Verbal statement]* models that discriminate between the data.\n* *[Mathematical statement]* models that estimate the conditional distribution P(Y|X)\n\n\nIn unsupervised learning, it's clear that the mathematical statements do not hold; because there is no Y in this case. However, can the verbal statements of generative/discriminative models hold in an unsupervised context?\n\n\nMy first thought was K-means vs Gaussian Mixture Model (GMM). K-means may practically seem like a discriminative model, as the end goal is usually to assign (aka discriminate) data points into specific clusters, while in GMMs we explicitly model P(X) and then use it to assign data points to clusters. But K-means is just a special case of GMMs where the covariance of the Gaussians is fixed and uniform mixing coefficients is assumed, so the dichotomy may not be valid in this case.\n\n\nBut what about something like PCA vs Probabilistic PCA? Can the dichotomy hold in that case? and In general, through the whole realm of \"not that well-defined\" unsupervised problems, is there's a place for the generative/discriminative dichotomy?   ", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "64zwt1", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 6, "report_reasons": null, "author": "mostafa-samir", "saved": false, "mod_reports": [], "name": "t3_64zwt1", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": 1492023791.0, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/64zwt1/d_does_the_generativediscriminative_dichotomy/", "num_reports": null, "locked": false, "stickied": false, "created": 1492049577.0, "url": "https://www.reddit.com/r/MachineLearning/comments/64zwt1/d_does_the_generativediscriminative_dichotomy/", "author_flair_text": null, "quarantine": false, "title": "[D] Does the Generative/Discriminative dichotomy extend to unsupervised learning?", "created_utc": 1492020777.0, "distinguished": null, "media": null, "upvote_ratio": 0.79, "num_comments": 3, "visited": false, "subreddit_type": "public", "ups": 6}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64zwt1", "likes": null, "replies": "", "user_reports": [], "id": "dg6cod9", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "OutOfApplesauce", "parent_id": "t3_64zwt1", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I'm only talking about one point, but isn't the point of  unsupervised techniques to *find* Y?  It seems really easy in my mind to use unsupervised learning to generate data especially in cases like clustering or anomaly detection.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m only talking about one point, but isn&amp;#39;t the point of  unsupervised techniques to &lt;em&gt;find&lt;/em&gt; Y?  It seems really easy in my mind to use unsupervised learning to generate data especially in cases like clustering or anomaly detection.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6cod9", "score_hidden": false, "stickied": false, "created": 1492052088.0, "created_utc": 1492023288.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64zwt1", "likes": null, "replies": "", "user_reports": [], "id": "dg6dv55", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "goblin_got_game", "parent_id": "t3_64zwt1", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Autoregressive models are sort of discriminative while being fully generative in that they factorize the joint as \n\n    log p(x1,..,xd) = log p(x1) + log p(x2|x1) + log p(x3|x1,x2) + ... + log p(xd|x1,...,x(d-1)).  \n\nThus you can think of them as ensembling a bunch of models that predict the next dimension from the previous ones.  Many of the unsupervised word embedding methods have this autoregressive flavor to them.  For instance, CBOW optimizes p(w|c1,..cK) where w is the 'center' word and the c's are the context words appearing around w.  Training proceeds by moving forward in the text so w becomes a context word and one of the c's becomes the center word.     ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Autoregressive models are sort of discriminative while being fully generative in that they factorize the joint as &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;log p(x1,..,xd) = log p(x1) + log p(x2|x1) + log p(x3|x1,x2) + ... + log p(xd|x1,...,x(d-1)).  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thus you can think of them as ensembling a bunch of models that predict the next dimension from the previous ones.  Many of the unsupervised word embedding methods have this autoregressive flavor to them.  For instance, CBOW optimizes p(w|c1,..cK) where w is the &amp;#39;center&amp;#39; word and the c&amp;#39;s are the context words appearing around w.  Training proceeds by moving forward in the text so w becomes a context word and one of the c&amp;#39;s becomes the center word.     &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6dv55", "score_hidden": false, "stickied": false, "created": 1492053321.0, "created_utc": 1492024521.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64zwt1", "likes": null, "replies": "", "user_reports": [], "id": "dg6n2k4", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "eaturbrainz", "parent_id": "t3_64zwt1", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "The *broad* point of unsupervised learning is to find some sparse representation from which the data can be (noisily) reconstructed.  A good way to put this is: we want to find some Q(X) that minimizes the KL divergence (or some other information divergence) from P(X) (the true data distribution), while still being easy to train, easy to compute with, and offering us *enough* information Q(Y|X) to do *something* useful with.\n\nSo with clustering, we can usually noisily generate a point within a given cluster (or hierarchy, intersection, etc of clusters).  With GMMs or other probabilistic methods, we're basically trying to estimate the data density while sticking to some parametric form.  With PCA and other forms of dimensionality reduction, we're trying to find a sparse set of parameters whose distribution in the training data yields a good approximation to the true test-data distribution.\n\nThis extends all the way up to energy-based learning, GANs, and probabilistic programs: we're trying to approximate the data distribution with a tractable model structure.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The &lt;em&gt;broad&lt;/em&gt; point of unsupervised learning is to find some sparse representation from which the data can be (noisily) reconstructed.  A good way to put this is: we want to find some Q(X) that minimizes the KL divergence (or some other information divergence) from P(X) (the true data distribution), while still being easy to train, easy to compute with, and offering us &lt;em&gt;enough&lt;/em&gt; information Q(Y|X) to do &lt;em&gt;something&lt;/em&gt; useful with.&lt;/p&gt;\n\n&lt;p&gt;So with clustering, we can usually noisily generate a point within a given cluster (or hierarchy, intersection, etc of clusters).  With GMMs or other probabilistic methods, we&amp;#39;re basically trying to estimate the data density while sticking to some parametric form.  With PCA and other forms of dimensionality reduction, we&amp;#39;re trying to find a sparse set of parameters whose distribution in the training data yields a good approximation to the true test-data distribution.&lt;/p&gt;\n\n&lt;p&gt;This extends all the way up to energy-based learning, GANs, and probabilistic programs: we&amp;#39;re trying to approximate the data distribution with a tractable model structure.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6n2k4", "score_hidden": false, "stickied": false, "created": 1492063300.0, "created_utc": 1492034500.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]