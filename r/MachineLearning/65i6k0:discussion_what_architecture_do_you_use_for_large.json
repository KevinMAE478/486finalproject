[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have reviews that have upto 5000 words and instead of passing word vectors as inputs to the LSTM I am passing sentence embeddings(skip-thought vectors) as inputs to the LSTM.&lt;/p&gt;\n\n&lt;p&gt;Is there any standard architecture that can be used for such long text data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "I have reviews that have upto 5000 words and instead of passing word vectors as inputs to the LSTM I am passing sentence embeddings(skip-thought vectors) as inputs to the LSTM.\n\nIs there any standard architecture that can be used for such long text data?", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "65i6k0", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 15, "report_reasons": null, "author": "cvikasreddy", "saved": false, "mod_reports": [], "name": "t3_65i6k0", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65i6k0/discussion_what_architecture_do_you_use_for_large/", "num_reports": null, "locked": false, "stickied": false, "created": 1492275118.0, "url": "https://www.reddit.com/r/MachineLearning/comments/65i6k0/discussion_what_architecture_do_you_use_for_large/", "author_flair_text": null, "quarantine": false, "title": "[Discussion] What architecture do you use for large text sentences?", "created_utc": 1492246318.0, "distinguished": null, "media": null, "upvote_ratio": 0.83, "num_comments": 18, "visited": false, "subreddit_type": "public", "ups": 15}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": "", "user_reports": [], "id": "dgb7iet", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "TalkingJellyFish", "parent_id": "t1_dgaiaxz", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Chats - classifying sentiment and customer intent. \nI never got a Conv working better than an RNN for my tasks but they train much faster (wall time). \nAuxiliary tasks help either way, domain specific parts of speech etc. I've been experimenting with having the model predict the topic distribution an LDA outputs, just to bridge the gap between character level inputs and semantic concepts a bit faster. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Chats - classifying sentiment and customer intent. \nI never got a Conv working better than an RNN for my tasks but they train much faster (wall time). \nAuxiliary tasks help either way, domain specific parts of speech etc. I&amp;#39;ve been experimenting with having the model predict the topic distribution an LDA outputs, just to bridge the gap between character level inputs and semantic concepts a bit faster. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb7iet", "score_hidden": false, "stickied": false, "created": 1492320810.0, "created_utc": 1492292010.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgaiaxz", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "shgidigo", "parent_id": "t1_dgahwqg", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "What categories did you classify? \n I'm working on classifying texts into topics, been using shallow convolutions, been reaching mediocre results,~ 60% accuarcy with ~20 categories", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What categories did you classify? \n I&amp;#39;m working on classifying texts into topics, been using shallow convolutions, been reaching mediocre results,~ 60% accuarcy with ~20 categories&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgaiaxz", "score_hidden": false, "stickied": false, "created": 1492279525.0, "created_utc": 1492250725.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": "", "user_reports": [], "id": "dgc4elc", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "nickshahml", "parent_id": "t1_dgb7chg", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Wow. I'll explore the repo and comment on the issue section of your repo. I'm thinking of passing the sentences at a word level through embedding and constructing an \"image\" of [batch_size, timesteps, embedding_size]. Let me check it out and go from there. Thanks for doing it in TF!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wow. I&amp;#39;ll explore the repo and comment on the issue section of your repo. I&amp;#39;m thinking of passing the sentences at a word level through embedding and constructing an &amp;quot;image&amp;quot; of [batch_size, timesteps, embedding_size]. Let me check it out and go from there. Thanks for doing it in TF!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgc4elc", "score_hidden": false, "stickied": false, "created": 1492383914.0, "created_utc": 1492355114.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb7chg", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "TalkingJellyFish", "parent_id": "t1_dgaoohu", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "[I put up a repo just for you](https://github.com/talolard/DenseContinuousSentances) :-) Still rough around the edged. \nI think with classifying whole chunks of text (sentance/ paragraph) you get hit hard by vanishing gradient quick. Their is a lot of distance between your raw input and the signal your loss provides. I usually add an auxiliary task like POS tagging (domain specific) to encourage the network to learn lower level parts of the task faster. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/talolard/DenseContinuousSentances\"&gt;I put up a repo just for you&lt;/a&gt; :-) Still rough around the edged. \nI think with classifying whole chunks of text (sentance/ paragraph) you get hit hard by vanishing gradient quick. Their is a lot of distance between your raw input and the signal your loss provides. I usually add an auxiliary task like POS tagging (domain specific) to encourage the network to learn lower level parts of the task faster. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb7chg", "score_hidden": false, "stickied": false, "created": 1492320585.0, "created_utc": 1492291785.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dgaoohu", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "nickshahml", "parent_id": "t1_dgahwqg", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Hey TalkingJellyFish, have you found for **language** binary classification works well with Dense Nets? I'm thinking you would do 1d convolutions with Dense Net. Do you know of any repos that have done this?", "edited": 1492287821.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey TalkingJellyFish, have you found for &lt;strong&gt;language&lt;/strong&gt; binary classification works well with Dense Nets? I&amp;#39;m thinking you would do 1d convolutions with Dense Net. Do you know of any repos that have done this?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgaoohu", "score_hidden": false, "stickied": false, "created": 1492294579.0, "created_utc": 1492265779.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgahwqg", "gilded": 0, "archived": false, "score": 11, "report_reasons": null, "author": "TalkingJellyFish", "parent_id": "t3_65i6k0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I struggled with this for a while.\nI ended up using convolutions and got much more reasonable trying times.\nCheck out googles bytenet for an example at the charecter level.\n\nIf you're doing sentence classification, I had success adapting densenets", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I struggled with this for a while.\nI ended up using convolutions and got much more reasonable trying times.\nCheck out googles bytenet for an example at the charecter level.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re doing sentence classification, I had success adapting densenets&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgahwqg", "score_hidden": false, "stickied": false, "created": 1492278222.0, "created_utc": 1492249422.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 11}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": "", "user_reports": [], "id": "dgaqf9k", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "syllogism_", "parent_id": "t3_65i6k0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I usually project the document labels down to the sentences, and then  train a per-sentence model. Then to get the document prediction I average the sentence predictions.\n\nThe reason is that the inter-sentence effects aren't usually very important --- and models are really bad at capturing them anyway. So you may as well have a \"bag of sentences\".\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I usually project the document labels down to the sentences, and then  train a per-sentence model. Then to get the document prediction I average the sentence predictions.&lt;/p&gt;\n\n&lt;p&gt;The reason is that the inter-sentence effects aren&amp;#39;t usually very important --- and models are really bad at capturing them anyway. So you may as well have a &amp;quot;bag of sentences&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgaqf9k", "score_hidden": false, "stickied": false, "created": 1492297313.0, "created_utc": 1492268513.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": "", "user_reports": [], "id": "dgbblje", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "santty128", "parent_id": "t1_dgb8fc6", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Well there are two components that matter regarding these convolutional architectures. Pick WaveNet for instance: 1) Causal convolutions, where receptive field only looks backwards in time to respect sequential nature from past to future, such that y(t) depends on x(t), x(t-1), ... x(t-T) (being T the receptive field you claimed, including the atrous structures and multiple layers). 2) Recurrent operation, where each output sample y(t) is extracted by a full propagation of x(t) ... x(t-T) and then fed back to become x(t), and this is why an autoregressive architecture like WaveNet is slow to generate samples. In the speech case there are 16.000 feed forward propagations per second =/. This second point is then what gives the truth notion of order to the generation, emulating an RNN somehow, because every previously generated sample y(t-1) is fed back such that it's the input x(t) as in RNNs. So a convnet is not ordered by itself, the order is given with the posterior recursion methodology. In the QRNN case this second step is done with the recurrent pooling operation that includes different gating flavours, applying the same principle of feeding back predictions. I totally recommend reading the QRNN paper as it gives clear understanding on this two-stage methodology.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well there are two components that matter regarding these convolutional architectures. Pick WaveNet for instance: 1) Causal convolutions, where receptive field only looks backwards in time to respect sequential nature from past to future, such that y(t) depends on x(t), x(t-1), ... x(t-T) (being T the receptive field you claimed, including the atrous structures and multiple layers). 2) Recurrent operation, where each output sample y(t) is extracted by a full propagation of x(t) ... x(t-T) and then fed back to become x(t), and this is why an autoregressive architecture like WaveNet is slow to generate samples. In the speech case there are 16.000 feed forward propagations per second =/. This second point is then what gives the truth notion of order to the generation, emulating an RNN somehow, because every previously generated sample y(t-1) is fed back such that it&amp;#39;s the input x(t) as in RNNs. So a convnet is not ordered by itself, the order is given with the posterior recursion methodology. In the QRNN case this second step is done with the recurrent pooling operation that includes different gating flavours, applying the same principle of feeding back predictions. I totally recommend reading the QRNN paper as it gives clear understanding on this two-stage methodology.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbblje", "score_hidden": false, "stickied": false, "created": 1492326645.0, "created_utc": 1492297845.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb8fc6", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "TalkingJellyFish", "parent_id": "t1_dgb80rq", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "You're challenging my assumptions. \nI'm assuming that for any given layer in a  convnet each unit is looking at whatever is in its receptive field and that if order matters for the task then the Convent will learn that implicitly. In other words, their is no loss of order dependent information when using convolutions. As evidence I bring forth Bytenet for translation and wavenet for speech synthesis which are convolutional models applied successfully to order dependent tasks. Am I missing something? \nAlso Quasi RNN has been on my to read list for a while, I'll get at it today ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;re challenging my assumptions. \nI&amp;#39;m assuming that for any given layer in a  convnet each unit is looking at whatever is in its receptive field and that if order matters for the task then the Convent will learn that implicitly. In other words, their is no loss of order dependent information when using convolutions. As evidence I bring forth Bytenet for translation and wavenet for speech synthesis which are convolutional models applied successfully to order dependent tasks. Am I missing something? \nAlso Quasi RNN has been on my to read list for a while, I&amp;#39;ll get at it today &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb8fc6", "score_hidden": false, "stickied": false, "created": 1492322060.0, "created_utc": 1492293260.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb80rq", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "santty128", "parent_id": "t1_dgb7kqs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "If it's a matter of speed but you still want to retain order you should check Quasi-RNN (https://arxiv.org/abs/1611.01576), very nice idea mixing the best of conv (speed) + recurrent (sequential behavior) :) ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If it&amp;#39;s a matter of speed but you still want to retain order you should check Quasi-RNN (&lt;a href=\"https://arxiv.org/abs/1611.01576\"&gt;https://arxiv.org/abs/1611.01576&lt;/a&gt;), very nice idea mixing the best of conv (speed) + recurrent (sequential behavior) :) &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb80rq", "score_hidden": false, "stickied": false, "created": 1492321503.0, "created_utc": 1492292703.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgb7kqs", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "TalkingJellyFish", "parent_id": "t1_dgaia1f", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I'm not making any claims about which one works better. I am claiming that Convs work faster because they can do more in parallel. It won't get me published in NIPS but it gets the job done. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not making any claims about which one works better. I am claiming that Convs work faster because they can do more in parallel. It won&amp;#39;t get me published in NIPS but it gets the job done. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgb7kqs", "score_hidden": false, "stickied": false, "created": 1492320895.0, "created_utc": 1492292095.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgaia1f", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "santty128", "parent_id": "t3_65i6k0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "What's the task? classifying the topic? It's a matter of \"how much does the order affect my ultimate task\". You might be fine using conv structures with sentence embeddings (I'm assuming you've something like paragraphs) to find local relations between them without a sense of order and then classify. However if you need to retain the sequential behaviour, what's the final amount of input vectors to your LSTM? If it's &lt; few hundreds an LSTM w/ some gradient clipping should be fine I think.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the task? classifying the topic? It&amp;#39;s a matter of &amp;quot;how much does the order affect my ultimate task&amp;quot;. You might be fine using conv structures with sentence embeddings (I&amp;#39;m assuming you&amp;#39;ve something like paragraphs) to find local relations between them without a sense of order and then classify. However if you need to retain the sequential behaviour, what&amp;#39;s the final amount of input vectors to your LSTM? If it&amp;#39;s &amp;lt; few hundreds an LSTM w/ some gradient clipping should be fine I think.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgaia1f", "score_hidden": false, "stickied": false, "created": 1492279444.0, "created_utc": 1492250644.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": "", "user_reports": [], "id": "dgarccs", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "ma2rten", "parent_id": "t3_65i6k0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I actually think a linear classifier with ngrams and tf-idf weighting is hard to beat in this case. Bag of words has an advantage on longer text because it is getting more statistics whereas LSTMs have a disadvantage.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I actually think a linear classifier with ngrams and tf-idf weighting is hard to beat in this case. Bag of words has an advantage on longer text because it is getting more statistics whereas LSTMs have a disadvantage.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgarccs", "score_hidden": false, "stickied": false, "created": 1492298635.0, "created_utc": 1492269835.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": "", "user_reports": [], "id": "dgbqwo5", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "cvikasreddy", "parent_id": "t1_dgahwqh", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Is there any open source implementation of DenseNets applied on text ?\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is there any open source implementation of DenseNets applied on text ?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbqwo5", "score_hidden": false, "stickied": false, "created": 1492350217.0, "created_utc": 1492321417.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgahwqh", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "TalkingJellyFish", "parent_id": "t3_65i6k0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I struggled with this for a while.\nI ended up using convolutions and got much more reasonable trying times.\nCheck out googles bytenet for an example at the charecter level.\n\nIf you're doing sentence classification, I had success adapting densenets", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I struggled with this for a while.\nI ended up using convolutions and got much more reasonable trying times.\nCheck out googles bytenet for an example at the charecter level.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re doing sentence classification, I had success adapting densenets&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgahwqh", "score_hidden": false, "stickied": false, "created": 1492278223.0, "created_utc": 1492249423.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65i6k0", "likes": null, "replies": "", "user_reports": [], "id": "dgbsjt4", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "TalkingJellyFish", "parent_id": "t1_dgbqw1j", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I put up a rough version [here](https://github.com/talolard/DenseContinuousSentances)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I put up a rough version &lt;a href=\"https://github.com/talolard/DenseContinuousSentances\"&gt;here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbsjt4", "score_hidden": false, "stickied": false, "created": 1492354078.0, "created_utc": 1492325278.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}], "after": null, "before": null}}, "user_reports": [], "id": "dgbqw1j", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "cvikasreddy", "parent_id": "t1_dgaop0t", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Is there any open source implementation of DenseNets applied on text ?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is there any open source implementation of DenseNets applied on text ?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgbqw1j", "score_hidden": false, "stickied": false, "created": 1492350178.0, "created_utc": 1492321378.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgaop0t", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "darkconfidantislife", "parent_id": "t3_65i6k0", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Call me crazy, but convolutional DenseNets have worked well for sentence classification tasks for me. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Call me crazy, but convolutional DenseNets have worked well for sentence classification tasks for me. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgaop0t", "score_hidden": false, "stickied": false, "created": 1492294603.0, "created_utc": 1492265803.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]