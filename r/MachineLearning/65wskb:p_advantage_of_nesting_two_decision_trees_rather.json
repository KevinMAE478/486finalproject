[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3 classes which data will be classified into.&lt;/p&gt;\n\n&lt;p&gt;Class B and C are fairly similar, Class A is very different.&lt;/p&gt;\n\n&lt;p&gt;Is there any advantage in running a decision tree to seperate (A) or (B and C) first, then building a decision tree to run on (B and C) rather than trying to seperate them all in one go?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using ID3, and would just call B and C a single group if I were to run the two decision tree approach, then split them after I have taken A out of the picture.&lt;/p&gt;\n\n&lt;p&gt;(I&amp;#39;m right that the calculation for entropy can be extended for 3 classes right?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "selftext": "I have 3 classes which data will be classified into.\n\nClass B and C are fairly similar, Class A is very different.\n\nIs there any advantage in running a decision tree to seperate (A) or (B and C) first, then building a decision tree to run on (B and C) rather than trying to seperate them all in one go?\n\nI'm using ID3, and would just call B and C a single group if I were to run the two decision tree approach, then split them after I have taken A out of the picture.\n\n(I'm right that the calculation for entropy can be extended for 3 classes right?)", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Project", "id": "65wskb", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 2, "report_reasons": null, "author": "Stripes96", "saved": false, "mod_reports": [], "name": "t3_65wskb", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "self.MachineLearning", "hidden": false, "thumbnail": "self", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "four", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": true, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/65wskb/p_advantage_of_nesting_two_decision_trees_rather/", "num_reports": null, "locked": false, "stickied": false, "created": 1492477137.0, "url": "https://www.reddit.com/r/MachineLearning/comments/65wskb/p_advantage_of_nesting_two_decision_trees_rather/", "author_flair_text": null, "quarantine": false, "title": "[P] advantage of nesting two decision trees rather than using single larger one", "created_utc": 1492448337.0, "distinguished": null, "media": null, "upvote_ratio": 1.0, "num_comments": 2, "visited": false, "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65wskb", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_65wskb", "likes": null, "replies": "", "user_reports": [], "id": "dgeup3u", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Stripes96", "parent_id": "t1_dgdyrk7", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thank you so much - I'm coming down to the wire on this project and you answered both my questions when I thought I was high and dry", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you so much - I&amp;#39;m coming down to the wire on this project and you answered both my questions when I thought I was high and dry&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgeup3u", "score_hidden": false, "stickied": false, "created": 1492530232.0, "created_utc": 1492501432.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dgdyrk7", "gilded": 1, "archived": false, "score": 1, "report_reasons": null, "author": "micro_cam", "parent_id": "t3_65wskb", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Entropy and gini impurity work fine with multiple classes and most decision tree and random forest implementations do this.\n\nLog loss doesn't extend in a way that lets you learn multiple classes with most trees so most boosted decision tree implementations end up using an ensemble of trees for each class. Since it sounds like you're not interested in an ensemble this doesn't really apply to you.\n\nIf you do multiple trees you probably want three trees, A vs (B and C). B vs (A and C) and C vs (A and B). You technically only need two but the choice of how to split things up will make things less stable.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Entropy and gini impurity work fine with multiple classes and most decision tree and random forest implementations do this.&lt;/p&gt;\n\n&lt;p&gt;Log loss doesn&amp;#39;t extend in a way that lets you learn multiple classes with most trees so most boosted decision tree implementations end up using an ensemble of trees for each class. Since it sounds like you&amp;#39;re not interested in an ensemble this doesn&amp;#39;t really apply to you.&lt;/p&gt;\n\n&lt;p&gt;If you do multiple trees you probably want three trees, A vs (B and C). B vs (A and C) and C vs (A and B). You technically only need two but the choice of how to split things up will make things less stable.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dgdyrk7", "score_hidden": false, "stickied": false, "created": 1492485581.0, "created_utc": 1492456781.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]