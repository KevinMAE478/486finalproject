[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Discussion", "id": "64rpje", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 72, "report_reasons": null, "author": "yvespeirsman", "saved": false, "mod_reports": [], "name": "t3_64rpje", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "nlp.yvespeirsman.be", "hidden": false, "preview": {"images": [{"source": {"url": "https://i.redditmedia.com/4-JqnYOZFIYd0CQyWAIK5vFlL7WEvSXONTRQQrLfD9U.jpg?s=bcdeac26137d6676e672004583044a0d", "width": 1292, "height": 760}, "resolutions": [{"url": "https://i.redditmedia.com/4-JqnYOZFIYd0CQyWAIK5vFlL7WEvSXONTRQQrLfD9U.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=84300e59231c51c70c6b3d868389950f", "width": 108, "height": 63}, {"url": "https://i.redditmedia.com/4-JqnYOZFIYd0CQyWAIK5vFlL7WEvSXONTRQQrLfD9U.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=ae31bfc2ff3636ce4003457241ef83de", "width": 216, "height": 127}, {"url": "https://i.redditmedia.com/4-JqnYOZFIYd0CQyWAIK5vFlL7WEvSXONTRQQrLfD9U.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=f10cc8b26b4a179e45a8d5f082864813", "width": 320, "height": 188}, {"url": "https://i.redditmedia.com/4-JqnYOZFIYd0CQyWAIK5vFlL7WEvSXONTRQQrLfD9U.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=639c42d37f71f8c80a610484bbe267c8", "width": 640, "height": 376}, {"url": "https://i.redditmedia.com/4-JqnYOZFIYd0CQyWAIK5vFlL7WEvSXONTRQQrLfD9U.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=960&amp;s=c561987185f618294029ad6f8370ac6f", "width": 960, "height": 564}, {"url": "https://i.redditmedia.com/4-JqnYOZFIYd0CQyWAIK5vFlL7WEvSXONTRQQrLfD9U.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=1080&amp;s=2fecb897c67e46df0def771b4071e3b9", "width": 1080, "height": 635}], "variants": {}, "id": "073a9ZerugPpIyA3ZU0j9LJ2YPuykSz9ALxyNhyuzYs"}], "enabled": false}, "thumbnail": "https://b.thumbs.redditmedia.com/Nr8zDfOkjo5gGyiUNQQw2B3uM2lPStEXcbT_vvaB4xc.jpg", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "one", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "post_hint": "link", "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/64rpje/d_anything2vec_or_how_word2vec_conquered_nlp/", "num_reports": null, "locked": false, "stickied": false, "created": 1491955101.0, "url": "http://nlp.yvespeirsman.be/blog/anything2vec/", "author_flair_text": null, "quarantine": false, "title": "[D] Anything2Vec, or How Word2Vec Conquered NLP", "created_utc": 1491926301.0, "distinguished": null, "media": null, "upvote_ratio": 0.87, "num_comments": 27, "visited": false, "subreddit_type": "public", "ups": 72}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg7chmm", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "liconvalleysi", "parent_id": "t1_dg4xjl8", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "The title could have also read \"Word2Vec is based on an approach from Lawrence Berkeley National Lab\" https://www.kaggle.com/c/word2vec-nlp-tutorial/discussion/12349", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The title could have also read &amp;quot;Word2Vec is based on an approach from Lawrence Berkeley National Lab&amp;quot; &lt;a href=\"https://www.kaggle.com/c/word2vec-nlp-tutorial/discussion/12349\"&gt;https://www.kaggle.com/c/word2vec-nlp-tutorial/discussion/12349&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7chmm", "score_hidden": false, "stickied": false, "created": 1492104206.0, "created_utc": 1492075406.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4xjl8", "gilded": 0, "archived": false, "score": 40, "report_reasons": null, "author": "Latent_space", "parent_id": "t3_64rpje", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "this title is a frustrating level of contention. nlp is far from conquered. and deep learning is considered to be effective in nlp, but not as effective as it was in computer vision.\n\n(edit: it -&gt; in)", "edited": 1491945396.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;this title is a frustrating level of contention. nlp is far from conquered. and deep learning is considered to be effective in nlp, but not as effective as it was in computer vision.&lt;/p&gt;\n\n&lt;p&gt;(edit: it -&amp;gt; in)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4xjl8", "score_hidden": false, "stickied": false, "created": 1491973869.0, "created_utc": 1491945069.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 40}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg57rc9", "gilded": 0, "archived": false, "score": 3, "report_reasons": null, "author": "epicwisdom", "parent_id": "t1_dg55xdn", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "To be specific, [here are the SemEval tasks](http://alt.qcri.org/semeval2017/index.php?id=tasks), and in particular, [Task 2](http://alt.qcri.org/semeval2017/task2/), which is the one that ConceptNet Numberbatch won.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;To be specific, &lt;a href=\"http://alt.qcri.org/semeval2017/index.php?id=tasks\"&gt;here are the SemEval tasks&lt;/a&gt;, and in particular, &lt;a href=\"http://alt.qcri.org/semeval2017/task2/\"&gt;Task 2&lt;/a&gt;, which is the one that ConceptNet Numberbatch won.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg57rc9", "score_hidden": false, "stickied": false, "created": 1491987084.0, "created_utc": 1491958284.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 3}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg5wllf", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "maxToTheJ", "parent_id": "t1_dg5o3jw", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I honestly find more use in the self promotion as long as it has other sources and information than a joke , or just a generic critique (although sometimes a critique is appropriate) , or a Product/project manager interested in machine learning parroting what he has heard elsewhere ", "edited": 1492006685.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I honestly find more use in the self promotion as long as it has other sources and information than a joke , or just a generic critique (although sometimes a critique is appropriate) , or a Product/project manager interested in machine learning parroting what he has heard elsewhere &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5wllf", "score_hidden": false, "stickied": false, "created": 1492034948.0, "created_utc": 1492006148.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5o3jw", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "wsnqwn", "parent_id": "t1_dg57khl", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "The comment is mostly self-promotion and the only thing it shows is that the next person coming up with a new technique to create word embeddings should think twice about releasing pre-trained models. Comparing a model trained on the spammy multilingual CommonCrawl dumps with a model Google trained on tons of high-quality English news articles really doesn't reflect the strength of the underlying methods.\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The comment is mostly self-promotion and the only thing it shows is that the next person coming up with a new technique to create word embeddings should think twice about releasing pre-trained models. Comparing a model trained on the spammy multilingual CommonCrawl dumps with a model Google trained on tons of high-quality English news articles really doesn&amp;#39;t reflect the strength of the underlying methods.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5o3jw", "score_hidden": false, "stickied": false, "created": 1492017902.0, "created_utc": 1491989102.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg57khl", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "maxToTheJ", "parent_id": "t1_dg55xdn", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Thanks for the summary and survey with relevant links. Posts like yours are why I am still subscribed despite the change in demographics ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the summary and survey with relevant links. Posts like yours are why I am still subscribed despite the change in demographics &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg57khl", "score_hidden": false, "stickied": false, "created": 1491986831.0, "created_utc": 1491958031.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg62nzp", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "JustFinishedBSG", "parent_id": "t1_dg5y6pn", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Maybe it's just me who knows :)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe it&amp;#39;s just me who knows :)&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg62nzp", "score_hidden": false, "stickied": false, "created": 1492041805.0, "created_utc": 1492013005.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5y6pn", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "rspeer", "parent_id": "t1_dg5n33d", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Man. I'll try to change that!", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Man. I&amp;#39;ll try to change that!&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5y6pn", "score_hidden": false, "stickied": false, "created": 1492036844.0, "created_utc": 1492008044.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5n33d", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "JustFinishedBSG", "parent_id": "t1_dg55xdn", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Your article has a very angry and bitter tone haha. Understandably", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your article has a very angry and bitter tone haha. Understandably&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5n33d", "score_hidden": false, "stickied": false, "created": 1492014682.0, "created_utc": 1491985882.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg55xdn", "gilded": 0, "archived": false, "score": 38, "report_reasons": null, "author": "rspeer", "parent_id": "t1_dg4jgjm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Levy and Goldberg wrote [a survey paper](https://levyomer.files.wordpress.com/2014/04/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf) where they tried word2vec and GloVe with a bunch of different parameters. That led people to come to the over-broad conclusion that word2vec outperforms GloVe all the time, which I don't think is true.\n\nOne thing that happened with GloVe is that they released two pre-trained versions: a version trained on 42 billion tokens, which worked pretty well but was inconclusively better than word2vec; and a version trained on 840 billion tokens, which should have worked much better but kind of got mangled.\n\nMany people tried the 840B data, said \"well that didn't work\", and backed off to the 42B data or to word2vec. Most papers I see that use pre-trained GloVe are referring to the 42B data.\n\nAt Luminoso, we identified the problems in the 840B data (spurious differences due to capitalization, UTF-8 errors, and over-weighted dimensions that could be fixed with L1 normalization), and [we fixed them](https://blog.conceptnet.io/2016/05/19/an-introduction-to-the-conceptnet-vector-ensemble/). GloVe 840B renormalized was the best word-vector data you could get in 2015.\n\nNow, what I don't understand is why everyone still talks about word vectors you could get in 2013 or 2015. This article asks \"what can we gain by adding explicit linguistic information beyond word order?\" and in fact people have been *doing* pretty much that, adding explicit information from knowledge graphs.\n\nSo let me summarize some things you should know if you don't want to be frozen in time in this field:\n\n* The best paper award at NAACL 2015 went to Manaal Faruqui, for \"retrofitting\", a wonderfully straightforward technique for fixing the blind spots of existing word embeddings using structured knowledge.\n* The state-of-the-art word vector system of 2016 was [NASARI](http://lcl.uniroma1.it/nasari/) by Jos\u00e9 Camacho-Collados at Sapienza University of Rome, which uses knowledge from BabelNet.\n* The state-of-the-art word vector system of 2017, winning by a large margin at the SemEval 2017 competition in four languages, is [ConceptNet Numberbatch](https://blog.conceptnet.io/2017/03/02/how-luminoso-made-conceptnet-into-the-best-word-vectors-and-won-at-semeval/), developed at Luminoso. It uses knowledge from [ConceptNet](http://www.conceptnet.io/), in addition to word2vec and renormalized GloVe.\n\nTo be clear: I develop ConceptNet Numberbatch. But there is nothing subjective about the SemEval results.", "edited": 1491957790.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Levy and Goldberg wrote &lt;a href=\"https://levyomer.files.wordpress.com/2014/04/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf\"&gt;a survey paper&lt;/a&gt; where they tried word2vec and GloVe with a bunch of different parameters. That led people to come to the over-broad conclusion that word2vec outperforms GloVe all the time, which I don&amp;#39;t think is true.&lt;/p&gt;\n\n&lt;p&gt;One thing that happened with GloVe is that they released two pre-trained versions: a version trained on 42 billion tokens, which worked pretty well but was inconclusively better than word2vec; and a version trained on 840 billion tokens, which should have worked much better but kind of got mangled.&lt;/p&gt;\n\n&lt;p&gt;Many people tried the 840B data, said &amp;quot;well that didn&amp;#39;t work&amp;quot;, and backed off to the 42B data or to word2vec. Most papers I see that use pre-trained GloVe are referring to the 42B data.&lt;/p&gt;\n\n&lt;p&gt;At Luminoso, we identified the problems in the 840B data (spurious differences due to capitalization, UTF-8 errors, and over-weighted dimensions that could be fixed with L1 normalization), and &lt;a href=\"https://blog.conceptnet.io/2016/05/19/an-introduction-to-the-conceptnet-vector-ensemble/\"&gt;we fixed them&lt;/a&gt;. GloVe 840B renormalized was the best word-vector data you could get in 2015.&lt;/p&gt;\n\n&lt;p&gt;Now, what I don&amp;#39;t understand is why everyone still talks about word vectors you could get in 2013 or 2015. This article asks &amp;quot;what can we gain by adding explicit linguistic information beyond word order?&amp;quot; and in fact people have been &lt;em&gt;doing&lt;/em&gt; pretty much that, adding explicit information from knowledge graphs.&lt;/p&gt;\n\n&lt;p&gt;So let me summarize some things you should know if you don&amp;#39;t want to be frozen in time in this field:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The best paper award at NAACL 2015 went to Manaal Faruqui, for &amp;quot;retrofitting&amp;quot;, a wonderfully straightforward technique for fixing the blind spots of existing word embeddings using structured knowledge.&lt;/li&gt;\n&lt;li&gt;The state-of-the-art word vector system of 2016 was &lt;a href=\"http://lcl.uniroma1.it/nasari/\"&gt;NASARI&lt;/a&gt; by Jos\u00e9 Camacho-Collados at Sapienza University of Rome, which uses knowledge from BabelNet.&lt;/li&gt;\n&lt;li&gt;The state-of-the-art word vector system of 2017, winning by a large margin at the SemEval 2017 competition in four languages, is &lt;a href=\"https://blog.conceptnet.io/2017/03/02/how-luminoso-made-conceptnet-into-the-best-word-vectors-and-won-at-semeval/\"&gt;ConceptNet Numberbatch&lt;/a&gt;, developed at Luminoso. It uses knowledge from &lt;a href=\"http://www.conceptnet.io/\"&gt;ConceptNet&lt;/a&gt;, in addition to word2vec and renormalized GloVe.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To be clear: I develop ConceptNet Numberbatch. But there is nothing subjective about the SemEval results.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg55xdn", "score_hidden": false, "stickied": false, "created": 1491984658.0, "created_utc": 1491955858.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 38}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg55h20", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "maxToTheJ", "parent_id": "t1_dg55b5z", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I don't know. He must acquit.\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know. He must acquit.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg55h20", "score_hidden": false, "stickied": false, "created": 1491984070.0, "created_utc": 1491955270.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 9}}], "after": null, "before": null}}, "user_reports": [], "id": "dg55b5z", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "beltsazar", "parent_id": "t1_dg4lhej", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Could you explain?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Could you explain?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg55b5z", "score_hidden": false, "stickied": false, "created": 1491983848.0, "created_utc": 1491955048.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4lhej", "gilded": 0, "archived": false, "score": 29, "report_reasons": null, "author": "skhehw", "parent_id": "t1_dg4jgjm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "it didn't fit.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;it didn&amp;#39;t fit.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4lhej", "score_hidden": false, "stickied": false, "created": 1491960634.0, "created_utc": 1491931834.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 29}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg5vawk", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "nickdhaynes", "parent_id": "t1_dg5oz86", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Nope, that previous comment definitely wasn't meant to be a scientific/statistical statement. The only point that I was trying to make is that I'm skeptical of any claims that word2vec is absolutely better than GloVe for task X (or vice versa), especially when the two algorithms are trained on different datasets. And the fact that their performance is \"similar\" is consistent with the intuition that the algorithms are capturing \"similar\" information.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nope, that previous comment definitely wasn&amp;#39;t meant to be a scientific/statistical statement. The only point that I was trying to make is that I&amp;#39;m skeptical of any claims that word2vec is absolutely better than GloVe for task X (or vice versa), especially when the two algorithms are trained on different datasets. And the fact that their performance is &amp;quot;similar&amp;quot; is consistent with the intuition that the algorithms are capturing &amp;quot;similar&amp;quot; information.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5vawk", "score_hidden": false, "stickied": false, "created": 1492033258.0, "created_utc": 1492004458.0, "depth": 5, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5oz86", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Latent_space", "parent_id": "t1_dg58rzk", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "'pretty close' feels a bit too vague for this type of thing.  statistical significance matters in science.  \n\nedit: sure, they're similar. there's also [conceptnet](https://blog.conceptnet.io/2017/03/02/how-luminoso-made-conceptnet-into-the-best-word-vectors-and-won-at-semeval/) which used a semantic lexicon to integrate the various embeddings.  so, if we were asking if they correlate, they would definitely do so.  I just wanted to make the point that you can't swap them out and claim the same statistical performance :). \n\nedit2: actually read through rest of the thread. I see /u/rspeer  (conceptnet developer) showed up to discuss things. cool :). ", "edited": 1491992334.0, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#39;pretty close&amp;#39; feels a bit too vague for this type of thing.  statistical significance matters in science.  &lt;/p&gt;\n\n&lt;p&gt;edit: sure, they&amp;#39;re similar. there&amp;#39;s also &lt;a href=\"https://blog.conceptnet.io/2017/03/02/how-luminoso-made-conceptnet-into-the-best-word-vectors-and-won-at-semeval/\"&gt;conceptnet&lt;/a&gt; which used a semantic lexicon to integrate the various embeddings.  so, if we were asking if they correlate, they would definitely do so.  I just wanted to make the point that you can&amp;#39;t swap them out and claim the same statistical performance :). &lt;/p&gt;\n\n&lt;p&gt;edit2: actually read through rest of the thread. I see &lt;a href=\"/u/rspeer\"&gt;/u/rspeer&lt;/a&gt;  (conceptnet developer) showed up to discuss things. cool :). &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5oz86", "score_hidden": false, "stickied": false, "created": 1492020642.0, "created_utc": 1491991842.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg58rzk", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "nickdhaynes", "parent_id": "t1_dg4xgzm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Ehh, they're *pretty close* in that paper and in a couple others I've seen (that I'm too lazy to look up now). And the fact that the word2vec and GloVe embeddings were trained on different datasets makes me somewhat skeptical of apples-to-apples comparisons.\n\nOverall, the evidence that I've seen supports /u/lmcinnes's claim that the two algorithms are basically capturing the same information.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ehh, they&amp;#39;re &lt;em&gt;pretty close&lt;/em&gt; in that paper and in a couple others I&amp;#39;ve seen (that I&amp;#39;m too lazy to look up now). And the fact that the word2vec and GloVe embeddings were trained on different datasets makes me somewhat skeptical of apples-to-apples comparisons.&lt;/p&gt;\n\n&lt;p&gt;Overall, the evidence that I&amp;#39;ve seen supports &lt;a href=\"/u/lmcinnes\"&gt;/u/lmcinnes&lt;/a&gt;&amp;#39;s claim that the two algorithms are basically capturing the same information.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg58rzk", "score_hidden": false, "stickied": false, "created": 1491988403.0, "created_utc": 1491959603.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 6}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg5pcij", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "Latent_space", "parent_id": "t1_dg5p7xo", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "it's crazy how effective and simple it is.  ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;it&amp;#39;s crazy how effective and simple it is.  &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5pcij", "score_hidden": false, "stickied": false, "created": 1492021753.0, "created_utc": 1491992953.0, "depth": 4, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg5p7xo", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "Jean-Porte", "parent_id": "t1_dg4xgzm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Great paper, thanks", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Great paper, thanks&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5p7xo", "score_hidden": false, "stickied": false, "created": 1492021374.0, "created_utc": 1491992574.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg5wt4d", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "svmmvs", "parent_id": "t1_dg4xgzm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "In a recent talk by Rus Salakhu, he said that Glove vectors generally outperform word2vec.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In a recent talk by Rus Salakhu, he said that Glove vectors generally outperform word2vec.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg5wt4d", "score_hidden": false, "stickied": false, "created": 1492035209.0, "created_utc": 1492006409.0, "depth": 3, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4xgzm", "gilded": 0, "archived": false, "score": 6, "report_reasons": null, "author": "Latent_space", "parent_id": "t1_dg4mkst", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "There's actually papers showing that they out-perform each other on different tasks. e.g. [all but the top](https://arxiv.org/pdf/1702.01417.pdf)\n\n", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s actually papers showing that they out-perform each other on different tasks. e.g. &lt;a href=\"https://arxiv.org/pdf/1702.01417.pdf\"&gt;all but the top&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4xgzm", "score_hidden": false, "stickied": false, "created": 1491973780.0, "created_utc": 1491944980.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 6}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4mkst", "gilded": 0, "archived": false, "score": 9, "report_reasons": null, "author": "lmcinnes", "parent_id": "t1_dg4jgjm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I'm still considering it a good alternative model. GloVe and word2vec have more in common than people think -- they can both be presented as matrix factorization problems related to word counts based on contexts. It's really all about whether you count word-context pairs, or word-word co-occurences in a context. These things are not so far apart.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m still considering it a good alternative model. GloVe and word2vec have more in common than people think -- they can both be presented as matrix factorization problems related to word counts based on contexts. It&amp;#39;s really all about whether you count word-context pairs, or word-word co-occurences in a context. These things are not so far apart.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4mkst", "score_hidden": false, "stickied": false, "created": 1491961824.0, "created_utc": 1491933024.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 9}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg8y8dj", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "popcorncolonel", "parent_id": "t1_dg58mrz", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "It's kind of crazy that they were able to get away with calling their method word2vec, since any word embedding at all can be described as word2vec. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s kind of crazy that they were able to get away with calling their method word2vec, since any word embedding at all can be described as word2vec. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8y8dj", "score_hidden": false, "stickied": false, "created": 1492189910.0, "created_utc": 1492161110.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg58mrz", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "nickdhaynes", "parent_id": "t1_dg4jgjm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Just to add to other responses (which are all good) - I definitely see people using \"word2vec\" and \"dense word embeddings\" interchangeably, even when those embeddings aren't generated by the word2vec algorithm.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just to add to other responses (which are all good) - I definitely see people using &amp;quot;word2vec&amp;quot; and &amp;quot;dense word embeddings&amp;quot; interchangeably, even when those embeddings aren&amp;#39;t generated by the word2vec algorithm.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg58mrz", "score_hidden": false, "stickied": false, "created": 1491988219.0, "created_utc": 1491959419.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg8y7ii", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "popcorncolonel", "parent_id": "t1_dg4m3fs", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Glove is really not complex either. You just set up the global count matrix and use minibatch SGD to factorize it. ", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Glove is really not complex either. You just set up the global count matrix and use minibatch SGD to factorize it. &lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg8y7ii", "score_hidden": false, "stickied": false, "created": 1492189831.0, "created_utc": 1492161031.0, "depth": 2, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4m3fs", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "olBaa", "parent_id": "t1_dg4jgjm", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Far more complex from the implementation point of view (you can write SGNS in like 200 lines of C++); the initial experiments were not fair, and noone seemed to care after that.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Far more complex from the implementation point of view (you can write SGNS in like 200 lines of C++); the initial experiments were not fair, and noone seemed to care after that.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4m3fs", "score_hidden": false, "stickied": false, "created": 1491961296.0, "created_utc": 1491932496.0, "depth": 1, "mod_reports": [], "subreddit_type": "public", "ups": 5}}], "after": null, "before": null}}, "user_reports": [], "id": "dg4jgjm", "gilded": 0, "archived": false, "score": 5, "report_reasons": null, "author": "maxToTheJ", "parent_id": "t3_64rpje", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "So what happened with glove?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So what happened with glove?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg4jgjm", "score_hidden": false, "stickied": false, "created": 1491958420.0, "created_utc": 1491929620.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 5}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64rpje", "likes": null, "replies": "", "user_reports": [], "id": "dg6smgf", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "MagnesiumCarbonate", "parent_id": "t3_64rpje", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "I feel like this claim \n\n&gt;  The next victim that has fallen prey to the word2vec framework is topic modelling.\n\nis not well supported by the evidence presented:\n\n&gt; unfortunately [Moody's] paper does not offer an explicit comparison with LDA topics\n\n&gt; because [Niu and Dai] only give a few examples, their argument feels rather anecdotic\n\nAdmittedly the author himself acknowledges that he's still not convinced... Evaluating topic models independent of applications is hard ...", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I feel like this claim &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The next victim that has fallen prey to the word2vec framework is topic modelling.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;is not well supported by the evidence presented:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;unfortunately [Moody&amp;#39;s] paper does not offer an explicit comparison with LDA topics&lt;/p&gt;\n\n&lt;p&gt;because [Niu and Dai] only give a few examples, their argument feels rather anecdotic&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Admittedly the author himself acknowledges that he&amp;#39;s still not convinced... Evaluating topic models independent of applications is hard ...&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg6smgf", "score_hidden": false, "stickied": false, "created": 1492070183.0, "created_utc": 1492041383.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}]