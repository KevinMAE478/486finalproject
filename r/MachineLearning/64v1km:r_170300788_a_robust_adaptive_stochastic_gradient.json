[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Research", "id": "64v1km", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 6, "report_reasons": null, "author": "xingdongrobotics", "saved": false, "mod_reports": [], "name": "t3_64v1km", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "arxiv.org", "hidden": false, "thumbnail": "default", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "three", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/64v1km/r_170300788_a_robust_adaptive_stochastic_gradient/", "num_reports": null, "locked": false, "stickied": false, "created": 1491987631.0, "url": "https://arxiv.org/abs/1703.00788", "author_flair_text": null, "quarantine": false, "title": "[R] [1703.00788] A Robust Adaptive Stochastic Gradient Method for Deep Learning", "created_utc": 1491958831.0, "distinguished": null, "media": null, "upvote_ratio": 0.71, "num_comments": 2, "visited": false, "subreddit_type": "public", "ups": 6}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64v1km", "likes": null, "replies": "", "user_reports": [], "id": "dg586t3", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "arXiv_abstract_bot", "parent_id": "t3_64v1km", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Title: A Robust Adaptive Stochastic Gradient Method for Deep Learning  \n\nAuthors: [Caglar Gulcehre](http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1), [Jose Sotelo](http://arxiv.org/find/cs/1/au:+Sotelo_J/0/1/0/all/0/1), [Marcin Moczulski](http://arxiv.org/find/cs/1/au:+Moczulski_M/0/1/0/all/0/1), [Yoshua Bengio](http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1)  \n\n&gt; Abstract: Stochastic gradient algorithms are the main focus of large-scale optimization problems and led to important successes in the recent advancement of the deep learning algorithms. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose an adaptive learning rate algorithm, which utilizes stochastic curvature information of the loss function for automatically tuning the learning rates. The information about the element- wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.  \n\n[PDF link](https://arxiv.org/pdf/1703.00788)  [Landing page](https://arxiv.org/abs/1703.00788)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Title: A Robust Adaptive Stochastic Gradient Method for Deep Learning  &lt;/p&gt;\n\n&lt;p&gt;Authors: &lt;a href=\"http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1\"&gt;Caglar Gulcehre&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Sotelo_J/0/1/0/all/0/1\"&gt;Jose Sotelo&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Moczulski_M/0/1/0/all/0/1\"&gt;Marcin Moczulski&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\"&gt;Yoshua Bengio&lt;/a&gt;  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Abstract: Stochastic gradient algorithms are the main focus of large-scale optimization problems and led to important successes in the recent advancement of the deep learning algorithms. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose an adaptive learning rate algorithm, which utilizes stochastic curvature information of the loss function for automatically tuning the learning rates. The information about the element- wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.  &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/1703.00788\"&gt;PDF link&lt;/a&gt;  &lt;a href=\"https://arxiv.org/abs/1703.00788\"&gt;Landing page&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg586t3", "score_hidden": false, "stickied": false, "created": 1491987659.0, "created_utc": 1491958859.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_64v1km", "likes": null, "replies": "", "user_reports": [], "id": "dg7cbc7", "gilded": 0, "archived": false, "score": 2, "report_reasons": null, "author": "JustFinishedBSG", "parent_id": "t3_64v1km", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "What has changed since the 2014 version of the paper except a lot more figures?", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What has changed since the 2014 version of the paper except a lot more figures?&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7cbc7", "score_hidden": false, "stickied": false, "created": 1492103671.0, "created_utc": 1492074871.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 2}}], "after": null, "before": null}}]