[{"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t3", "data": {"contest_mode": false, "banned_by": null, "media_embed": {}, "subreddit": "MachineLearning", "selftext_html": null, "selftext": "", "likes": null, "suggested_sort": null, "user_reports": [], "secure_media": null, "link_flair_text": "Research", "id": "655vpp", "gilded": 0, "secure_media_embed": {}, "clicked": false, "score": 17, "report_reasons": null, "author": "bobchennan", "saved": false, "mod_reports": [], "name": "t3_655vpp", "subreddit_name_prefixed": "r/MachineLearning", "approved_by": null, "over_18": false, "domain": "arxiv.org", "hidden": false, "thumbnail": "default", "subreddit_id": "t5_2r3gv", "edited": false, "link_flair_css_class": "three", "author_flair_css_class": null, "downs": 0, "brand_safe": true, "archived": false, "removal_reason": null, "is_self": false, "hide_score": false, "spoiler": false, "permalink": "/r/MachineLearning/comments/655vpp/r170403732_learning_from_demonstrations_for_real/", "num_reports": null, "locked": false, "stickied": false, "created": 1492123306.0, "url": "https://arxiv.org/abs/1704.03732", "author_flair_text": null, "quarantine": false, "title": "[R][1704.03732] Learning from Demonstrations for Real World Reinforcement Learning", "created_utc": 1492094506.0, "distinguished": null, "media": null, "upvote_ratio": 0.82, "num_comments": 2, "visited": false, "subreddit_type": "public", "ups": 17}}], "after": null, "before": null}}, {"kind": "Listing", "data": {"modhash": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655vpp", "likes": null, "replies": "", "user_reports": [], "id": "dg86xfn", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "pushkar3", "parent_id": "t3_655vpp", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "As Elon said.. we are living in a simulation... and it happens to be an Atari simulation.", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As Elon said.. we are living in a simulation... and it happens to be an Atari simulation.&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg86xfn", "score_hidden": false, "stickied": false, "created": 1492144948.0, "created_utc": 1492116148.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "removal_reason": null, "link_id": "t3_655vpp", "likes": null, "replies": "", "user_reports": [], "id": "dg7mvsm", "gilded": 0, "archived": false, "score": 1, "report_reasons": null, "author": "arXiv_abstract_bot", "parent_id": "t3_655vpp", "subreddit_name_prefixed": "r/MachineLearning", "controversiality": 0, "body": "Title: Learning from Demonstrations for Real World Reinforcement Learning  \n\nAuthors: [Todd Hester](http://arxiv.org/find/cs/1/au:+Hester_T/0/1/0/all/0/1), [Matej Vecerik](http://arxiv.org/find/cs/1/au:+Vecerik_M/0/1/0/all/0/1), [Olivier Pietquin](http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1), [Marc Lanctot](http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1), [Tom Schaul](http://arxiv.org/find/cs/1/au:+Schaul_T/0/1/0/all/0/1), [Bilal Piot](http://arxiv.org/find/cs/1/au:+Piot_B/0/1/0/all/0/1), [Andrew Sendonaris](http://arxiv.org/find/cs/1/au:+Sendonaris_A/0/1/0/all/0/1), [Gabriel Dulac- Arnold](http://arxiv.org/find/cs/1/au:+Dulac_Arnold_G/0/1/0/all/0/1), [Ian Osband](http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1), [John Agapiou](http://arxiv.org/find/cs/1/au:+Agapiou_J/0/1/0/all/0/1), [Joel Z. Leibo](http://arxiv.org/find/cs/1/au:+Leibo_J/0/1/0/all/0/1), [Audrunas Gruslys](http://arxiv.org/find/cs/1/au:+Gruslys_A/0/1/0/all/0/1)  \n\n&gt; Abstract: Deep reinforcement learning (RL) has achieved several high profile successes in difficult control problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages this data to massively accelerate the learning process even from relatively small amounts of demonstration data. DQfD works by combining temporal difference updates with large-margin classification of the demonstrator's actions. We show that DQfD has better initial performance than Deep Q-Networks (DQN) on 40 of 42 Atari games and it receives more average rewards than DQN on 27 of 42 Atari games. We also demonstrate that DQfD learns faster than DQN even when given poor demonstration data.  \n\n[PDF link](https://arxiv.org/pdf/1704.03732)  [Landing page](https://arxiv.org/abs/1704.03732)", "edited": false, "downs": 0, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Title: Learning from Demonstrations for Real World Reinforcement Learning  &lt;/p&gt;\n\n&lt;p&gt;Authors: &lt;a href=\"http://arxiv.org/find/cs/1/au:+Hester_T/0/1/0/all/0/1\"&gt;Todd Hester&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Vecerik_M/0/1/0/all/0/1\"&gt;Matej Vecerik&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1\"&gt;Olivier Pietquin&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1\"&gt;Marc Lanctot&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Schaul_T/0/1/0/all/0/1\"&gt;Tom Schaul&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Piot_B/0/1/0/all/0/1\"&gt;Bilal Piot&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Sendonaris_A/0/1/0/all/0/1\"&gt;Andrew Sendonaris&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Dulac_Arnold_G/0/1/0/all/0/1\"&gt;Gabriel Dulac- Arnold&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1\"&gt;Ian Osband&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Agapiou_J/0/1/0/all/0/1\"&gt;John Agapiou&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Leibo_J/0/1/0/all/0/1\"&gt;Joel Z. Leibo&lt;/a&gt;, &lt;a href=\"http://arxiv.org/find/cs/1/au:+Gruslys_A/0/1/0/all/0/1\"&gt;Audrunas Gruslys&lt;/a&gt;  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Abstract: Deep reinforcement learning (RL) has achieved several high profile successes in difficult control problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages this data to massively accelerate the learning process even from relatively small amounts of demonstration data. DQfD works by combining temporal difference updates with large-margin classification of the demonstrator&amp;#39;s actions. We show that DQfD has better initial performance than Deep Q-Networks (DQN) on 40 of 42 Atari games and it receives more average rewards than DQN on 27 of 42 Atari games. We also demonstrate that DQfD learns faster than DQN even when given poor demonstration data.  &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/1704.03732\"&gt;PDF link&lt;/a&gt;  &lt;a href=\"https://arxiv.org/abs/1704.03732\"&gt;Landing page&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "subreddit": "MachineLearning", "name": "t1_dg7mvsm", "score_hidden": false, "stickied": false, "created": 1492123311.0, "created_utc": 1492094511.0, "depth": 0, "mod_reports": [], "subreddit_type": "public", "ups": 1}}], "after": null, "before": null}}]